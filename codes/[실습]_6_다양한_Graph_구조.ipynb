{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsYrzxX5DaqK"
      },
      "source": [
        "# [실습] 다양한 Graph 구조\n",
        "\n",
        "그동안 배운 요소들을 바탕으로, 이번에는 기존의 Graph 구조를 보다 확장시켜 보겠습니다.   \n",
        "\n",
        "간단한 Router 구조,\n",
        "\n",
        "하나의 출발점에서 여러 개로 분리되는 Parallel Calling 이후에 결과를 합치는 Map Reduce 방식,\n",
        "\n",
        "생성자와 평가자의 구조를 반복하는 Evaluator-Optimizer 방식을 구현해 보겠습니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1JC3bK9DaqL"
      },
      "outputs": [],
      "source": [
        "!pip install langgraph langchain langchain_google_genai langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z592IDzIDaqM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['GOOGLE_API_KEY'] = 'AIxxx'\n",
        "\n",
        "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Gemini API는 분당 10개 요청으로 제한\n",
        "# 즉, 초당 약 0.167개 요청 (10/60)\n",
        "rate_limiter = InMemoryRateLimiter(\n",
        "    requests_per_second=0.167,  # 분당 10개 요청\n",
        "    check_every_n_seconds=0.1,  # 100ms마다 체크\n",
        "    max_bucket_size=10,  # 최대 버스트 크기\n",
        ")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    rate_limiter=rate_limiter,\n",
        "    # temperature\n",
        "    # max_tokens\n",
        "\n",
        "    thinking_budget = 500  # 추론(Reasoning) 토큰 길이 제한\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MAE1pIFDaqM"
      },
      "source": [
        "## 1. Router\n",
        "라우터는 State의 값을 참고하여, 목적에 따라 서로 다른 노드로 전달하는 방식을 의미합니다.   \n",
        "주로 사용자의 입력을 분류하여 서로 다른 작업을 연결하는 의도 분류(Intent Classfication)에서 활용됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWkZTeKdDaqM"
      },
      "outputs": [],
      "source": [
        "from typing_extensions import TypedDict, Annotated, Literal, List\n",
        "from pydantic import BaseModel, Field\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "class Recipe(BaseModel):\n",
        "    name: str = Field(..., description=\"음식 이름\")\n",
        "    difficulty: str = Field(..., description=\"만들기의 난이도\")\n",
        "    origin: str = Field(..., description=\"원산지\")\n",
        "    ingredients: List[str] = Field(..., description=\"재료 목록\")\n",
        "    instructions: List[str] = Field(..., description=\"조리법\")\n",
        "    taste: List[str] = Field(..., description=\"맛에 대한 한 마디의 묘사!\")\n",
        "\n",
        "class Movie(BaseModel):\n",
        "    name: str = Field(..., description=\"영화 이름\")\n",
        "    director: str = Field(..., description=\"감독명\")\n",
        "    actor: List[str] = Field(..., description=\"주연 배우: 최대 3명까지\")\n",
        "    recommendation: str = Field(..., description=\"추천하는 이유!\")\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "    query: str\n",
        "    classification: str\n",
        "    recipe: Recipe\n",
        "    movie:Movie\n",
        "    advice : Literal['네!', '아니오.']\n",
        "    # Literal: 범위가 특정 값으로 한정되는 경우\n",
        "    answer: str\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfewW4rYDaqM"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "import random\n",
        "\n",
        "def recommend_recipe(state):\n",
        "    prompt = ChatPromptTemplate([\n",
        "    ('system','당신은 전세계의 이색적인 퓨전 조리법의 전문가입니다.'),\n",
        "    ('user','''{query}''')\n",
        "    ])\n",
        "\n",
        "    recipe_chain = prompt | llm.with_structured_output(Recipe)\n",
        "\n",
        "    return {'recipe':recipe_chain.invoke(state)}\n",
        "    # query --> query\n",
        "\n",
        "def recommend_movie(state):\n",
        "    prompt = ChatPromptTemplate([\n",
        "    ('system','당신은 고전 영화의 전문가입니다.'),\n",
        "    ('user','''{query}''')\n",
        "    ])\n",
        "\n",
        "    movie_chain = prompt | llm.with_structured_output(Movie)\n",
        "\n",
        "    return {'movie':movie_chain.invoke(state)}\n",
        "    # query --> query\n",
        "\n",
        "\n",
        "def talk(state):\n",
        "    return {'answer':llm.invoke(state['query']).content}\n",
        "\n",
        "\n",
        "def counsel(state):\n",
        "    if random.random()>=0.5:\n",
        "        return {'advice':'네!'}\n",
        "    else:\n",
        "        return {'advice':'아니오.'}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Hu6yFdlDaqM"
      },
      "outputs": [],
      "source": [
        "def route(state):\n",
        "\n",
        "    prompt = ChatPromptTemplate(\n",
        "        [('system', '''당신의 역할은 사용자의 질문에 대답할 사람을 선택하는 것입니다.\n",
        "1) 음식 관련 질문: 'FOOD'만 출력하세요.\n",
        "2) 영화 관련 질문: 'MOVIE'만 출력하세요.\n",
        "3) 고민 상담: 'COUNSEL'만 출력하세요.\n",
        "4) 그 외의 대화: 'TALK'만 출력하세요.\n",
        "             '''),\n",
        "             ('user','User Query: {query}')\n",
        "        ]\n",
        "    )\n",
        "    # Structured_Output으로 만들 수도 있습니다!\n",
        "\n",
        "    route_chain = prompt | llm\n",
        "\n",
        "    return {\"classification\": route_chain.invoke(state).content}\n",
        "    # query --> query\n",
        "\n",
        "\n",
        "def route_decision(state):\n",
        "    # Exact Match 대신 조금 안정적인 조건식\n",
        "\n",
        "    if \"FOOD\" in state[\"classification\"]:\n",
        "        return \"recommend_recipe\"\n",
        "    elif \"MOVIE\" in state[\"classification\"]:\n",
        "        return \"recommend_movie\"\n",
        "    elif \"TALK\" in state[\"classification\"]:\n",
        "        return \"talk\"\n",
        "    else:\n",
        "        return \"counsel\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFKpzJMhDaqM"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "builder = StateGraph(State)\n",
        "\n",
        "builder.add_node('recommend_movie', recommend_movie)\n",
        "builder.add_node('recommend_recipe', recommend_recipe)\n",
        "builder.add_node('counsel', counsel)\n",
        "builder.add_node('talk', talk)\n",
        "builder.add_node('route', route)\n",
        "\n",
        "builder.add_edge(START, 'route')\n",
        "builder.add_conditional_edges('route', route_decision,\n",
        "                              {'recommend_movie':'recommend_movie',\n",
        "                               'recommend_recipe':'recommend_recipe',\n",
        "                               'counsel':'counsel',\n",
        "                               'talk':'talk'})\n",
        "\n",
        "builder.add_edge('recommend_movie', END)\n",
        "builder.add_edge('recommend_recipe', END)\n",
        "builder.add_edge('counsel', END)\n",
        "builder.add_edge('talk', END)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "geuxT7gMDaqN"
      },
      "outputs": [],
      "source": [
        "graph = builder.compile()\n",
        "graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgHC00M1DaqN"
      },
      "outputs": [],
      "source": [
        "query = '2월에 어울리는 한국영화 추천해줘.'\n",
        "\n",
        "result = graph.invoke({'query':query})\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5CvJCxvRrGH"
      },
      "outputs": [],
      "source": [
        "query = '연두부로 만들 수 있는 파인 다이닝 메뉴 추천해주세요'\n",
        "\n",
        "result = graph.invoke({'query':query})\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5Uz7vWfSTk1"
      },
      "outputs": [],
      "source": [
        "query = 'MoE 구조에 대해 5문장으로 설명해주세요.'\n",
        "\n",
        "result = graph.invoke({'query':query})\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_aFXbYvDaqN"
      },
      "source": [
        "## 2. Map-Reduce"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78zn6Sw_DaqN"
      },
      "source": [
        "위에서는 분류 후에 1번의 LLM을 호출했는데요.   \n",
        "각자 실행하고 합치는 구조도 만들 수 있습니다.   \n",
        "\n",
        "대표적인 작업인 리포트 작성 구조를 보겠습니다.   \n",
        "최초의 LLM이 주제에 대한 섹션을 먼저 구성하고, 섹션별 리포트를 각각의 LLM이 작성하는 방식입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZhYoD0qDaqN"
      },
      "outputs": [],
      "source": [
        "# 전체 섹션의 구획: Contents (Chapter List)\n",
        "# Chapter: name, outline\n",
        "class Chapter(BaseModel):\n",
        "    name: str = Field(description=\"챕터의 이름\")\n",
        "    outline: str = Field(description=\"챕터의 주요 내용, 1문장 길이로\")\n",
        "\n",
        "\n",
        "class Contents(BaseModel):\n",
        "    contents: List[Chapter] = Field(description=\"전체 리포트의 섹션 구성\")\n",
        "\n",
        "\n",
        "planner = llm.with_structured_output(Contents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWO6Hp-sDaqN"
      },
      "outputs": [],
      "source": [
        "example = planner.invoke(\"LLM의 발전 과정에 대한 보고서 구획을 작성해 주세요.\")\n",
        "example.contents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z02IWYAIDaqN"
      },
      "source": [
        "그래프에서 사용할 State를 정의합니다.   \n",
        "\n",
        "이번에는 중간 Writer LLM이 사용할 State를 별도로 만들어 보겠습니다.   \n",
        "이렇게 구성하면 최종 State에서 필요한 부분만 저장할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_MIbKB8DaqN"
      },
      "outputs": [],
      "source": [
        "import operator\n",
        "\n",
        "# reducer 구조: operator.add\n",
        "# 단순 + 연산 구조 (리스트의 + 연산이므로 append)\n",
        "\n",
        "class State(TypedDict):\n",
        "    topic: str\n",
        "    contents: list[Chapter]\n",
        "    completed_sections: Annotated[list, operator.add]\n",
        "    final_report: str\n",
        "\n",
        "\n",
        "# 섹션 Writer가 사용할 State\n",
        "class SubState(TypedDict):\n",
        "    chapter: Chapter\n",
        "    completed_sections: Annotated[list, operator.add]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn9oVR_xDaqO"
      },
      "source": [
        "섹션을 생성하는 노드를 구성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VIpcYLzDaqO"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
        "\n",
        "def orchestrator(state: State):\n",
        "\n",
        "    prompt = ChatPromptTemplate([\n",
        "        ('system', \"주제에 대한 전문가 수준의 깊이 있는 한국어 보고서를 쓰려고 합니다. 보고서의 섹션 구성과, 각 섹션의 간단한 설명을 작성해 주세요.\"),\n",
        "        ('user', \"주제: {topic}\")\n",
        "    ])\n",
        "    chain = prompt | planner\n",
        "\n",
        "    # chain 결과물: Contents (contents: List[Chapter])\n",
        "\n",
        "    return {\"contents\": chain.invoke(state).contents}\n",
        "    # state: topic --> topic\n",
        "    # Return: List[Chapter]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IC0OpL4NDaqO"
      },
      "source": [
        "섹션별 내용을 처리하는 노드를 구성합니다.   \n",
        "State에는 각각의 Chapter가 아닌 Chapter의 리스트인 Contents가 들어 있는데요.   \n",
        "\n",
        "`SubState`를 이용해, 각각의 Chapter를 처리하도록 정의하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6S5AwvDbDaqO"
      },
      "outputs": [],
      "source": [
        "def llm_call(state: SubState):\n",
        "    # SubState :  chapter, completed_sections 2개 property\n",
        "\n",
        "    chapter = state['chapter']\n",
        "\n",
        "    prompt = ChatPromptTemplate([\n",
        "        ('system',\"아래 섹션에 대한 상세한 한국어 보고서를 작성하세요.\" ),\n",
        "        ('user', \"섹션 이름과 주제는 다음과 같습니다: {name} --> {outline}\")\n",
        "    ])\n",
        "\n",
        "    chain = prompt | llm\n",
        "\n",
        "    return {\"completed_sections\": [chain.invoke({'name':chapter.name, 'outline':chapter.outline}).content]}\n",
        "    # 리스트로 Wrap하는 이유 중요(Reduce Operator 합치기 위해서)\n",
        "\n",
        "\n",
        "# 생성된 섹션별 결과들을 결합\n",
        "def synthesizer(state: State):\n",
        "\n",
        "    completed_sections = state[\"completed_sections\"]\n",
        "\n",
        "    completed_report_sections = \"\\n\\n---\\n\\n\".join(completed_sections)\n",
        "    # join: 전체 리스트 스트링으로 결합하기\n",
        "\n",
        "    return {\"final_report\": completed_report_sections}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gH9uKHSGDaqO"
      },
      "source": [
        "**가장 중요한 부분입니다😁😁**   \n",
        "langgraph의 Send()를 이용하면, 리스트의 원소 개수만큼 서브모듈을 호출할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuqkIRicDaqO"
      },
      "outputs": [],
      "source": [
        "from langgraph.types import Send\n",
        "\n",
        "def assign_workers(state: State):\n",
        "    # Send: 노드를 호출하며, 값을 전달해 준다\n",
        "    # state['contents']의 개수를 기본적으로 알 수 없는데,\n",
        "    # 이를 통해 개수만큼 llm_call을 생성하여 호출할 수 있음\n",
        "\n",
        "    return [Send(\"llm_call\", {\"chapter\": s}) for s in state[\"contents\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEdmwAR1DaqO"
      },
      "source": [
        "그래프를 구성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruDhpkxEDaqO"
      },
      "outputs": [],
      "source": [
        "builder = StateGraph(State)\n",
        "\n",
        "builder.add_node(\"orchestrator\", orchestrator) # 구획 짜고\n",
        "builder.add_node(\"llm_call\", llm_call) # 섹션별 글쓰고\n",
        "builder.add_node(\"synthesizer\", synthesizer) # 합치고\n",
        "\n",
        "\n",
        "builder.add_edge(START, \"orchestrator\")\n",
        "\n",
        "builder.add_conditional_edges(\"orchestrator\", assign_workers, [\"llm_call\"])\n",
        "# assign_workers의 결과에 따라 llm_call을 호출\n",
        "\n",
        "builder.add_edge(\"llm_call\", \"synthesizer\")\n",
        "# 생성된 섹션들은 synthesizer로 이동\n",
        "\n",
        "builder.add_edge(\"synthesizer\", END) # 끝\n",
        "\n",
        "\n",
        "graph = builder.compile()\n",
        "graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4_faj1ODaqO"
      },
      "outputs": [],
      "source": [
        "for data in graph.stream({\"topic\": \"GPT 1부터 최신 LLM까지의 발전과정\"}, stream_mode='updates'):\n",
        "    print(data)\n",
        "    print('--------------')\n",
        "    # 생성은 병렬적이지만 합치는 순서는 호출한 순서"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkeaSELrDaqO"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Markdown\n",
        "Markdown(data['synthesizer'][\"final_report\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90zJyo8uDaqO"
      },
      "source": [
        "## 3. Evaluator-Optimizer 구조"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vzbth_cBDaqO"
      },
      "source": [
        "LLM의 최초 출력을 바로 사용해도 되지만, 평가 기준을 두고 반복적으로 검증하게 한다면 그 품질을 높일 수 있습니다.    \n",
        "\n",
        "주어진 `instruction`에 대한 파이썬 코드를 작성하고, 이를 연속적으로 최적화합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgbT_syXDaqO"
      },
      "outputs": [],
      "source": [
        "class State(TypedDict):\n",
        "    code: str\n",
        "    instruction: str\n",
        "    feedback: str\n",
        "    optimized: str\n",
        "\n",
        "class Feedback(BaseModel):\n",
        "    grade: Literal[\"optimized\", \"not optimized\"] = Field(description=\"코드가 최적화되었는지 판단합니다.\")\n",
        "    feedback: str = Field(description=\"코드의 개선이 필요하다면, 어떤 부분을 개선할 수 있을지 설명합니다.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tLvrdjHDaqO"
      },
      "source": [
        "노드 구조를 구성합니다.   \n",
        "코드를 생성하는 Generator와, 코드를 비판적으로 평가하는 Optimizer 구조를 구현합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_LEagHYDaqP"
      },
      "outputs": [],
      "source": [
        "evaluator = llm.with_structured_output(Feedback)\n",
        "\n",
        "def code_generator(state: State):\n",
        "    # 코드를 생성하고, 피드백에 따라 리팩토링합니다.\n",
        "\n",
        "    if state.get(\"feedback\"):\n",
        "        # 피드백이 있으면\n",
        "\n",
        "        result = llm.invoke(\n",
        "            f\"\"\"다음 문제를 해결하는 파이썬 코드를 작성하세요. 설명 없이 코드만 출력하세요.\n",
        "Instruction:{state['instruction']}\n",
        "다음의 피드백을 고려하세요.\n",
        "Feedback: {state['feedback']}\"\"\")\n",
        "    else:\n",
        "        result = llm.invoke(f\"\"\"다음 문제를 해결하는 파이썬 코드를 작성하세요. 설명 없이 코드만 출력하세요.\n",
        "Instruction:{state['instruction']}\"\"\")\n",
        "\n",
        "    return {\"code\": result.content}\n",
        "\n",
        "\n",
        "def code_evaluator(state: State):\n",
        "    # 실제로는 유닛 테스트를 생성해서 검증하는 것도 필요하겠습니다.\n",
        "\n",
        "    result = evaluator.invoke(f\"\"\"다음의 문제를 해결하는 파이썬 코드가 최적화되었는지 한국어로 평가하세요.\n",
        "평가 기준은 코드의 길이와 실행 속도 및 메모리 효율성입니다.\n",
        "코드의 길이가 제일 중요합니다.\n",
        "---\n",
        "Instruction:{state['instruction']}\n",
        "---\n",
        "Source Code: {state['code']}\"\"\")\n",
        "\n",
        "    return {\"optimized\": result.grade, \"feedback\": result.feedback}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Optimized 값에 따라 경로 설정\n",
        "def route_code(state: State):\n",
        "    return 'Accepted' if state[\"optimized\"] == \"optimized\" else 'Rejected + Feedback'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQqW_zrtDaqP"
      },
      "outputs": [],
      "source": [
        "builder = StateGraph(State)\n",
        "\n",
        "builder.add_node(\"code_generator\", code_generator)\n",
        "builder.add_node(\"code_evaluator\", code_evaluator)\n",
        "\n",
        "builder.add_edge(START, \"code_generator\")\n",
        "builder.add_edge(\"code_generator\", \"code_evaluator\")\n",
        "\n",
        "builder.add_conditional_edges(\"code_evaluator\",route_code,\n",
        "                              {\"Accepted\": END,\"Rejected + Feedback\": \"code_generator\"})\n",
        "\n",
        "graph = builder.compile()\n",
        "graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_-QBY4oDaqP"
      },
      "outputs": [],
      "source": [
        "for data in graph.stream({'instruction':'자연수보다 작은 소수 개수 구하기'}, stream_mode='updates'):\n",
        "    if 'code_generator' in data:\n",
        "        print(data['code_generator']['code'])\n",
        "    else:\n",
        "        print(data['code_evaluator']['optimized'],'-->', data['code_evaluator']['feedback'])\n",
        "    print('--------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enP8sZ-kYgBD"
      },
      "outputs": [],
      "source": [
        "for data in graph.stream({'instruction':'자연수 소인수분해'}, stream_mode='updates'):\n",
        "    if 'code_generator' in data:\n",
        "        print(data['code_generator']['code'])\n",
        "    else:\n",
        "        print(data['code_evaluator']['optimized'],'-->', data['code_evaluator']['feedback'])\n",
        "    print('--------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for data in graph.stream({'instruction':'Convex Hull 문제'}, stream_mode='updates'):\n",
        "    if 'code_generator' in data:\n",
        "        print(data['code_generator']['code'])\n",
        "    else:\n",
        "        print(data['code_evaluator']['optimized'],'-->', data['code_evaluator']['feedback'])\n",
        "    print('--------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

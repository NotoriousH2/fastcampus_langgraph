{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsYrzxX5DaqK"
      },
      "source": [
        "# [실습] Ollama과 vLLM을 이용한 Agentic Work 만들기   \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBGUmtU0ChST"
      },
      "source": [
        "## 라이브러리 설치\n",
        "\n",
        "\n",
        "이번 실습은 무료 코랩(T4 GPU)이 아닌 고성능 GPU 라이브러리에서 진행합니다.   \n",
        "무료 코랩으로 진행하시는 경우, GPU 성능의 한계로 vLLM 실행이 어렵습니다.\n",
        "\n",
        "<br><br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lPVjc9mChST",
        "outputId": "531f969b-be62-4110-ed22-d3180797a843",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!pip install transformers bitsandbytes openai langchain_openai -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1JC3bK9DaqL",
        "outputId": "d3e122a4-4b57-4818-d530-0fd63a205973"
      },
      "outputs": [],
      "source": [
        "!pip install langchain_experimental langgraph langchain langchain_community langchain_huggingface dotenv -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvXDl6a-ChSU",
        "outputId": "fd5b1af6-fa00-441f-803c-7fa6c5b88e85"
      },
      "outputs": [],
      "source": [
        "# Pytorch 버전 호환\n",
        "# vllm 0.10 : Pytorch 2.7.1\n",
        "# vllm 0.9.x == 2.7.0\n",
        "# vllm 0.8.x == 2.6.0\n",
        "# vllm 0.6.x == 2.5.1\n",
        "\n",
        "!pip install pyzmq flashinfer-python==0.2.10 vllm==0.10.1 -q\n",
        "!pip install  flashinfer-python pyzmq vllm -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-K10yruCvnu"
      },
      "source": [
        "설치 후에는 세션을 재시작해 주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-Q0yOegChSU",
        "outputId": "3dbaf4e0-c1ca-4268-aaf9-6f39fd3614f5",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Flash Attention: 리눅스 전용 설치방법\n",
        "# Windows 설치는 https://github.com/kingbri1/flash-attention/releases 참고\n",
        "!pip install flash-attn --no-build-isolation -q\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwLiGVjiChSV"
      },
      "source": [
        "설치할 라이브러리가 많으므로, 가급적 설치 후 세션 재시작을 수행해 주세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfDjPosVChSV"
      },
      "source": [
        "vLLM은 캐싱을 통해 효과적인 추론과 동시 실행을 지원합니다.   \n",
        "아래 코드를 터미널에서 실행하세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIRl33XXChSV"
      },
      "source": [
        "```\n",
        "vllm serve unsloth/gemma-3-12b-it-bnb-4bit --dtype auto --max_model_len 32768 --quantization bitsandbytes --served_model_name gemma3 --max_num_seqs 1\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uiptF1yChSW"
      },
      "source": [
        "어느 정도 시간이 지나면, vLLM 서빙이 완료됩니다.   \n",
        "vllm Serve가 정상적으로 완료되면, 8000번 포트에서 모델을 확인하실 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7RLw4ynDEqa"
      },
      "outputs": [],
      "source": [
        "# 모델 주소 확인\n",
        "!curl http://0.0.0.0:8000/v1/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huewOgXWChSW"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI, OpenAI\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    base_url=\"http://localhost:8000/v1\",\n",
        "    api_key=\"token-abc123\",\n",
        "    model=\"gemma3\",\n",
        "    temperature=0.5,\n",
        "    max_tokens=1024\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKU1bgY9ChSW",
        "outputId": "aba1396f-2e7f-406b-bcbb-8d67d590a0e3"
      },
      "outputs": [],
      "source": [
        "for s in llm.stream(\"vLLM이 뭐야?\"):\n",
        "    print(s.content, end='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXqmbS-lChSW",
        "outputId": "d6187fef-523f-445b-da64-ae2452328047"
      },
      "outputs": [],
      "source": [
        "def convert_chat(messages, add_generation_prompt = True):\n",
        "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=add_generation_prompt)\n",
        "\n",
        "example = [{'role':'user', 'content':'안녕'}]\n",
        "convert_chat(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_P4V-vgChSW"
      },
      "source": [
        "## 2. HuggingFace LLM과 툴 연동하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwiBEnDuChSW"
      },
      "source": [
        "먼저 툴을 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIOEJGlIChSW",
        "outputId": "68a464c6-3f6c-4bd4-a350-ca836db149c0"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_core.tools import tool\n",
        "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
        "from datetime import datetime\n",
        "\n",
        "tavily_search = TavilySearchResults(\n",
        "    max_results=3)\n",
        "\n",
        "repl_tool = PythonREPLTool()\n",
        "repl_tool.invoke(\"for i in range(10): print(i)\")\n",
        "\n",
        "\n",
        "@tool\n",
        "def current_date() -> str:\n",
        "    \"현재 날짜를 %y-%m-%d 형식으로 반환합니다.\"\n",
        "    return datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "\n",
        "\n",
        "tools = [tavily_search, repl_tool, current_date]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY-T4e9hChSW"
      },
      "source": [
        "vLLM에서는 Tool Binding을 수행하려면   \n",
        "--enable-auto-tool-choice 를 추가한 뒤, 전용 파서를 연결해야 합니다.   \n",
        "Llama, Qwen, Hermes 등의 모델이 가능합니다.   \n",
        "https://docs.vllm.ai/en/stable/features/tool_calling.htm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcZfOJvTChSX",
        "outputId": "fef8a96b-f6d8-4ac2-db98-3fe102ab580a"
      },
      "outputs": [],
      "source": [
        "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
        "\n",
        "\n",
        "convert_to_openai_tool(tools[2])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Av22_b_eChSX"
      },
      "source": [
        "툴 설명이 담긴 문자열을 구성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFAAolWtChSX",
        "outputId": "794c42db-e9af-4108-90f8-1325ea4db5f5"
      },
      "outputs": [],
      "source": [
        "tool_desc = str('\\n---\\n'.join([str(convert_to_openai_tool(tool)) for tool in tools]))\n",
        "print(tool_desc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxPl5cfWChSX"
      },
      "source": [
        "시스템 프롬프트를 구성합니다.   \n",
        "성능에 매우 중요한 영향을 미치므로, 영어로 작성했습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLF16NzcChSX"
      },
      "outputs": [],
      "source": [
        "system_prompt = f'''\n",
        "You are a helpful assistant with tools below.\n",
        "You can decide whether to invoke any functions or not.\n",
        "If you decide to use any of tools.\n",
        "print name and required parameters of the tool within a json blob correctly.\n",
        "For python code, Return the object as a raw dictionary, without escaping quotes or newlines.\n",
        "\n",
        "for tool use: wrap your output within ```tool_code```.\n",
        "\n",
        "Example:\n",
        "```tool_code\n",
        "{{\"name\":'name of tool', \"arguments\":{{List of apparent argument and parameters}}}}\n",
        "```\n",
        "\n",
        "When the output of the tool is provided, it will be wrapped within ``tool_output```\n",
        "Answer accordingly from the result of the tool output.\n",
        "\n",
        "The question might need some sequential, multiple tool execution.\n",
        "Think Step by Step.\n",
        "\n",
        "The following tools are available:\n",
        "{tool_desc}'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGNheWMdChSX"
      },
      "source": [
        "시스템 프롬프트에 들어가야 하는 내용은 다음과 같습니다.\n",
        "- Tool Format\n",
        "- Tool Call Format\n",
        "- Tool Result Format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6hbNYp0ChSY"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, ToolMessage\n",
        "\n",
        "\n",
        "messages = [SystemMessage(system_prompt),\n",
        "            HumanMessage('오늘 날짜가 며칠이니?')]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcxyBL1bChSY",
        "outputId": "53dd0fab-5a69-4966-aa7b-819ada117091"
      },
      "outputs": [],
      "source": [
        "response = llm.invoke(messages)\n",
        "messages.append(response)\n",
        "response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHphfjKGChSY",
        "outputId": "ae9cf9c4-35bd-4ae4-de1c-7b33608b0a70"
      },
      "outputs": [],
      "source": [
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hf3ikmr5ChSY"
      },
      "source": [
        "tool_code를 받았으니, 해당 내용을 파싱합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Mg_NaBOChSY",
        "outputId": "e09d54bf-c3ca-450a-c116-1aee57844edf"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "\n",
        "\n",
        "def parse_tool(text):\n",
        "    try:\n",
        "        text = text.split('```tool_code\\n')[1].split('\\n```')[0]\n",
        "        # tool_code로 wrap된 중간 코드 추출\n",
        "\n",
        "        parsed = ast.literal_eval(text)\n",
        "        # Dict 형태의 값 변환 (json load와 유사)\n",
        "\n",
        "        name = parsed.get('name')\n",
        "        arguments = parsed.get('arguments', {})\n",
        "        # name과 argument return\n",
        "        return {'name':name, 'arguments':arguments}\n",
        "    except (ValueError, SyntaxError):\n",
        "        return None\n",
        "\n",
        "result = parse_tool(response.content)\n",
        "name,arguments = result['name'], result['arguments']\n",
        "name, arguments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaTQK_seChSZ"
      },
      "source": [
        "툴 실행을 연결합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plDwh8fNChSZ",
        "outputId": "7bd96f8b-a81a-41c7-ed88-9858f1a7214e"
      },
      "outputs": [],
      "source": [
        "# 툴 이름과 툴 연결\n",
        "tool_dict = {tool.name: tool for tool in tools}\n",
        "\n",
        "def execute_tool(name, arguments):\n",
        "    # 툴 실행한 뒤 tool_output으로 wrap\n",
        "    result = f'''```tool_output\n",
        "{tool_dict[name].invoke(arguments)}\n",
        "```'''\n",
        "    return result\n",
        "\n",
        "tool_result = execute_tool(**parse_tool(response.content))\n",
        "\n",
        "print(tool_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vALn3uIChSZ",
        "outputId": "33d48f10-c301-4709-9a28-5da925fbcc59"
      },
      "outputs": [],
      "source": [
        "messages.append(HumanMessage(tool_result))\n",
        "messages[1:]\n",
        "# 질문 + Tool 요청 + Tool 결과"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-00rE8xChSc",
        "outputId": "63010bcb-b25f-4c1a-8bbf-d783ec3af7b4"
      },
      "outputs": [],
      "source": [
        "# 결과 해석\n",
        "response = llm.invoke(messages)\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuoZLPE-ChSc",
        "outputId": "f74077dd-1109-4c04-941e-15c3afe3345e"
      },
      "outputs": [],
      "source": [
        "messages = [SystemMessage(system_prompt),\n",
        "            HumanMessage('2025년 4월 발표된 GPT-4.1 모델이 어떤 모델이야? 한국어로 설명해줘.')]\n",
        "response = llm.invoke(messages)\n",
        "messages.append(response)\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4UwJtbSChSc",
        "outputId": "050fa880-3c6a-4132-da28-d012773a7389"
      },
      "outputs": [],
      "source": [
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVIJ0E3gChSc",
        "outputId": "c96088d6-c498-4269-fe59-d71584106b1b"
      },
      "outputs": [],
      "source": [
        "tool_result = execute_tool(**parse_tool(response.content))\n",
        "tool_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbPDgbu7ChSd",
        "outputId": "04963625-442d-433e-f51e-1f717b666b2c"
      },
      "outputs": [],
      "source": [
        "messages[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G636d4DkChSd",
        "outputId": "420c583c-2335-4bfe-dcc1-81d7aca04675"
      },
      "outputs": [],
      "source": [
        "messages.append(HumanMessage(tool_result))\n",
        "response = llm.invoke(messages)\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zM4gli-1ChSd"
      },
      "source": [
        "일반적인 입출력 관계의 툴은 이와 같은 방식으로 간단하게 구성할 수 있습니다.   \n",
        "(만약, Python_REPL과 같이 argument가 복잡한 툴을 수행하는 경우에는   \n",
        "별도의 함수로 변환하거나 결과물을 수정하는 작업이 필요할 수 있습니다.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qx8blhUwChSd"
      },
      "source": [
        "해당 구현을 통해, ReAct Agent 구조를 만들어 보겠습니다.    \n",
        "bind_tools가 없기 때문에, 기존의 Tool Message를 사용하기 어렵습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJuCpEQ5ChSd"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages : Annotated[list, add_messages]   # 메시지 맥락을 저장하는 리스트\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9bMAts4ChSd"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import ToolMessage\n",
        "\n",
        "tool_list = {tool.name: tool for tool in tools}\n",
        "# tool 목록 dict로 생성\n",
        "\n",
        "def tool_node(state):\n",
        "    tool_outputs = []\n",
        "    tool_call_msgs = state['messages'][-1]\n",
        "    # 마지막 메시지: 툴 콜링 메시지\n",
        "    if '```tool_code' in tool_call_msgs.content:\n",
        "        tool_result = execute_tool(**parse_tool(tool_call_msgs.content))\n",
        "        # tool 실행 결과 얻기 (결과는 ```tool_output```)\n",
        "        tool_outputs.append(HumanMessage(tool_result))\n",
        "\n",
        "    return {'messages': tool_outputs}\n",
        "\n",
        "def agent(state):\n",
        "    system_prompt = SystemMessage(f'''\n",
        "You are a helpful assistant with tools below.\n",
        "You can decide whether to invoke any functions or not.\n",
        "If you decide to use any of tools.\n",
        "print name and required parameters of the tool within a json blob correctly.\n",
        "For python code, Return the object as a raw dictionary, without escaping quotes or newlines.\n",
        "\n",
        "for tool use: wrap your output within ```tool_code```.\n",
        "\n",
        "Example:\n",
        "```tool_code\n",
        "{{\"name\":'name of tool', \"arguments\":{{List of apparent argument and parameters}}}}\n",
        "```\n",
        "\n",
        "When the output of the tool is provided, it will be wrapped within ``tool_output```\n",
        "Answer accordingly from the result of the tool output.\n",
        "\n",
        "The question might need some sequential, multiple tool execution.\n",
        "Think Step by Step.\n",
        "\n",
        "The following tools are available:\n",
        "{tool_desc}\n",
        "\n",
        "\n",
        "Answer in Korean.''')\n",
        "\n",
        "\n",
        "    response = llm.invoke([system_prompt] + state[\"messages\"])\n",
        "    return {'messages': response}\n",
        "\n",
        "def tool_needed(state):\n",
        "\n",
        "    last_msg = state['messages'][-1]\n",
        "    if '```tool_code' in last_msg.content: # 툴 콜링이 필요하면\n",
        "        return \"continue\"\n",
        "    else:\n",
        "        return \"finish\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3OJlZQ4ChSd",
        "outputId": "f713f4bb-3594-45d7-f567-e022a68665cc"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "builder = StateGraph(State)\n",
        "\n",
        "builder.add_node(\"agent\", agent)\n",
        "builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "builder.add_edge(START, 'agent'),\n",
        "builder.add_conditional_edges(\"agent\",\n",
        "                              tool_needed,\n",
        "                               {\"continue\": \"tools\",\"finish\": END})\n",
        "builder.add_edge(\"tools\", \"agent\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6snqe_OChSd",
        "outputId": "4765f078-2d0a-421b-99fa-205c093de54b"
      },
      "outputs": [],
      "source": [
        "graph = builder.compile()\n",
        "graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUzr8D4cChSd",
        "outputId": "998f8f3e-8b5e-4d54-ef0e-756d9ac43b3a"
      },
      "outputs": [],
      "source": [
        "response = graph.invoke({'messages':[HumanMessage(content=\"오늘 날짜에 태어난 유명인들 조사해서 알려줘.\")]})\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-Amj3waChSd"
      },
      "source": [
        "이번에는 병렬 실행이 가능한 vLLM을 이용해, 리포트 작성 모듈을 구성해 봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkVkmgviChSd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZhYoD0qDaqN"
      },
      "outputs": [],
      "source": [
        "from typing_extensions import TypedDict, Annotated, Literal, List\n",
        "from pydantic import BaseModel, Field\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "\n",
        "# 전체 섹션의 구획: Contents (Chapter List)\n",
        "# Chapter: name, outline\n",
        "class Chapter(BaseModel):\n",
        "    name: str = Field(description=\"챕터의 이름\")\n",
        "    outline: str = Field(description=\"챕터의 주요 내용, 1문장 길이로\")\n",
        "\n",
        "\n",
        "class Contents(BaseModel):\n",
        "    contents: List[Chapter] = Field(description=\"전체 리포트의 섹션 구성\")\n",
        "\n",
        "parser = PydanticOutputParser(pydantic_object=Contents)\n",
        "\n",
        "format_str = parser.get_format_instructions()\n",
        "\n",
        "planner = llm | parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWO6Hp-sDaqN",
        "outputId": "b2f1a9b0-c88f-492f-fecd-c7b3ba688a73"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "with torch.inference_mode():\n",
        "    example = planner.invoke(f\"LLM의 발전 과정에 대한 보고서 구획을 작성해 주세요. \\n{format_str}\")\n",
        "example.contents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iNjCpVeChSh",
        "outputId": "c9ef2648-481c-45c5-ebd2-e91465c57b0c"
      },
      "outputs": [],
      "source": [
        "from langchain.output_parsers import OutputFixingParser\n",
        "\n",
        "# parse 불가능한 출력이 주어지면, llm을 통해 교정하는 파서\n",
        "new_parser = OutputFixingParser.from_llm(parser=parser, llm=llm)\n",
        "\n",
        "planner = llm | new_parser\n",
        "\n",
        "with torch.inference_mode():\n",
        "    example = planner.invoke(f\"LLM의 발전 과정에 대한 3챕터 구성의 보고서 구획을 작성해 주세요. \\n{format_str}\")\n",
        "example.contents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z02IWYAIDaqN"
      },
      "source": [
        "그래프에서 사용할 State를 정의합니다.   \n",
        "\n",
        "이번에는 중간 Writer LLM이 사용할 State를 별도로 만들어 보겠습니다.   \n",
        "이렇게 구성하면 최종 State에서 필요한 부분만 저장할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_MIbKB8DaqN"
      },
      "outputs": [],
      "source": [
        "import operator\n",
        "\n",
        "# reducer 구조: operator.add\n",
        "# 단순 + 연산 구조 (리스트의 + 연산이므로 append)\n",
        "\n",
        "class State(TypedDict):\n",
        "    topic: str\n",
        "    contents: list[Chapter]\n",
        "    completed_sections: Annotated[list, operator.add]\n",
        "    final_report: str\n",
        "\n",
        "\n",
        "# 섹션 Writer가 사용할 State\n",
        "class SubState(TypedDict):\n",
        "    chapter: Chapter\n",
        "    completed_sections: Annotated[list, operator.add]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn9oVR_xDaqO"
      },
      "source": [
        "섹션을 생성하는 노드를 구성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VIpcYLzDaqO"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "def orchestrator(state: State):\n",
        "\n",
        "    prompt = ChatPromptTemplate([\n",
        "        ('system', \"주제에 대한 전문가 수준의 깊이 있는 한국어 보고서를 쓰려고 합니다. 보고서의 섹션 구성과, 각 섹션의 간단한 설명을 작성해 주세요.\"),\n",
        "        ('user', \"\"\"주제: {topic}\n",
        "---\n",
        "\n",
        "{instruction}\n",
        "\n",
        "\"\"\")\n",
        "    ])\n",
        "    chain = prompt.partial(instruction = {format_str}) | planner\n",
        "\n",
        "    # chain 결과물: Contents (contents: List[Chapter])\n",
        "\n",
        "    return {\"contents\": chain.invoke(state).contents}\n",
        "    # state: topic --> topic\n",
        "    # Return: List[Chapter]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IC0OpL4NDaqO"
      },
      "source": [
        "섹션별 내용을 처리하는 노드를 구성합니다.   \n",
        "State에는 각각의 Chapter가 아닌 Chapter의 리스트인 Contents가 들어 있는데요.   \n",
        "\n",
        "`SubState`를 이용해, 각각의 Chapter를 처리하도록 정의하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6S5AwvDbDaqO"
      },
      "outputs": [],
      "source": [
        "def llm_call(state: SubState):\n",
        "    # SubState :  chapter, completed_sections 2개 property\n",
        "\n",
        "    chapter = state['chapter']\n",
        "\n",
        "    prompt = ChatPromptTemplate([\n",
        "        ('system',\"아래 섹션에 대한 상세한 한국어 보고서를 작성하세요.\" ),\n",
        "        ('user', \"섹션 이름과 주제는 다음과 같습니다: {name} --> {outline}\")\n",
        "    ])\n",
        "\n",
        "    chain = prompt | llm\n",
        "    with torch.inference_mode():\n",
        "        return {\"completed_sections\": [chain.invoke({'name':chapter.name, 'outline':chapter.outline}).content]}\n",
        "    # 리스트로 Wrap하는 이유 중요(Reduce Operator 합치기 위해서)\n",
        "\n",
        "\n",
        "# 생성된 섹션별 결과들을 결합\n",
        "def synthesizer(state: State):\n",
        "\n",
        "    completed_sections = state[\"completed_sections\"]\n",
        "\n",
        "    completed_report_sections = \"\\n\\n---\\n\\n\".join(completed_sections)\n",
        "    # join: 전체 리스트 스트링으로 결합하기\n",
        "\n",
        "    return {\"final_report\": completed_report_sections}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gH9uKHSGDaqO"
      },
      "source": [
        "**가장 중요한 부분입니다😁😁**   \n",
        "langgraph의 Send()를 이용하면, 리스트의 원소 개수만큼 서브모듈을 호출할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuqkIRicDaqO"
      },
      "outputs": [],
      "source": [
        "from langgraph.constants import Send\n",
        "\n",
        "def assign_workers(state: State):\n",
        "    # Send: 노드를 호출하며, 값을 전달해 준다\n",
        "    # state['contents']의 개수를 기본적으로 알 수 없는데,\n",
        "    # 이를 통해 개수만큼 llm_call을 생성하여 호출할 수 있음\n",
        "    with torch.inference_mode():\n",
        "        return [Send(\"llm_call\", {\"chapter\": s}) for s in state[\"contents\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEdmwAR1DaqO"
      },
      "source": [
        "그래프를 구성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruDhpkxEDaqO"
      },
      "outputs": [],
      "source": [
        "builder = StateGraph(State)\n",
        "\n",
        "builder.add_node(\"orchestrator\", orchestrator) # 구획 짜고\n",
        "builder.add_node(\"llm_call\", llm_call) # 섹션별 글쓰고\n",
        "builder.add_node(\"synthesizer\", synthesizer) # 합치고\n",
        "\n",
        "\n",
        "builder.add_edge(START, \"orchestrator\")\n",
        "\n",
        "builder.add_conditional_edges(\"orchestrator\", assign_workers, [\"llm_call\"])\n",
        "# assign_workers의 결과에 따라 llm_call을 호출\n",
        "\n",
        "builder.add_edge(\"llm_call\", \"synthesizer\")\n",
        "# 생성된 섹션들은 synthesizer로 이동\n",
        "\n",
        "builder.add_edge(\"synthesizer\", END) # 끝\n",
        "\n",
        "\n",
        "graph = builder.compile()\n",
        "# graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4_faj1ODaqO",
        "outputId": "b29d6cd7-6c1f-46bf-e806-704e16a06c7f"
      },
      "outputs": [],
      "source": [
        "with torch.inference_mode():\n",
        "    for data in graph.stream({\"topic\": \"GPT 1부터 최신 LLM까지의 발전과정 (총 3챕터 길이)\"}, stream_mode='updates'):\n",
        "        print(data)\n",
        "        print('--------------')\n",
        "        # 생성은 병렬적이지만 합치는 순서는 호출한 순서"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkeaSELrDaqO",
        "outputId": "ef743644-20bf-407d-8d12-5c0fea3965d9"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Markdown\n",
        "print(data['synthesizer'][\"final_report\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZp83TwNChSh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

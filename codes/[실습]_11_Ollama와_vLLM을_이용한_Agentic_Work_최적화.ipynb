{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsYrzxX5DaqK"
      },
      "source": [
        "# [ì‹¤ìŠµ] Ollamaê³¼ vLLMì„ ì´ìš©í•œ Agentic Work ë§Œë“¤ê¸°   \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBGUmtU0ChST"
      },
      "source": [
        "## ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "\n",
        "\n",
        "ì´ë²ˆ ì‹¤ìŠµì€ ë¬´ë£Œ ì½”ë©(T4 GPU)ì´ ì•„ë‹Œ ê³ ì„±ëŠ¥ GPU ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì§„í–‰í•©ë‹ˆë‹¤.   \n",
        "ë¬´ë£Œ ì½”ë©ìœ¼ë¡œ ì§„í–‰í•˜ì‹œëŠ” ê²½ìš°, GPU ì„±ëŠ¥ì˜ í•œê³„ë¡œ vLLM ì‹¤í–‰ì´ ì–´ë µìŠµë‹ˆë‹¤.\n",
        "\n",
        "<br><br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lPVjc9mChST",
        "outputId": "531f969b-be62-4110-ed22-d3180797a843",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!pip install transformers bitsandbytes openai langchain_openai -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1JC3bK9DaqL",
        "outputId": "d3e122a4-4b57-4818-d530-0fd63a205973"
      },
      "outputs": [],
      "source": [
        "!pip install langchain_experimental langgraph langchain langchain_community langchain_huggingface dotenv -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvXDl6a-ChSU",
        "outputId": "fd5b1af6-fa00-441f-803c-7fa6c5b88e85"
      },
      "outputs": [],
      "source": [
        "# Pytorch ë²„ì „ í˜¸í™˜\n",
        "# vllm 0.10 : Pytorch 2.7.1\n",
        "# vllm 0.9.x == 2.7.0\n",
        "# vllm 0.8.x == 2.6.0\n",
        "# vllm 0.6.x == 2.5.1\n",
        "\n",
        "!pip install pyzmq flashinfer-python==0.2.10 vllm==0.10.1 -q\n",
        "!pip install  flashinfer-python pyzmq vllm -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-K10yruCvnu"
      },
      "source": [
        "ì„¤ì¹˜ í›„ì—ëŠ” ì„¸ì…˜ì„ ì¬ì‹œì‘í•´ ì£¼ì„¸ìš”."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-Q0yOegChSU",
        "outputId": "3dbaf4e0-c1ca-4268-aaf9-6f39fd3614f5",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Flash Attention: ë¦¬ëˆ…ìŠ¤ ì „ìš© ì„¤ì¹˜ë°©ë²•\n",
        "# Windows ì„¤ì¹˜ëŠ” https://github.com/kingbri1/flash-attention/releases ì°¸ê³ \n",
        "!pip install flash-attn --no-build-isolation -q\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwLiGVjiChSV"
      },
      "source": [
        "ì„¤ì¹˜í•  ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ë§ìœ¼ë¯€ë¡œ, ê°€ê¸‰ì  ì„¤ì¹˜ í›„ ì„¸ì…˜ ì¬ì‹œì‘ì„ ìˆ˜í–‰í•´ ì£¼ì„¸ìš”."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfDjPosVChSV"
      },
      "source": [
        "vLLMì€ ìºì‹±ì„ í†µí•´ íš¨ê³¼ì ì¸ ì¶”ë¡ ê³¼ ë™ì‹œ ì‹¤í–‰ì„ ì§€ì›í•©ë‹ˆë‹¤.   \n",
        "ì•„ë˜ ì½”ë“œë¥¼ í„°ë¯¸ë„ì—ì„œ ì‹¤í–‰í•˜ì„¸ìš”."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIRl33XXChSV"
      },
      "source": [
        "```\n",
        "vllm serve unsloth/gemma-3-12b-it-bnb-4bit --dtype auto --max_model_len 32768 --quantization bitsandbytes --served_model_name gemma3 --max_num_seqs 1\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uiptF1yChSW"
      },
      "source": [
        "ì–´ëŠ ì •ë„ ì‹œê°„ì´ ì§€ë‚˜ë©´, vLLM ì„œë¹™ì´ ì™„ë£Œë©ë‹ˆë‹¤.   \n",
        "vllm Serveê°€ ì •ìƒì ìœ¼ë¡œ ì™„ë£Œë˜ë©´, 8000ë²ˆ í¬íŠ¸ì—ì„œ ëª¨ë¸ì„ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7RLw4ynDEqa"
      },
      "outputs": [],
      "source": [
        "# ëª¨ë¸ ì£¼ì†Œ í™•ì¸\n",
        "!curl http://0.0.0.0:8000/v1/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huewOgXWChSW"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI, OpenAI\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    base_url=\"http://localhost:8000/v1\",\n",
        "    api_key=\"token-abc123\",\n",
        "    model=\"gemma3\",\n",
        "    temperature=0.5,\n",
        "    max_tokens=1024\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKU1bgY9ChSW",
        "outputId": "aba1396f-2e7f-406b-bcbb-8d67d590a0e3"
      },
      "outputs": [],
      "source": [
        "for s in llm.stream(\"vLLMì´ ë­ì•¼?\"):\n",
        "    print(s.content, end='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXqmbS-lChSW",
        "outputId": "d6187fef-523f-445b-da64-ae2452328047"
      },
      "outputs": [],
      "source": [
        "def convert_chat(messages, add_generation_prompt = True):\n",
        "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=add_generation_prompt)\n",
        "\n",
        "example = [{'role':'user', 'content':'ì•ˆë…•'}]\n",
        "convert_chat(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_P4V-vgChSW"
      },
      "source": [
        "## 2. HuggingFace LLMê³¼ íˆ´ ì—°ë™í•˜ê¸°"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwiBEnDuChSW"
      },
      "source": [
        "ë¨¼ì € íˆ´ì„ ì„¤ì •í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIOEJGlIChSW",
        "outputId": "68a464c6-3f6c-4bd4-a350-ca836db149c0"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_core.tools import tool\n",
        "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
        "from datetime import datetime\n",
        "\n",
        "tavily_search = TavilySearchResults(\n",
        "    max_results=3)\n",
        "\n",
        "repl_tool = PythonREPLTool()\n",
        "repl_tool.invoke(\"for i in range(10): print(i)\")\n",
        "\n",
        "\n",
        "@tool\n",
        "def current_date() -> str:\n",
        "    \"í˜„ì¬ ë‚ ì§œë¥¼ %y-%m-%d í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.\"\n",
        "    return datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "\n",
        "\n",
        "tools = [tavily_search, repl_tool, current_date]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY-T4e9hChSW"
      },
      "source": [
        "vLLMì—ì„œëŠ” Tool Bindingì„ ìˆ˜í–‰í•˜ë ¤ë©´   \n",
        "--enable-auto-tool-choice ë¥¼ ì¶”ê°€í•œ ë’¤, ì „ìš© íŒŒì„œë¥¼ ì—°ê²°í•´ì•¼ í•©ë‹ˆë‹¤.   \n",
        "Llama, Qwen, Hermes ë“±ì˜ ëª¨ë¸ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.   \n",
        "https://docs.vllm.ai/en/stable/features/tool_calling.htm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcZfOJvTChSX",
        "outputId": "fef8a96b-f6d8-4ac2-db98-3fe102ab580a"
      },
      "outputs": [],
      "source": [
        "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
        "\n",
        "\n",
        "convert_to_openai_tool(tools[2])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Av22_b_eChSX"
      },
      "source": [
        "íˆ´ ì„¤ëª…ì´ ë‹´ê¸´ ë¬¸ìì—´ì„ êµ¬ì„±í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFAAolWtChSX",
        "outputId": "794c42db-e9af-4108-90f8-1325ea4db5f5"
      },
      "outputs": [],
      "source": [
        "tool_desc = str('\\n---\\n'.join([str(convert_to_openai_tool(tool)) for tool in tools]))\n",
        "print(tool_desc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxPl5cfWChSX"
      },
      "source": [
        "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.   \n",
        "ì„±ëŠ¥ì— ë§¤ìš° ì¤‘ìš”í•œ ì˜í–¥ì„ ë¯¸ì¹˜ë¯€ë¡œ, ì˜ì–´ë¡œ ì‘ì„±í–ˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLF16NzcChSX"
      },
      "outputs": [],
      "source": [
        "system_prompt = f'''\n",
        "You are a helpful assistant with tools below.\n",
        "You can decide whether to invoke any functions or not.\n",
        "If you decide to use any of tools.\n",
        "print name and required parameters of the tool within a json blob correctly.\n",
        "For python code, Return the object as a raw dictionary, without escaping quotes or newlines.\n",
        "\n",
        "for tool use: wrap your output within ```tool_code```.\n",
        "\n",
        "Example:\n",
        "```tool_code\n",
        "{{\"name\":'name of tool', \"arguments\":{{List of apparent argument and parameters}}}}\n",
        "```\n",
        "\n",
        "When the output of the tool is provided, it will be wrapped within ``tool_output```\n",
        "Answer accordingly from the result of the tool output.\n",
        "\n",
        "The question might need some sequential, multiple tool execution.\n",
        "Think Step by Step.\n",
        "\n",
        "The following tools are available:\n",
        "{tool_desc}'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGNheWMdChSX"
      },
      "source": [
        "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— ë“¤ì–´ê°€ì•¼ í•˜ëŠ” ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
        "- Tool Format\n",
        "- Tool Call Format\n",
        "- Tool Result Format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6hbNYp0ChSY"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, ToolMessage\n",
        "\n",
        "\n",
        "messages = [SystemMessage(system_prompt),\n",
        "            HumanMessage('ì˜¤ëŠ˜ ë‚ ì§œê°€ ë©°ì¹ ì´ë‹ˆ?')]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcxyBL1bChSY",
        "outputId": "53dd0fab-5a69-4966-aa7b-819ada117091"
      },
      "outputs": [],
      "source": [
        "response = llm.invoke(messages)\n",
        "messages.append(response)\n",
        "response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHphfjKGChSY",
        "outputId": "ae9cf9c4-35bd-4ae4-de1c-7b33608b0a70"
      },
      "outputs": [],
      "source": [
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hf3ikmr5ChSY"
      },
      "source": [
        "tool_codeë¥¼ ë°›ì•˜ìœ¼ë‹ˆ, í•´ë‹¹ ë‚´ìš©ì„ íŒŒì‹±í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Mg_NaBOChSY",
        "outputId": "e09d54bf-c3ca-450a-c116-1aee57844edf"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "\n",
        "\n",
        "def parse_tool(text):\n",
        "    try:\n",
        "        text = text.split('```tool_code\\n')[1].split('\\n```')[0]\n",
        "        # tool_codeë¡œ wrapëœ ì¤‘ê°„ ì½”ë“œ ì¶”ì¶œ\n",
        "\n",
        "        parsed = ast.literal_eval(text)\n",
        "        # Dict í˜•íƒœì˜ ê°’ ë³€í™˜ (json loadì™€ ìœ ì‚¬)\n",
        "\n",
        "        name = parsed.get('name')\n",
        "        arguments = parsed.get('arguments', {})\n",
        "        # nameê³¼ argument return\n",
        "        return {'name':name, 'arguments':arguments}\n",
        "    except (ValueError, SyntaxError):\n",
        "        return None\n",
        "\n",
        "result = parse_tool(response.content)\n",
        "name,arguments = result['name'], result['arguments']\n",
        "name, arguments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaTQK_seChSZ"
      },
      "source": [
        "íˆ´ ì‹¤í–‰ì„ ì—°ê²°í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plDwh8fNChSZ",
        "outputId": "7bd96f8b-a81a-41c7-ed88-9858f1a7214e"
      },
      "outputs": [],
      "source": [
        "# íˆ´ ì´ë¦„ê³¼ íˆ´ ì—°ê²°\n",
        "tool_dict = {tool.name: tool for tool in tools}\n",
        "\n",
        "def execute_tool(name, arguments):\n",
        "    # íˆ´ ì‹¤í–‰í•œ ë’¤ tool_outputìœ¼ë¡œ wrap\n",
        "    result = f'''```tool_output\n",
        "{tool_dict[name].invoke(arguments)}\n",
        "```'''\n",
        "    return result\n",
        "\n",
        "tool_result = execute_tool(**parse_tool(response.content))\n",
        "\n",
        "print(tool_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vALn3uIChSZ",
        "outputId": "33d48f10-c301-4709-9a28-5da925fbcc59"
      },
      "outputs": [],
      "source": [
        "messages.append(HumanMessage(tool_result))\n",
        "messages[1:]\n",
        "# ì§ˆë¬¸ + Tool ìš”ì²­ + Tool ê²°ê³¼"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-00rE8xChSc",
        "outputId": "63010bcb-b25f-4c1a-8bbf-d783ec3af7b4"
      },
      "outputs": [],
      "source": [
        "# ê²°ê³¼ í•´ì„\n",
        "response = llm.invoke(messages)\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuoZLPE-ChSc",
        "outputId": "f74077dd-1109-4c04-941e-15c3afe3345e"
      },
      "outputs": [],
      "source": [
        "messages = [SystemMessage(system_prompt),\n",
        "            HumanMessage('2025ë…„ 4ì›” ë°œí‘œëœ GPT-4.1 ëª¨ë¸ì´ ì–´ë–¤ ëª¨ë¸ì´ì•¼? í•œêµ­ì–´ë¡œ ì„¤ëª…í•´ì¤˜.')]\n",
        "response = llm.invoke(messages)\n",
        "messages.append(response)\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4UwJtbSChSc",
        "outputId": "050fa880-3c6a-4132-da28-d012773a7389"
      },
      "outputs": [],
      "source": [
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVIJ0E3gChSc",
        "outputId": "c96088d6-c498-4269-fe59-d71584106b1b"
      },
      "outputs": [],
      "source": [
        "tool_result = execute_tool(**parse_tool(response.content))\n",
        "tool_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbPDgbu7ChSd",
        "outputId": "04963625-442d-433e-f51e-1f717b666b2c"
      },
      "outputs": [],
      "source": [
        "messages[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G636d4DkChSd",
        "outputId": "420c583c-2335-4bfe-dcc1-81d7aca04675"
      },
      "outputs": [],
      "source": [
        "messages.append(HumanMessage(tool_result))\n",
        "response = llm.invoke(messages)\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zM4gli-1ChSd"
      },
      "source": [
        "ì¼ë°˜ì ì¸ ì…ì¶œë ¥ ê´€ê³„ì˜ íˆ´ì€ ì´ì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ê°„ë‹¨í•˜ê²Œ êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.   \n",
        "(ë§Œì•½, Python_REPLê³¼ ê°™ì´ argumentê°€ ë³µì¡í•œ íˆ´ì„ ìˆ˜í–‰í•˜ëŠ” ê²½ìš°ì—ëŠ”   \n",
        "ë³„ë„ì˜ í•¨ìˆ˜ë¡œ ë³€í™˜í•˜ê±°ë‚˜ ê²°ê³¼ë¬¼ì„ ìˆ˜ì •í•˜ëŠ” ì‘ì—…ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qx8blhUwChSd"
      },
      "source": [
        "í•´ë‹¹ êµ¬í˜„ì„ í†µí•´, ReAct Agent êµ¬ì¡°ë¥¼ ë§Œë“¤ì–´ ë³´ê² ìŠµë‹ˆë‹¤.    \n",
        "bind_toolsê°€ ì—†ê¸° ë•Œë¬¸ì—, ê¸°ì¡´ì˜ Tool Messageë¥¼ ì‚¬ìš©í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJuCpEQ5ChSd"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages : Annotated[list, add_messages]   # ë©”ì‹œì§€ ë§¥ë½ì„ ì €ì¥í•˜ëŠ” ë¦¬ìŠ¤íŠ¸\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9bMAts4ChSd"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import ToolMessage\n",
        "\n",
        "tool_list = {tool.name: tool for tool in tools}\n",
        "# tool ëª©ë¡ dictë¡œ ìƒì„±\n",
        "\n",
        "def tool_node(state):\n",
        "    tool_outputs = []\n",
        "    tool_call_msgs = state['messages'][-1]\n",
        "    # ë§ˆì§€ë§‰ ë©”ì‹œì§€: íˆ´ ì½œë§ ë©”ì‹œì§€\n",
        "    if '```tool_code' in tool_call_msgs.content:\n",
        "        tool_result = execute_tool(**parse_tool(tool_call_msgs.content))\n",
        "        # tool ì‹¤í–‰ ê²°ê³¼ ì–»ê¸° (ê²°ê³¼ëŠ” ```tool_output```)\n",
        "        tool_outputs.append(HumanMessage(tool_result))\n",
        "\n",
        "    return {'messages': tool_outputs}\n",
        "\n",
        "def agent(state):\n",
        "    system_prompt = SystemMessage(f'''\n",
        "You are a helpful assistant with tools below.\n",
        "You can decide whether to invoke any functions or not.\n",
        "If you decide to use any of tools.\n",
        "print name and required parameters of the tool within a json blob correctly.\n",
        "For python code, Return the object as a raw dictionary, without escaping quotes or newlines.\n",
        "\n",
        "for tool use: wrap your output within ```tool_code```.\n",
        "\n",
        "Example:\n",
        "```tool_code\n",
        "{{\"name\":'name of tool', \"arguments\":{{List of apparent argument and parameters}}}}\n",
        "```\n",
        "\n",
        "When the output of the tool is provided, it will be wrapped within ``tool_output```\n",
        "Answer accordingly from the result of the tool output.\n",
        "\n",
        "The question might need some sequential, multiple tool execution.\n",
        "Think Step by Step.\n",
        "\n",
        "The following tools are available:\n",
        "{tool_desc}\n",
        "\n",
        "\n",
        "Answer in Korean.''')\n",
        "\n",
        "\n",
        "    response = llm.invoke([system_prompt] + state[\"messages\"])\n",
        "    return {'messages': response}\n",
        "\n",
        "def tool_needed(state):\n",
        "\n",
        "    last_msg = state['messages'][-1]\n",
        "    if '```tool_code' in last_msg.content: # íˆ´ ì½œë§ì´ í•„ìš”í•˜ë©´\n",
        "        return \"continue\"\n",
        "    else:\n",
        "        return \"finish\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3OJlZQ4ChSd",
        "outputId": "f713f4bb-3594-45d7-f567-e022a68665cc"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "builder = StateGraph(State)\n",
        "\n",
        "builder.add_node(\"agent\", agent)\n",
        "builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "builder.add_edge(START, 'agent'),\n",
        "builder.add_conditional_edges(\"agent\",\n",
        "                              tool_needed,\n",
        "                               {\"continue\": \"tools\",\"finish\": END})\n",
        "builder.add_edge(\"tools\", \"agent\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6snqe_OChSd",
        "outputId": "4765f078-2d0a-421b-99fa-205c093de54b"
      },
      "outputs": [],
      "source": [
        "graph = builder.compile()\n",
        "graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUzr8D4cChSd",
        "outputId": "998f8f3e-8b5e-4d54-ef0e-756d9ac43b3a"
      },
      "outputs": [],
      "source": [
        "response = graph.invoke({'messages':[HumanMessage(content=\"ì˜¤ëŠ˜ ë‚ ì§œì— íƒœì–´ë‚œ ìœ ëª…ì¸ë“¤ ì¡°ì‚¬í•´ì„œ ì•Œë ¤ì¤˜.\")]})\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-Amj3waChSd"
      },
      "source": [
        "ì´ë²ˆì—ëŠ” ë³‘ë ¬ ì‹¤í–‰ì´ ê°€ëŠ¥í•œ vLLMì„ ì´ìš©í•´, ë¦¬í¬íŠ¸ ì‘ì„± ëª¨ë“ˆì„ êµ¬ì„±í•´ ë´…ì‹œë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkVkmgviChSd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZhYoD0qDaqN"
      },
      "outputs": [],
      "source": [
        "from typing_extensions import TypedDict, Annotated, Literal, List\n",
        "from pydantic import BaseModel, Field\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "\n",
        "# ì „ì²´ ì„¹ì…˜ì˜ êµ¬íš: Contents (Chapter List)\n",
        "# Chapter: name, outline\n",
        "class Chapter(BaseModel):\n",
        "    name: str = Field(description=\"ì±•í„°ì˜ ì´ë¦„\")\n",
        "    outline: str = Field(description=\"ì±•í„°ì˜ ì£¼ìš” ë‚´ìš©, 1ë¬¸ì¥ ê¸¸ì´ë¡œ\")\n",
        "\n",
        "\n",
        "class Contents(BaseModel):\n",
        "    contents: List[Chapter] = Field(description=\"ì „ì²´ ë¦¬í¬íŠ¸ì˜ ì„¹ì…˜ êµ¬ì„±\")\n",
        "\n",
        "parser = PydanticOutputParser(pydantic_object=Contents)\n",
        "\n",
        "format_str = parser.get_format_instructions()\n",
        "\n",
        "planner = llm | parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWO6Hp-sDaqN",
        "outputId": "b2f1a9b0-c88f-492f-fecd-c7b3ba688a73"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "with torch.inference_mode():\n",
        "    example = planner.invoke(f\"LLMì˜ ë°œì „ ê³¼ì •ì— ëŒ€í•œ ë³´ê³ ì„œ êµ¬íšì„ ì‘ì„±í•´ ì£¼ì„¸ìš”. \\n{format_str}\")\n",
        "example.contents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iNjCpVeChSh",
        "outputId": "c9ef2648-481c-45c5-ebd2-e91465c57b0c"
      },
      "outputs": [],
      "source": [
        "from langchain.output_parsers import OutputFixingParser\n",
        "\n",
        "# parse ë¶ˆê°€ëŠ¥í•œ ì¶œë ¥ì´ ì£¼ì–´ì§€ë©´, llmì„ í†µí•´ êµì •í•˜ëŠ” íŒŒì„œ\n",
        "new_parser = OutputFixingParser.from_llm(parser=parser, llm=llm)\n",
        "\n",
        "planner = llm | new_parser\n",
        "\n",
        "with torch.inference_mode():\n",
        "    example = planner.invoke(f\"LLMì˜ ë°œì „ ê³¼ì •ì— ëŒ€í•œ 3ì±•í„° êµ¬ì„±ì˜ ë³´ê³ ì„œ êµ¬íšì„ ì‘ì„±í•´ ì£¼ì„¸ìš”. \\n{format_str}\")\n",
        "example.contents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z02IWYAIDaqN"
      },
      "source": [
        "ê·¸ë˜í”„ì—ì„œ ì‚¬ìš©í•  Stateë¥¼ ì •ì˜í•©ë‹ˆë‹¤.   \n",
        "\n",
        "ì´ë²ˆì—ëŠ” ì¤‘ê°„ Writer LLMì´ ì‚¬ìš©í•  Stateë¥¼ ë³„ë„ë¡œ ë§Œë“¤ì–´ ë³´ê² ìŠµë‹ˆë‹¤.   \n",
        "ì´ë ‡ê²Œ êµ¬ì„±í•˜ë©´ ìµœì¢… Stateì—ì„œ í•„ìš”í•œ ë¶€ë¶„ë§Œ ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_MIbKB8DaqN"
      },
      "outputs": [],
      "source": [
        "import operator\n",
        "\n",
        "# reducer êµ¬ì¡°: operator.add\n",
        "# ë‹¨ìˆœ + ì—°ì‚° êµ¬ì¡° (ë¦¬ìŠ¤íŠ¸ì˜ + ì—°ì‚°ì´ë¯€ë¡œ append)\n",
        "\n",
        "class State(TypedDict):\n",
        "    topic: str\n",
        "    contents: list[Chapter]\n",
        "    completed_sections: Annotated[list, operator.add]\n",
        "    final_report: str\n",
        "\n",
        "\n",
        "# ì„¹ì…˜ Writerê°€ ì‚¬ìš©í•  State\n",
        "class SubState(TypedDict):\n",
        "    chapter: Chapter\n",
        "    completed_sections: Annotated[list, operator.add]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn9oVR_xDaqO"
      },
      "source": [
        "ì„¹ì…˜ì„ ìƒì„±í•˜ëŠ” ë…¸ë“œë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VIpcYLzDaqO"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "def orchestrator(state: State):\n",
        "\n",
        "    prompt = ChatPromptTemplate([\n",
        "        ('system', \"ì£¼ì œì— ëŒ€í•œ ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ ê¹Šì´ ìˆëŠ” í•œêµ­ì–´ ë³´ê³ ì„œë¥¼ ì“°ë ¤ê³  í•©ë‹ˆë‹¤. ë³´ê³ ì„œì˜ ì„¹ì…˜ êµ¬ì„±ê³¼, ê° ì„¹ì…˜ì˜ ê°„ë‹¨í•œ ì„¤ëª…ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.\"),\n",
        "        ('user', \"\"\"ì£¼ì œ: {topic}\n",
        "---\n",
        "\n",
        "{instruction}\n",
        "\n",
        "\"\"\")\n",
        "    ])\n",
        "    chain = prompt.partial(instruction = {format_str}) | planner\n",
        "\n",
        "    # chain ê²°ê³¼ë¬¼: Contents (contents: List[Chapter])\n",
        "\n",
        "    return {\"contents\": chain.invoke(state).contents}\n",
        "    # state: topic --> topic\n",
        "    # Return: List[Chapter]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IC0OpL4NDaqO"
      },
      "source": [
        "ì„¹ì…˜ë³„ ë‚´ìš©ì„ ì²˜ë¦¬í•˜ëŠ” ë…¸ë“œë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.   \n",
        "Stateì—ëŠ” ê°ê°ì˜ Chapterê°€ ì•„ë‹Œ Chapterì˜ ë¦¬ìŠ¤íŠ¸ì¸ Contentsê°€ ë“¤ì–´ ìˆëŠ”ë°ìš”.   \n",
        "\n",
        "`SubState`ë¥¼ ì´ìš©í•´, ê°ê°ì˜ Chapterë¥¼ ì²˜ë¦¬í•˜ë„ë¡ ì •ì˜í•˜ê² ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6S5AwvDbDaqO"
      },
      "outputs": [],
      "source": [
        "def llm_call(state: SubState):\n",
        "    # SubState :  chapter, completed_sections 2ê°œ property\n",
        "\n",
        "    chapter = state['chapter']\n",
        "\n",
        "    prompt = ChatPromptTemplate([\n",
        "        ('system',\"ì•„ë˜ ì„¹ì…˜ì— ëŒ€í•œ ìƒì„¸í•œ í•œêµ­ì–´ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.\" ),\n",
        "        ('user', \"ì„¹ì…˜ ì´ë¦„ê³¼ ì£¼ì œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: {name} --> {outline}\")\n",
        "    ])\n",
        "\n",
        "    chain = prompt | llm\n",
        "    with torch.inference_mode():\n",
        "        return {\"completed_sections\": [chain.invoke({'name':chapter.name, 'outline':chapter.outline}).content]}\n",
        "    # ë¦¬ìŠ¤íŠ¸ë¡œ Wrapí•˜ëŠ” ì´ìœ  ì¤‘ìš”(Reduce Operator í•©ì¹˜ê¸° ìœ„í•´ì„œ)\n",
        "\n",
        "\n",
        "# ìƒì„±ëœ ì„¹ì…˜ë³„ ê²°ê³¼ë“¤ì„ ê²°í•©\n",
        "def synthesizer(state: State):\n",
        "\n",
        "    completed_sections = state[\"completed_sections\"]\n",
        "\n",
        "    completed_report_sections = \"\\n\\n---\\n\\n\".join(completed_sections)\n",
        "    # join: ì „ì²´ ë¦¬ìŠ¤íŠ¸ ìŠ¤íŠ¸ë§ìœ¼ë¡œ ê²°í•©í•˜ê¸°\n",
        "\n",
        "    return {\"final_report\": completed_report_sections}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gH9uKHSGDaqO"
      },
      "source": [
        "**ê°€ì¥ ì¤‘ìš”í•œ ë¶€ë¶„ì…ë‹ˆë‹¤ğŸ˜ğŸ˜**   \n",
        "langgraphì˜ Send()ë¥¼ ì´ìš©í•˜ë©´, ë¦¬ìŠ¤íŠ¸ì˜ ì›ì†Œ ê°œìˆ˜ë§Œí¼ ì„œë¸Œëª¨ë“ˆì„ í˜¸ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuqkIRicDaqO"
      },
      "outputs": [],
      "source": [
        "from langgraph.constants import Send\n",
        "\n",
        "def assign_workers(state: State):\n",
        "    # Send: ë…¸ë“œë¥¼ í˜¸ì¶œí•˜ë©°, ê°’ì„ ì „ë‹¬í•´ ì¤€ë‹¤\n",
        "    # state['contents']ì˜ ê°œìˆ˜ë¥¼ ê¸°ë³¸ì ìœ¼ë¡œ ì•Œ ìˆ˜ ì—†ëŠ”ë°,\n",
        "    # ì´ë¥¼ í†µí•´ ê°œìˆ˜ë§Œí¼ llm_callì„ ìƒì„±í•˜ì—¬ í˜¸ì¶œí•  ìˆ˜ ìˆìŒ\n",
        "    with torch.inference_mode():\n",
        "        return [Send(\"llm_call\", {\"chapter\": s}) for s in state[\"contents\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEdmwAR1DaqO"
      },
      "source": [
        "ê·¸ë˜í”„ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruDhpkxEDaqO"
      },
      "outputs": [],
      "source": [
        "builder = StateGraph(State)\n",
        "\n",
        "builder.add_node(\"orchestrator\", orchestrator) # êµ¬íš ì§œê³ \n",
        "builder.add_node(\"llm_call\", llm_call) # ì„¹ì…˜ë³„ ê¸€ì“°ê³ \n",
        "builder.add_node(\"synthesizer\", synthesizer) # í•©ì¹˜ê³ \n",
        "\n",
        "\n",
        "builder.add_edge(START, \"orchestrator\")\n",
        "\n",
        "builder.add_conditional_edges(\"orchestrator\", assign_workers, [\"llm_call\"])\n",
        "# assign_workersì˜ ê²°ê³¼ì— ë”°ë¼ llm_callì„ í˜¸ì¶œ\n",
        "\n",
        "builder.add_edge(\"llm_call\", \"synthesizer\")\n",
        "# ìƒì„±ëœ ì„¹ì…˜ë“¤ì€ synthesizerë¡œ ì´ë™\n",
        "\n",
        "builder.add_edge(\"synthesizer\", END) # ë\n",
        "\n",
        "\n",
        "graph = builder.compile()\n",
        "# graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4_faj1ODaqO",
        "outputId": "b29d6cd7-6c1f-46bf-e806-704e16a06c7f"
      },
      "outputs": [],
      "source": [
        "with torch.inference_mode():\n",
        "    for data in graph.stream({\"topic\": \"GPT 1ë¶€í„° ìµœì‹  LLMê¹Œì§€ì˜ ë°œì „ê³¼ì • (ì´ 3ì±•í„° ê¸¸ì´)\"}, stream_mode='updates'):\n",
        "        print(data)\n",
        "        print('--------------')\n",
        "        # ìƒì„±ì€ ë³‘ë ¬ì ì´ì§€ë§Œ í•©ì¹˜ëŠ” ìˆœì„œëŠ” í˜¸ì¶œí•œ ìˆœì„œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkeaSELrDaqO",
        "outputId": "ef743644-20bf-407d-8d12-5c0fea3965d9"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Markdown\n",
        "print(data['synthesizer'][\"final_report\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZp83TwNChSh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

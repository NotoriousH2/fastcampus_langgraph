{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsYrzxX5DaqK"
      },
      "source": [
        "# [실습] HuggingFace 공개 LLM을 활용한 Agentic Work 만들기\n",
        "\n",
        "HuggingFace에 게시된 공개 모델들의 경우, `bind_tools`와 같은 기능들이 제대로 연동되지 않거나,   \n",
        "자체적으로 Tool Calling 기능이 없는 경우가 많습니다.   \n",
        "\n",
        "이를 해결하기 위해, Tool Calling 방법을 직접 구성하고 Agentic Work를 구현해 보겠습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiuZNYFq14aN"
      },
      "source": [
        "## 라이브러리 설치\n",
        "\n",
        "\n",
        "이번 실습은 무료 코랩(T4 GPU)이 아닌 고성능 GPU 라이브러리에서 진행합니다.   \n",
        "무료 코랩으로 진행하시는 경우, GPU 성능의 한계로 실행이 오래 걸릴 수 있습니다.   \n",
        "(아래 코드에서, **T4 사용시 해제** 부분을 참고하세요!)\n",
        "\n",
        "<br><br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Udtv--pI14aN",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!pip install transformers bitsandbytes accelerate -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1JC3bK9DaqL"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade langchain_experimental langgraph langchain langchain_community langchain_huggingface dotenv -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1eMoe-G14aO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gYEfMXn14aO",
        "outputId": "276917bd-6172-4f0f-f252-d3126f7288fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Flash Attention: 리눅스 전용 설치방법\n",
        "# 무료 코랩(T4)에서는 설치 X\n",
        "\n",
        "# Windows 설치는 https://github.com/kingbri1/flash-attention/releases 참고\n",
        "!pip install flash-attn --no-build-isolation -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DtuBpmP14aO"
      },
      "source": [
        "설치할 라이브러리가 많으므로, 가급적 설치 후 세션 재시작을 수행해 주세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBhW7vFc14aO"
      },
      "source": [
        "허깅페이스 토큰을 인증합니다.    \n",
        "\n",
        "https://huggingface.co/settings/tokens 에서 Read 권한 토큰을 발급받습니다.   \n",
        "아래 코드에서는, env 파일에 `HF_READ_TOKEN`으로 저장했습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKRxTZ5n14aO"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVFaB-AA14aO",
        "outputId": "047ac881-646b-4d8f-93d6-a848a719e239"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "load_dotenv('env') #.env 파일인 경우 .env로 변경"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "siIa3Voy14aO",
        "outputId": "04a4ff69-9a66-4687-fb4f-b3733570a545"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['HF_TOKEN']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJEt8RpX14aO",
        "outputId": "0194c900-3ae1-479a-f954-2e64d8a5d1e8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
            "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "# 허깅페이스 토큰 로그인\n",
        "login(token=os.environ['HF_TOKEN'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d__aPk5_14aO"
      },
      "source": [
        "### 1. Gemma 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrzEKTP514aP"
      },
      "source": [
        "불러올 모델은 구글의 Gemma-3-12b-it 입니다.   \n",
        "실제 크기는 24GB 정도이지만, 4Bit 양자화를 통해 8~9GB 크기로 로드합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6QY8RHv14aP",
        "outputId": "a5be359d-d34b-48ba-c51e-99da2394aa45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# Model ID: google/gemma-3-12b-it\n"
          ]
        }
      ],
      "source": [
        "model_id = \"google/gemma-3-12b-it\"\n",
        "print(f\"# Model ID: {model_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qE5W5Bat14aP"
      },
      "source": [
        "트랜스포머 라이브러리를 이용해 모델을 불러옵니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z592IDzIDaqM"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "# 양자화 옵션: 4비트\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    # nf4 양자화\n",
        "\n",
        "    bnb_4bit_compute_dtype=\"bfloat16\", # 계산시에는 기존 자료형 bf16 사용\n",
        "    bnb_4bit_use_double_quant=True # 이중 양자화\n",
        ")\n",
        "\n",
        "\n",
        "# 토크나이저와 모델 로드\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "\n",
        "    torch_dtype=torch.bfloat16, # Gemma 3은 bf16 모델입니다!\n",
        "    device_map={\"\":0},  # 0번 GPU에 할당\n",
        "\n",
        "    quantization_config = quantization_config,\n",
        "    # 양자화 불필요시 제거\n",
        "\n",
        "    attn_implementation = 'eager'\n",
        "    # flash attention 설정\n",
        "    # Gemma Series가 아닌 경우 'eager'를 \"flash_attention_2\" 로 변경\n",
        "    # 미지원/사용하지 않는 경우 제거 (T4 등의 구버전 GPU 등)\n",
        "\n",
        ")\n",
        "\n",
        "# if hasattr(model, 'language_model'):\n",
        "#     model = model.language_model\n",
        "# Multimodal 모델의 경우, Language Model만 선택할 수도 있으나\n",
        "# 비전 모델의 사이즈가 매우 작기 때문에 큰 차이는 없음\n",
        "\n",
        "# Train X (eval)\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRYqBlyQ14aP"
      },
      "source": [
        "모델을 불러온 뒤에는 Text-Generation 파이프라인을 구성합니다.   \n",
        "이 때, 적절한 샘플링 파라미터를 설정하기 위해 모델 홈페이지를 참고할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gFbGhuK14aP",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace\n",
        "\n",
        "# Sampling 파라미터 설정: https://huggingface.co/google/gemma-3-12b-it\n",
        "# 홈페이지 권장 사양이지만, 적절하게 바꿔도 됨\n",
        "gen_config = dict(\n",
        "    do_sample=True,\n",
        "    max_new_tokens=2048,\n",
        "    repetition_penalty = 1.05,\n",
        "\n",
        "    temperature = 1.0,\n",
        "    top_p = 0.95,\n",
        "    top_k = 64\n",
        ")\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False,\n",
        "    # return_full_text=True <-- 입력 프롬프트를 포함한 전체 출력하기\n",
        "    **gen_config\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a-z0Pbt14aP"
      },
      "source": [
        "모델의 토크나이저를 확인하여, 채팅 템플릿 구성을 확인합니다.   \n",
        "전체를 이해할 필요는 없지만, Tool이나 Function이 있는지 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRHzfqUk14aP"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.chat_template)\n",
        "# Tool 미지원 😣"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqWVrzLe14aP"
      },
      "source": [
        "모델을 랭체인과 연동합니다.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZsarwsm14aP"
      },
      "outputs": [],
      "source": [
        "base_llm = HuggingFacePipeline(pipeline=pipe, pipeline_kwargs=gen_config)\n",
        "# base_llm: 입력 전 Tokenizer로 템플릿 변환 필수, 스트리밍 가능\n",
        "\n",
        "llm = ChatHuggingFace(llm=base_llm, tokenizer=tokenizer)\n",
        "# llm: 자동 템플릿 변환(ChatGoogleGenAI와 동일), 스트리밍 불가"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPs0pT9z14aP"
      },
      "outputs": [],
      "source": [
        "def convert_chat(messages, add_generation_prompt = True):\n",
        "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=add_generation_prompt)\n",
        "\n",
        "example = [{'role':'user', 'content':'안녕'}]\n",
        "convert_chat(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tf93bsuV14aQ"
      },
      "outputs": [],
      "source": [
        "for s in base_llm.stream(convert_chat(example)):\n",
        "    print(s, end='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lG9dLxzP14aQ"
      },
      "outputs": [],
      "source": [
        "llm.invoke(\"안녕?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwnCxMeP14aQ"
      },
      "source": [
        "## 2. HuggingFace LLM과 툴 연동하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc0lmLMh14aQ"
      },
      "source": [
        "먼저 툴을 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxxyvxZn14aQ"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_core.tools import tool\n",
        "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
        "from datetime import datetime\n",
        "\n",
        "tavily_search = TavilySearchResults(\n",
        "    max_results=3)\n",
        "\n",
        "repl_tool = PythonREPLTool()\n",
        "repl_tool.invoke(\"for i in range(10): print(i)\")\n",
        "\n",
        "\n",
        "@tool\n",
        "def current_date() -> str:\n",
        "    \"현재 날짜를 %y-%m-%d 형식으로 반환합니다.\"\n",
        "    return datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "\n",
        "\n",
        "tools = [tavily_search, repl_tool, current_date]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPAdZG1H14aQ"
      },
      "source": [
        "`ChatHuggingFace()`에도 `bind_tools()`이 구현되어 있으나,   \n",
        "실제로 실행되지 않는 경우가 많습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZoPFhiN14aQ"
      },
      "outputs": [],
      "source": [
        "llm_with_tools = llm.bind_tools(tools)\n",
        "llm_with_tools.invoke(\"오늘 날짜가 어떻게 되니?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwvRgFfO14aQ"
      },
      "outputs": [],
      "source": [
        "llm_with_tools.invoke(\"너는 어떤 툴이나 함수를 가지고 있니?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rP-lWBN14aQ"
      },
      "source": [
        "이런 경우, 툴을 실행하기 위해서는 직접 시스템 메시지를 구성해야 합니다.   \n",
        "Tool 역할 또한 존재하지 않으므로 User 역할을 툴 대용으로 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBkBjlRE14aQ",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "llm_with_tools.kwargs['tools']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pirXIlE14aQ"
      },
      "source": [
        "툴 설명이 담긴 문자열을 구성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAvTnfq614aQ"
      },
      "outputs": [],
      "source": [
        "tool_desc = str('\\n---\\n'.join([str(x) for x in llm_with_tools.kwargs['tools']]))\n",
        "print(tool_desc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AaY3Syl14aQ"
      },
      "source": [
        "시스템 프롬프트를 구성합니다.   \n",
        "성능에 매우 중요한 영향을 미치므로, 영어로 작성했습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMNnrs6s14aQ"
      },
      "outputs": [],
      "source": [
        "system_prompt = f'''\n",
        "You are a helpful assistant with tools below.\n",
        "You can decide whether to invoke any functions or not.\n",
        "If you decide to use any of tools.\n",
        "print name and required parameters of the tool within a json blob correctly.\n",
        "For python code, Return the object as a raw dictionary, without escaping quotes or newlines.\n",
        "\n",
        "for tool use: wrap your output within ```tool_code```.\n",
        "\n",
        "Example:\n",
        "```tool_code\n",
        "{{\"name\":'name of tool', \"arguments\":{{List of apparent argument and parameters}}}}\n",
        "```\n",
        "\n",
        "When the output of the tool is provided, it will be wrapped within ``tool_output```\n",
        "Answer accordingly from the result of the tool output.\n",
        "\n",
        "The question might need some sequential, multiple tool execution.\n",
        "Think Step by Step.\n",
        "\n",
        "The following tools are available:\n",
        "{tool_desc}'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zl0MkeqQ14aR"
      },
      "source": [
        "시스템 프롬프트에 들어가야 하는 내용은 다음과 같습니다.\n",
        "- Tool Format\n",
        "- Tool Call Format\n",
        "- Tool Result Format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLS6guf414aV"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, ToolMessage\n",
        "\n",
        "\n",
        "messages = [SystemMessage(system_prompt),\n",
        "            HumanMessage('오늘 날짜가 며칠이니?')]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izjgV8rS14aW"
      },
      "outputs": [],
      "source": [
        "\n",
        "response = llm.invoke(messages)\n",
        "messages.append(response)\n",
        "response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVRGa7X914aW"
      },
      "outputs": [],
      "source": [
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6okfSfZe14aW"
      },
      "source": [
        "tool_code를 받았으니, 해당 내용을 파싱합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnDI-EaK14aW"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "\n",
        "\n",
        "def parse_tool(text):\n",
        "    try:\n",
        "        text = text.split('```tool_code\\n')[1].split('\\n```')[0]\n",
        "        # tool_code로 wrap된 중간 코드 추출\n",
        "\n",
        "        parsed = ast.literal_eval(text)\n",
        "        # Dict 형태의 값 변환 (json load와 유사)\n",
        "\n",
        "        name = parsed.get('name')\n",
        "        arguments = parsed.get('arguments', {})\n",
        "        # name과 argument return\n",
        "        return {'name':name, 'arguments':arguments}\n",
        "    except (ValueError, SyntaxError):\n",
        "        return None\n",
        "\n",
        "result = parse_tool(response.content)\n",
        "name,arguments = result['name'], result['arguments']\n",
        "name, arguments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgwuDSRE14aW"
      },
      "source": [
        "툴 실행을 연결합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfNnf7x514aW"
      },
      "outputs": [],
      "source": [
        "# 툴 이름과 툴 연결\n",
        "tool_dict = {tool.name: tool for tool in tools}\n",
        "\n",
        "def execute_tool(name, arguments):\n",
        "    # 툴 실행한 뒤 tool_output으로 wrap\n",
        "    result = f'''```tool_output\n",
        "{tool_dict[name].invoke(arguments)}\n",
        "```'''\n",
        "    return result\n",
        "\n",
        "tool_result = execute_tool(**parse_tool(response.content))\n",
        "\n",
        "print(tool_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImCgtT-714aW"
      },
      "outputs": [],
      "source": [
        "messages.append(HumanMessage(tool_result))\n",
        "messages[1:]\n",
        "# 질문 + Tool 요청 + Tool 결과"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIPtqA1514aW"
      },
      "outputs": [],
      "source": [
        "# 결과 해석\n",
        "response = llm.invoke(messages)\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPtixRcX14aW"
      },
      "outputs": [],
      "source": [
        "messages = [SystemMessage(system_prompt),\n",
        "            HumanMessage('2025년 4월 발표된 GPT-4.1 모델이 어떤 모델이야? 한국어로 설명해줘.')]\n",
        "response = llm.invoke(messages)\n",
        "messages.append(response)\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdLc7MVO14aW"
      },
      "outputs": [],
      "source": [
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIOlRjoY14aW"
      },
      "outputs": [],
      "source": [
        "tool_result = execute_tool(**parse_tool(response.content))\n",
        "tool_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dg51FbMB14aW"
      },
      "outputs": [],
      "source": [
        "messages[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwbD7jk214aX"
      },
      "outputs": [],
      "source": [
        "messages.append(HumanMessage(tool_result))\n",
        "response = llm.invoke(messages)\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gojp1ieM14aX"
      },
      "source": [
        "일반적인 입출력 관계의 툴은 이와 같은 방식으로 간단하게 구성할 수 있습니다.   \n",
        "(만약, Python_REPL과 같이 argument가 복잡한 툴을 수행하는 경우에는   \n",
        "별도의 함수로 변환하거나 결과물을 수정하는 작업이 필요할 수 있습니다.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEPVMsmk14aX"
      },
      "source": [
        "해당 구현을 통해, ReAct Agent 구조를 만들어 보겠습니다.    \n",
        "bind_tools가 없기 때문에, 기존의 Tool Message를 사용하기 어렵습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vP38zKqB14aX"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages : Annotated[list, add_messages]   # 메시지 맥락을 저장하는 리스트\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ur7uj1AG14aX"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import ToolMessage\n",
        "\n",
        "tool_list = {tool.name: tool for tool in tools}\n",
        "# tool 목록 dict로 생성\n",
        "\n",
        "def tool_node(state):\n",
        "    tool_outputs = []\n",
        "    tool_call_msgs = state['messages'][-1]\n",
        "    # 마지막 메시지: 툴 콜링 메시지\n",
        "    if '```tool_code' in tool_call_msgs.content:\n",
        "        tool_result = execute_tool(**parse_tool(tool_call_msgs.content))\n",
        "        # tool 실행 결과 얻기 (결과는 ```tool_output```)\n",
        "        tool_outputs.append(HumanMessage(tool_result))\n",
        "\n",
        "    return {'messages': tool_outputs}\n",
        "\n",
        "def agent(state):\n",
        "    system_prompt = SystemMessage(f'''\n",
        "You are a helpful assistant with tools below.\n",
        "You can decide whether to invoke any functions or not.\n",
        "If you decide to use any of tools.\n",
        "print name and required parameters of the tool within a json blob correctly.\n",
        "For python code, Return the object as a raw dictionary, without escaping quotes or newlines.\n",
        "\n",
        "for tool use: wrap your output within ```tool_code```.\n",
        "\n",
        "Example:\n",
        "```tool_code\n",
        "{{\"name\":'name of tool', \"arguments\":{{List of apparent argument and parameters}}}}\n",
        "```\n",
        "\n",
        "When the output of the tool is provided, it will be wrapped within ``tool_output```\n",
        "Answer accordingly from the result of the tool output.\n",
        "\n",
        "The question might need some sequential, multiple tool execution.\n",
        "Think Step by Step.\n",
        "\n",
        "The following tools are available:\n",
        "{tool_desc}\n",
        "\n",
        "\n",
        "Answer in Korean.''')\n",
        "\n",
        "\n",
        "    response = llm.invoke([system_prompt] + state[\"messages\"])\n",
        "    return {'messages': response}\n",
        "\n",
        "def tool_needed(state):\n",
        "\n",
        "    last_msg = state['messages'][-1]\n",
        "    if '```tool_code' in last_msg.content: # 툴 콜링이 필요하면\n",
        "        return \"continue\"\n",
        "    else:\n",
        "        return \"finish\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tgTNu8f14aX"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "builder = StateGraph(State)\n",
        "\n",
        "builder.add_node(\"agent\", agent)\n",
        "builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "builder.add_edge(START, 'agent'),\n",
        "builder.add_conditional_edges(\"agent\",\n",
        "                              tool_needed,\n",
        "                               {\"continue\": \"tools\",\"finish\": END})\n",
        "builder.add_edge(\"tools\", \"agent\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIaHRhJq14aX"
      },
      "outputs": [],
      "source": [
        "graph = builder.compile()\n",
        "graph # 생성한 그래프 시각화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwtyHK-C14aX"
      },
      "outputs": [],
      "source": [
        "response = graph.invoke({'messages':[HumanMessage(content=\"오늘 날짜에 태어난 유명인들 조사해서 알려줘.\")]})\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gDQQHN_14aX"
      },
      "source": [
        "Gemma 3는 Native Tool Call을 지원하는 모델은 아닙니다.  \n",
        "하지만, 어느 정도 함수 실행 능력을 보이는데요.   \n",
        "\n",
        "마찬가지로, Gemma 3 이외의 LLM 모델을 사용하는 경우에도,  \n",
        "각각의 Tool 스펙에 대한 문서를 참고하여 진행하실 수 있으나, 성능은 조금 떨어질 수 있습니다.   \n",
        "Ex) Phi-4 https://huggingface.co/microsoft/Phi-4-mini-instruct\n",
        "\n",
        "<br><br>\n",
        "이번에는 Qwen 2.5 7B를 이용해 진행해 보겠습니다.\n",
        "\n",
        "**세션 초기화 후 여기서부터 다시 시작해 주세요!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSelbmwB14aX",
        "outputId": "13751504-363e-4803-8251-dae0429caee1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv('env')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxqnxWpk14aX",
        "outputId": "0b5b7820-8e92-44a6-f548-9bd2f2042406"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# Model ID: Qwen/Qwen2.5-7B-Instruct\n"
          ]
        }
      ],
      "source": [
        "model_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "print(f\"# Model ID: {model_id}\")\n",
        "# Full Precision 로드: 15.6GB\n",
        "# GPU 부족시 양자화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KNPHUQ5_rKt"
      },
      "outputs": [],
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "# 양자화 옵션: 4비트\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    # nf4 양자화\n",
        "\n",
        "    bnb_4bit_compute_dtype=\"bfloat16\", # 계산시에는 기존 자료형 bf16 사용\n",
        "    bnb_4bit_use_double_quant=True # 이중 양자화\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668,
          "referenced_widgets": [
            "98c5923efc05402fa4b06d32ad1588ef",
            "daf5a315673147779de0b999a7d2315b",
            "7878d799f8074aa3a8fc164c0daeab04",
            "26999a37e67148c299927987210755b4",
            "9de1e5c8a8b04dc1891ec35e400e3ced",
            "5369d4cdad1e48a696c4545b36023c75",
            "6bdaf67b2c3042f7a9058b0a7262114d",
            "7fcd670f11ba4e218f009ce0baa01eef",
            "33e9fdbd4cc94f7a9dad92a150046c8e",
            "588182a5c01f4f81883486dc74b6469f",
            "a1deaade948a4ff6acb34df5ac5f26dd"
          ]
        },
        "id": "RnflkPZR14aY",
        "outputId": "c5833b90-b1c1-495c-f1f0-121ed8fd24dd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "98c5923efc05402fa4b06d32ad1588ef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Qwen2ForCausalLM(\n",
              "  (model): Qwen2Model(\n",
              "    (embed_tokens): Embedding(152064, 3584)\n",
              "    (layers): ModuleList(\n",
              "      (0-27): 28 x Qwen2DecoderLayer(\n",
              "        (self_attn): Qwen2Attention(\n",
              "          (q_proj): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
              "          (k_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
              "          (v_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
              "          (o_proj): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
              "        )\n",
              "        (mlp): Qwen2MLP(\n",
              "          (gate_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=18944, out_features=3584, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "    (rotary_emb): Qwen2RotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# 토크나이저와 모델 로드\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "\n",
        "    torch_dtype='auto',\n",
        "    device_map={\"\":0},  # 0번 GPU에 할당\n",
        "\n",
        "    quantization_config  = quantization_config,\n",
        "\n",
        "    # attn_implementation = 'flash_attention_2'\n",
        "    # flash attention 설정\n",
        "    # 미지원/사용하지 않는 경우 제거 (T4 등의 구버전 GPU 등)\n",
        "\n",
        ")\n",
        "\n",
        "# Train X (eval)\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZshkwSU14aY",
        "outputId": "565ac542-e095-4101-a1ec-45ab0fd9848b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace\n",
        "\n",
        "# Sampling 파라미터 설정: https://huggingface.co/Qwen/Qwen2.5-7B-Instruct\n",
        "# 홈페이지 권장 사양이지만, 적절하게 바꿔도 됨\n",
        "gen_config = dict(\n",
        "    do_sample=True,\n",
        "    max_new_tokens=2048,\n",
        "    repetition_penalty = 1.05,\n",
        "\n",
        "    temperature = 0.1,\n",
        "    top_p = 0.8,\n",
        "    top_k = 20\n",
        ")\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False,\n",
        "    # return_full_text=True <-- 입력 프롬프트를 포함한 전체 출력하기\n",
        "    **gen_config\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpGHbrzF14aY"
      },
      "source": [
        "Qwen 2.5 는 Tool 기능이 포함되어 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsDwcaHq14aY",
        "outputId": "5f4f271d-638b-4816-f2af-ac46cad9a71f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{%- if tools %}\n",
            "    {{- '<|im_start|>system\\n' }}\n",
            "    {%- if messages[0]['role'] == 'system' %}\n",
            "        {{- messages[0]['content'] }}\n",
            "    {%- else %}\n",
            "        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n",
            "    {%- endif %}\n",
            "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
            "    {%- for tool in tools %}\n",
            "        {{- \"\\n\" }}\n",
            "        {{- tool | tojson }}\n",
            "    {%- endfor %}\n",
            "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
            "{%- else %}\n",
            "    {%- if messages[0]['role'] == 'system' %}\n",
            "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
            "    {%- else %}\n",
            "        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n",
            "    {%- endif %}\n",
            "{%- endif %}\n",
            "{%- for message in messages %}\n",
            "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
            "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
            "    {%- elif message.role == \"assistant\" %}\n",
            "        {{- '<|im_start|>' + message.role }}\n",
            "        {%- if message.content %}\n",
            "            {{- '\\n' + message.content }}\n",
            "        {%- endif %}\n",
            "        {%- for tool_call in message.tool_calls %}\n",
            "            {%- if tool_call.function is defined %}\n",
            "                {%- set tool_call = tool_call.function %}\n",
            "            {%- endif %}\n",
            "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
            "            {{- tool_call.name }}\n",
            "            {{- '\", \"arguments\": ' }}\n",
            "            {{- tool_call.arguments | tojson }}\n",
            "            {{- '}\\n</tool_call>' }}\n",
            "        {%- endfor %}\n",
            "        {{- '<|im_end|>\\n' }}\n",
            "    {%- elif message.role == \"tool\" %}\n",
            "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
            "            {{- '<|im_start|>user' }}\n",
            "        {%- endif %}\n",
            "        {{- '\\n<tool_response>\\n' }}\n",
            "        {{- message.content }}\n",
            "        {{- '\\n</tool_response>' }}\n",
            "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
            "            {{- '<|im_end|>\\n' }}\n",
            "        {%- endif %}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "{%- if add_generation_prompt %}\n",
            "    {{- '<|im_start|>assistant\\n' }}\n",
            "{%- endif %}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.chat_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDoXXYMK14aY"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_core.tools import tool\n",
        "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
        "from datetime import datetime\n",
        "\n",
        "tavily_search = TavilySearchResults(\n",
        "    max_results=3)\n",
        "\n",
        "repl_tool = PythonREPLTool()\n",
        "\n",
        "\n",
        "@tool\n",
        "def current_date() -> str:\n",
        "    \"현재 날짜를 %y-%m-%d 형식으로 반환합니다.\"\n",
        "    return datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "\n",
        "\n",
        "tools = [tavily_search, repl_tool, current_date]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTcLxWoK14aY"
      },
      "outputs": [],
      "source": [
        "base_llm = HuggingFacePipeline(pipeline=pipe, pipeline_kwargs=gen_config)\n",
        "# base_llm: 입력 전 Tokenizer로 템플릿 변환 필수, 스트리밍 가능\n",
        "\n",
        "llm = ChatHuggingFace(llm=base_llm, tokenizer=tokenizer)\n",
        "# llm: 자동 템플릿 변환(ChatGoogleGenAI와 동일), 스트리밍 불가"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZhrctni14aY"
      },
      "source": [
        "툴 기능이 있어도, bind_tools는 사용이 어렵습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMKVBn3-14aY",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "llm_with_tools = llm.bind_tools(tools)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BXWUzvg14aY",
        "outputId": "5ce776cf-867d-4083-ff71-cc350f3df970"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='죄송합니다, 저는 직접 오늘의 날짜를 확인할 수 있는 도구를 사용할 수 없습니다. 하지만 일반적으로 컴퓨터나 스마트폰의 시스템 날짜가 정확한 경우가 많습니다. 또는 웹사이트나 앱을 통해 현재 날짜와 시간을 확인하실 수 있습니다. 예를 들어, 브라우저의 날짜와 시간을 확인하거나, Google 검색창에 \"현재 날짜와 시간\"을 입력하면 현재 날짜와 시간이 표시됩니다.', additional_kwargs={}, response_metadata={}, id='run-bf22927c-f7f0-409f-bede-53b21cb2e57c-0')"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm_with_tools.invoke(\"오늘 날짜가 어떻게 되니? 툴을 사용해서 알려줘.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXByDlQD14aY"
      },
      "source": [
        "툴을 json list로 전달하여 템플릿에 적용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HT-XCdOo14aY",
        "outputId": "53c65edf-1a1a-4325-d7a3-36370a8dbeba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'type': 'function', 'function': {'name': 'tavily_search_results_json', 'description': 'A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'Python_REPL', 'description': 'A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.', 'parameters': {'properties': {'query': {'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'current_date', 'description': '현재 날짜를 %y-%m-%d 형식으로 반환합니다.', 'parameters': {'properties': {}, 'type': 'object'}}}]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "tool_desc = llm_with_tools.kwargs['tools']\n",
        "print(tool_desc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQ_N2uSH14aY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sayymRP14aY"
      },
      "source": [
        "토크나이저에 tool을 전달하여, 전반적인 템플릿을 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAYKx-ca14aZ",
        "outputId": "44114933-9fc2-4a97-fdf4-8d4382831644"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|im_start|>system\n",
            "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
            "\n",
            "# Tools\n",
            "\n",
            "You may call one or more functions to assist with the user query.\n",
            "\n",
            "You are provided with function signatures within <tools></tools> XML tags:\n",
            "<tools>\n",
            "{\"type\": \"function\", \"function\": {\"name\": \"tavily_search_results_json\", \"description\": \"A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.\", \"parameters\": {\"properties\": {\"query\": {\"description\": \"search query to look up\", \"type\": \"string\"}}, \"required\": [\"query\"], \"type\": \"object\"}}}\n",
            "{\"type\": \"function\", \"function\": {\"name\": \"Python_REPL\", \"description\": \"A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\", \"parameters\": {\"properties\": {\"query\": {\"type\": \"string\"}}, \"required\": [\"query\"], \"type\": \"object\"}}}\n",
            "{\"type\": \"function\", \"function\": {\"name\": \"current_date\", \"description\": \"현재 날짜를 %y-%m-%d 형식으로 반환합니다.\", \"parameters\": {\"properties\": {}, \"type\": \"object\"}}}\n",
            "</tools>\n",
            "\n",
            "For each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n",
            "<tool_call>\n",
            "{\"name\": <function-name>, \"arguments\": <args-json-object>}\n",
            "</tool_call><|im_end|>\n",
            "<|im_start|>user\n",
            "오늘 날짜가 어떻게 돼?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def convert_chat_with_tools(messages, tools=None, add_generation_prompt = True):\n",
        "    return tokenizer.apply_chat_template(messages, tools=tools, tokenize=False, add_generation_prompt=add_generation_prompt)\n",
        "\n",
        "example = [{'role':'user', 'content':'오늘 날짜가 어떻게 돼?'}]\n",
        "print(convert_chat_with_tools(example, tools=tool_desc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PwXIVXY14aZ",
        "outputId": "dbd3b369-ce06-4b75-8750-2e4f7b432db4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<tool_call>\n",
            "{\"name\": \"current_date\", \"arguments\": {}}\n",
            "</tool_call>"
          ]
        }
      ],
      "source": [
        "for s in base_llm.stream(convert_chat_with_tools(example, tools=tool_desc)):\n",
        "    print(s, end='')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoA3jD4d14aZ"
      },
      "source": [
        "해당 내용을 가져가서, 시스템 프롬프트를 구성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNMMxgw414aZ"
      },
      "outputs": [],
      "source": [
        "# 최상의 결과를 위해, 토크나이저의 템플릿을 최대한 따릅니다.\n",
        "system_prompt = f'''\n",
        "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
        "\n",
        "# Tools\n",
        "\n",
        "You may call one or more functions to assist with the user query.\n",
        "\n",
        "You are provided with function signatures within <tools></tools> XML tags:\n",
        "<tools>\n",
        "{tool_desc}\n",
        "</tools>\n",
        "\n",
        "For each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n",
        "<tool_call>\n",
        "{{\"name\": <function-name>, \"arguments\": <args-json-object>}}\n",
        "</tool_call>\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqCmg9EF14aZ"
      },
      "source": [
        "Gemma와 동일하게 실행해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gdb8FThu14aZ",
        "outputId": "633da78d-c5e7-40a4-d085-e6064965ad78"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='<tool_call>\\n{\"name\": \"current_date\", \"arguments\": {}}\\n</tool_call>', additional_kwargs={}, response_metadata={}, id='run-35009295-26a0-4904-bedb-67dd569ee9b9-0')"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, ToolMessage\n",
        "\n",
        "messages = [SystemMessage(system_prompt),\n",
        "            HumanMessage('오늘 날짜가 며칠이니?')]\n",
        "\n",
        "response = llm.invoke(messages)\n",
        "messages.append(response)\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRZQrsfB14aZ"
      },
      "source": [
        "tool_call의 형식에 맞춰 코드를 수정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b-20HsO14aZ",
        "outputId": "8a5fc277-17d8-46c9-a483-1e4e86307f69"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('current_date', {})"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import ast\n",
        "\n",
        "def parse_tool(text):\n",
        "    try:\n",
        "        text = text.split('<tool_call>\\n')[1].split('\\n</tool_call>')[0]\n",
        "        # tool_code로 wrap된 중간 코드 추출\n",
        "\n",
        "        parsed = ast.literal_eval(text)\n",
        "        # Dict 형태의 값 변환 (json load와 유사)\n",
        "        name = parsed.get('name')\n",
        "        arguments = parsed.get('arguments', {})\n",
        "        # name과 argument return\n",
        "        return {'name':name, 'arguments':arguments}\n",
        "    except (ValueError, SyntaxError):\n",
        "        return None\n",
        "\n",
        "result = parse_tool(response.content)\n",
        "name,arguments = result['name'], result['arguments']\n",
        "name, arguments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSUaIpCr14aZ"
      },
      "source": [
        "툴을 실행하고, 그 결과는 ToolMessage로 전달합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5tRRRDpr14aZ",
        "outputId": "dd872cd3-fbf3-43a7-aa08-a22feec9536a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2025-04-16'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 툴 이름과 툴 연결\n",
        "tool_dict = {tool.name: tool for tool in tools}\n",
        "\n",
        "def execute_tool(name, arguments):\n",
        "    # 툴 실행한 뒤 tool_output으로 wrap\n",
        "    result = tool_dict[name].invoke(arguments)\n",
        "    return str(result)\n",
        "\n",
        "tool_result = execute_tool(**parse_tool(response.content))\n",
        "\n",
        "tool_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ShknVlD14aZ",
        "outputId": "08d8b086-480d-47b7-c8b0-daaca71e5e1c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='오늘 날짜가 며칠이니?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='<tool_call>\\n{\"name\": \"current_date\", \"arguments\": {}}\\n</tool_call>\\n<tool_call>\\n{\"name\": \"current_date\", \"arguments\": {}}\\n</tool_call>\\n<tool_call>\\n{\"name\": \"tavily_search_results_json\", \"arguments\": {\"query\": \"날짜 확인\"}}\\n</tool_call>\\n<tool_call>\\n{\"name\": \"current_date\", \"arguments\": {}}\\n</tool_call>\\n今天的日期是2024年3月1日。', additional_kwargs={}, response_metadata={}, id='run-342498ab-cb42-4790-855a-4dea89ff229b-0'),\n",
              " HumanMessage(content='2025-04-16', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "messages.append(HumanMessage(tool_result))\n",
        "messages[1:]\n",
        "# 질문 + Tool 요청 + Tool 결과"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3UQ30AX14aZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfiQVBqt14aZ",
        "outputId": "fbdfd2e8-709e-416c-8234-d6962986cb32"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='您提供的日期是2025年4月16日。如果您需要确认这个日期是否正确，或者需要与当前日期进行比较，请告诉我您的具体需求。如果只是简单地提供一个日期，那么这就是该日期：2025年4月16日。', additional_kwargs={}, response_metadata={}, id='run-b6eac70c-99e1-4b47-87d5-9538b8119113-0')"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = llm.invoke(messages)\n",
        "messages.append(response)\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZM__QJI14aa"
      },
      "source": [
        "동일한 구조로, 프롬프트를 수정하여 전체 에이전트를 구성할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-giuavz14aa"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages : Annotated[list, add_messages]   # 메시지 맥락을 저장하는 리스트\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_ihlip114aa"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import ToolMessage\n",
        "\n",
        "tool_list = {tool.name: tool for tool in tools}\n",
        "# tool 목록 dict로 생성\n",
        "\n",
        "def tool_node(state):\n",
        "    tool_outputs = []\n",
        "    tool_call_msgs = state['messages'][-1]\n",
        "    # 마지막 메시지: 툴 콜링 메시지\n",
        "    if '<tool_call>' in tool_call_msgs.content:\n",
        "        tool_result = execute_tool(**parse_tool(tool_call_msgs.content))\n",
        "        # tool 실행 결과 얻기 (결과는 ```tool_output```)\n",
        "        tool_outputs.append(HumanMessage(tool_result))\n",
        "\n",
        "    return {'messages': tool_outputs}\n",
        "\n",
        "def agent(state):\n",
        "    system_prompt = SystemMessage(f'''\n",
        "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
        "\n",
        "# Tools\n",
        "\n",
        "You may call a function to assist with the user query.\n",
        "\n",
        "You are provided with function signatures within <tools></tools> XML tags:\n",
        "<tools>\n",
        "{tool_desc}\n",
        "</tools>\n",
        "\n",
        "For function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n",
        "<tool_call>\n",
        "{{\"name\": <function-name>, \"arguments\": <args-json-object>}}\n",
        "</tool_call>\n",
        "\n",
        "# Tool Usage Rules - VERY IMPORTANT!\n",
        "\n",
        "1.  **Analyze the Task:** Carefully read the user's request and determine if any tools are needed.\n",
        "2.  **One Tool At A Time:** If you need to use a tool, you **MUST** choose and call **only ONE** tool in your response. Do **NOT** issue multiple `<tool_call>` tags in a single turn.\n",
        "3.  **Sequential Execution:** If the user's request requires information from multiple tool calls (e.g., searching for information and then getting the weather based on the search result), you **MUST** perform these calls sequentially.\n",
        "    * First, call the **single** tool needed for the first step.\n",
        "    * Wait for the result of that tool.\n",
        "    * Then, based on the result, decide if another **single** tool call is necessary for the next step. Issue that call in a *new* response turn.\n",
        "\n",
        "Answer in Korean.''')\n",
        "\n",
        "\n",
        "    response = llm.invoke([system_prompt] + state[\"messages\"])\n",
        "    return {'messages': response}\n",
        "\n",
        "def tool_needed(state):\n",
        "\n",
        "    last_msg = state['messages'][-1]\n",
        "    if '<tool_call>' in last_msg.content: # 툴 콜링이 필요하면\n",
        "        return \"continue\"\n",
        "    else:\n",
        "        return \"finish\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpYW7TQ314aa",
        "outputId": "81f36951-15e5-475e-dfc3-73e9285e04cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7f95063d8250>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 메모리 세팅\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "builder = StateGraph(State)\n",
        "\n",
        "builder.add_node(\"agent\", agent)\n",
        "builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "builder.add_edge(START, 'agent'),\n",
        "builder.add_conditional_edges(\"agent\",\n",
        "                              tool_needed,\n",
        "                               {\"continue\": \"tools\",\"finish\": END})\n",
        "builder.add_edge(\"tools\", \"agent\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsuR8tZd14aa",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "memory = MemorySaver()\n",
        "graph = builder.compile(checkpointer = memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIHSRouc14aa",
        "outputId": "01b6ca20-d74d-4805-ce85-d4c849a7d87c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='오늘이 며칠이야?', additional_kwargs={}, response_metadata={}, id='f1228164-6d8e-417d-b298-18d0edb03cb2'),\n",
              "  AIMessage(content='<tool_call>\\n{\"name\": \"current_date\", \"arguments\": {}}\\n</tool_call>', additional_kwargs={}, response_metadata={}, id='run-556125ef-c872-4fab-9fd4-55b031e06221-0'),\n",
              "  HumanMessage(content='2025-04-16', additional_kwargs={}, response_metadata={}, id='d48dce25-fcad-47ef-adaf-cdbf7e8cd902'),\n",
              "  AIMessage(content='오늘은 2025년 4월 16일입니다.', additional_kwargs={}, response_metadata={}, id='run-10dd8e0c-fe3c-40ee-862f-8df3def4ecca-0')]}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "\n",
        "response = graph.invoke({'messages':[HumanMessage(content=\"오늘이 며칠이야?\")]}, config)\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llXZkL_714aa",
        "outputId": "6a15ace9-f62a-4a6e-f8bf-c8d6cb176a76"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='오늘과 같은 날짜에 태어난 유명인들 조사해서 알려줘.', additional_kwargs={}, response_metadata={}, id='a78cddde-97c6-40a4-b514-00d38827ed92'),\n",
              "  AIMessage(content='ronics\\n{\"name\": \"tavily_search_results_json\", \"arguments\": {\"query\": \"유명인 오늘 같은 날짜에 태어남\"}}', additional_kwargs={}, response_metadata={}, id='run-7606bcad-bd2f-4fee-b35e-1e979a6ef3e6-0'),\n",
              "  HumanMessage(content='오늘과 같은 날짜에 태어난 유명인들 조사해서 알려줘.', additional_kwargs={}, response_metadata={}, id='de1b72f4-d897-46a4-92e9-10162db785d7'),\n",
              "  AIMessage(content='ronics\\n{\"name\": \"tavily_search_results_json\", \"arguments\": {\"query\": \"유명인 오늘 같은 날짜에 태어난 사람\"}}', additional_kwargs={}, response_metadata={}, id='run-903fce71-d0fa-4d57-b390-a5582c23cc85-0'),\n",
              "  HumanMessage(content='오늘과 같은 날짜에 태어난 유명인들 조사해서 알려줘.', additional_kwargs={}, response_metadata={}, id='c5684997-c263-402e-9396-e47ece92e422'),\n",
              "  AIMessage(content='ronics\\n{\"name\": \"tavily_search_results_json\", \"arguments\": {\"query\": \"유명인 오늘 생일\"}}', additional_kwargs={}, response_metadata={}, id='run-7fba29d1-44eb-418f-a070-e734c744a7f7-0')]}"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# sLLM: 결과가 Inconsistent...\n",
        "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
        "\n",
        "response = graph.invoke({'messages':[HumanMessage(content=\"오늘과 같은 날짜에 태어난 유명인들 조사해서 알려줘.\")]}, config)\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3opOZPT-14aa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlt75bEw14aa",
        "outputId": "f81775e1-b011-4b3c-df11-488e584d7656"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='2025년 4월에 출시된 GPT-4.1 모델에 대해 소개해줘.', additional_kwargs={}, response_metadata={}, id='8d9abf31-7608-4d03-9afb-706d2afcac5a'),\n",
              "  AIMessage(content='먼저, 현재 시점에서 GPT-4.1이라는 모델은 공개되지 않았습니다. GPT-4는 OpenAI에서 2023년 9월에 발표한 최신 모델이며, 그 이후로 더 업데이트된 버전은 아직 알려져 있지 않습니다. 따라서, 2025년 4월에 출시된 GPT-4.1에 대한 정보는 제공할 수 없습니다.\\n\\n하지만, GPT-4의 특징과 기대되는 개선점을 바탕으로 추측해 볼 수 있습니다. GPT-4는 이미 대화형 AI의 성능을 크게 향상시켰고, 더욱 정확하고 자연스러운 대화가 가능하도록 했습니다. 만약 GPT-4.1이 있다면, 아마도 다음과 같은 개선점들이 있을 것으로 예상됩니다:\\n\\n1. **더 높은 정확성**: 모델이 학습한 데이터의 양이 늘어나면서, 더욱 정확하고 신뢰할 수 있는 정보를 제공할 것으로 예상됩니다.\\n2. **더 나은 이해력**: 대화형 AI의 이해력이 더욱 향상되어, 사용자의 질문에 더 잘 대답할 수 있을 것입니다.\\n3. **더 자연스러운 대화**: 대화의 자연스러움이 더욱 향상되어, 사용자와의 대화가 더욱 원활해질 것으로 보입니다.\\n\\n위 내용은 추정사항이며, 실제 GPT-4.1의 경우 공식적인 발표나 발표자가 제공하는 정보를 참고해야 합니다.', additional_kwargs={}, response_metadata={}, id='run-4933bc55-b51d-4971-b003-d383b30c1cd3-0'),\n",
              "  HumanMessage(content='2025년 4월에 출시된 GPT-4.1 모델에 대해 소개해줘.', additional_kwargs={}, response_metadata={}, id='1591623c-b6f0-4973-ad70-314b0973ce7a'),\n",
              "  AIMessage(content='현재까지 공개된 정보에 따르면, GPT-4.1이라는 모델은 존재하지 않습니다. GPT-4는 2023년 9월에 OpenAI에서 발표한 최신 모델이며, 이후 더 업데이트된 버전은 아직 알려져 있지 않습니다.\\n\\n그럼에도 불구하고, 2025년 4월에 출시된 GPT-4.1 모델에 대한 가상의 소개를 해드릴 수 있습니다. 이는 추측적인 내용이므로 실제 제품과는 다를 수 있습니다.\\n\\n### GPT-4.1 모델 소개\\n\\n**1. 개요**\\nGPT-4.1은 2025년 4월에 출시된 최신 대화형 AI 모델입니다. 이 모델은 GPT-4의 성능을 기반으로 더욱 강화된 기능과 성능을 제공합니다.\\n\\n**2. 주요 특징**\\n\\n- **더 높은 정확성**: GPT-4.1은 더 많은 데이터와 더 강력한 학습 알고리즘을 통해 더욱 정확한 결과를 제공합니다. 이는 다양한 분야에서의 문제 해결 능력을 향상시킵니다.\\n  \\n- **더 나은 이해력**: 모델의 이해력이 더욱 향상되어, 사용자의 질문에 더욱 정확하게 대답할 수 있습니다. 이는 대화형 AI의 사용성을 크게 향상시킵니다.\\n  \\n- **더 자연스러운 대화**: 대화의 자연스러움이 더욱 향상되어, 사용자와의 대화가 더욱 원활해집니다. 이는 사용자 경험을 크게 개선합니다.\\n  \\n- **더 빠른 응답 시간**: GPT-4.1은 더 빠른 처리 속도를 통해 사용자에게 즉각적인 응답을 제공합니다. 이는 사용자와의 상호작용을 더욱 원활하게 만듭니다.\\n  \\n- **더 안전한 운영**: 모델의 안전성이 더욱 강화되어, 사용자 개인정보 보호와 데이터 보안을 더욱 강화합니다. 이는 사용자 신뢰를 크게 증가시킵니다.\\n\\n**3. 활용 사례**\\n\\n- **교육**: 교육 분야에서는 GPT-4.1을 통해 학생들의 질문에 더욱 정확하게 대답하고, 개인화된 학습 경험을 제공할 수 있습니다.\\n  \\n- **건강 관리**: 의료 분야에서는 GPT-4.1을 통해 환자의 건강 상태를 모니터링하고, 개인화된 치료 계획을 제안할 수 있습니다.\\n  \\n- **비즈니스 지원**: 비즈니스에서는 GPT-4.1을 통해 고객 서비스를 개선하고, 효율적인 의사결정을 지원할 수 있습니다.\\n\\n이처럼, GPT-4.1은 대화형 AI의 성능을 더욱 향상시키며, 다양한 분야에서의 활용도를 크게 확장할 것으로 예상됩니다.', additional_kwargs={}, response_metadata={}, id='run-f3e25724-b5ea-4ea6-b5ec-ef2b13b8b1fa-0')]}"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 검색 2\n",
        "config = {\"configurable\": {\"thread_id\": \"3\"}}\n",
        "\n",
        "response = graph.invoke({'messages':[HumanMessage(content=\"2025년 4월에 출시된 GPT-4.1 모델에 대해 소개해줘.\")]},\n",
        "                       config)\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRbTmsmI14aa"
      },
      "source": [
        "Tool을 사용하지 않는 Agentic Work의 경우에는 기존 LLM과 동일하게 실행이 가능합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruxvj95514aa"
      },
      "source": [
        "단, Transformers 라이브러리의 경우 동시 실행 등의 메커니즘이 최적화되어 있지 않기 때문에,   \n",
        "이전의 Send()와 같이 병렬 실행이 필요한 문제에서는 결과가 제대로 나오지 않을 수 있습니다.    \n",
        "\n",
        "이를 해결하는 방법은, Ollama나 vLLM과 같은 서빙 라이브러리를 사용하는 것입니다."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "26999a37e67148c299927987210755b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_588182a5c01f4f81883486dc74b6469f",
            "placeholder": "​",
            "style": "IPY_MODEL_a1deaade948a4ff6acb34df5ac5f26dd",
            "value": " 4/4 [01:22&lt;00:00, 19.82s/it]"
          }
        },
        "33e9fdbd4cc94f7a9dad92a150046c8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5369d4cdad1e48a696c4545b36023c75": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "588182a5c01f4f81883486dc74b6469f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bdaf67b2c3042f7a9058b0a7262114d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7878d799f8074aa3a8fc164c0daeab04": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7fcd670f11ba4e218f009ce0baa01eef",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_33e9fdbd4cc94f7a9dad92a150046c8e",
            "value": 4
          }
        },
        "7fcd670f11ba4e218f009ce0baa01eef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98c5923efc05402fa4b06d32ad1588ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_daf5a315673147779de0b999a7d2315b",
              "IPY_MODEL_7878d799f8074aa3a8fc164c0daeab04",
              "IPY_MODEL_26999a37e67148c299927987210755b4"
            ],
            "layout": "IPY_MODEL_9de1e5c8a8b04dc1891ec35e400e3ced"
          }
        },
        "9de1e5c8a8b04dc1891ec35e400e3ced": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1deaade948a4ff6acb34df5ac5f26dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "daf5a315673147779de0b999a7d2315b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5369d4cdad1e48a696c4545b36023c75",
            "placeholder": "​",
            "style": "IPY_MODEL_6bdaf67b2c3042f7a9058b0a7262114d",
            "value": "Loading checkpoint shards: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

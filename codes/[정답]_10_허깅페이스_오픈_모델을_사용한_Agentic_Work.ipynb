{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsYrzxX5DaqK"
      },
      "source": [
        "# [ì‹¤ìŠµ] HuggingFace ê³µê°œ LLMì„ í™œìš©í•œ Agentic Work ë§Œë“¤ê¸°\n",
        "\n",
        "HuggingFaceì— ê²Œì‹œëœ ê³µê°œ ëª¨ë¸ë“¤ì˜ ê²½ìš°, `bind_tools`ì™€ ê°™ì€ ê¸°ëŠ¥ë“¤ì´ ì œëŒ€ë¡œ ì—°ë™ë˜ì§€ ì•Šê±°ë‚˜,   \n",
        "ìì²´ì ìœ¼ë¡œ Tool Calling ê¸°ëŠ¥ì´ ì—†ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.   \n",
        "\n",
        "ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, Tool Calling ë°©ë²•ì„ ì§ì ‘ êµ¬ì„±í•˜ê³  Agentic Workë¥¼ êµ¬í˜„í•´ ë³´ê² ìŠµë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiuZNYFq14aN"
      },
      "source": [
        "## ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "\n",
        "\n",
        "ì´ë²ˆ ì‹¤ìŠµì€ ë¬´ë£Œ ì½”ë©(T4 GPU)ì´ ì•„ë‹Œ ê³ ì„±ëŠ¥ GPU ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì§„í–‰í•©ë‹ˆë‹¤.   \n",
        "ë¬´ë£Œ ì½”ë©ìœ¼ë¡œ ì§„í–‰í•˜ì‹œëŠ” ê²½ìš°, GPU ì„±ëŠ¥ì˜ í•œê³„ë¡œ ì‹¤í–‰ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.   \n",
        "(ì•„ë˜ ì½”ë“œì—ì„œ, **T4 ì‚¬ìš©ì‹œ í•´ì œ** ë¶€ë¶„ì„ ì°¸ê³ í•˜ì„¸ìš”!)\n",
        "\n",
        "<br><br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Udtv--pI14aN",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!pip install transformers bitsandbytes accelerate -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1JC3bK9DaqL"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade langchain_experimental langgraph langchain langchain_community langchain_huggingface dotenv -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1eMoe-G14aO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gYEfMXn14aO",
        "outputId": "276917bd-6172-4f0f-f252-d3126f7288fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Flash Attention: ë¦¬ëˆ…ìŠ¤ ì „ìš© ì„¤ì¹˜ë°©ë²•\n",
        "# ë¬´ë£Œ ì½”ë©(T4)ì—ì„œëŠ” ì„¤ì¹˜ X\n",
        "\n",
        "# Windows ì„¤ì¹˜ëŠ” https://github.com/kingbri1/flash-attention/releases ì°¸ê³ \n",
        "!pip install flash-attn --no-build-isolation -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DtuBpmP14aO"
      },
      "source": [
        "ì„¤ì¹˜í•  ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ë§ìœ¼ë¯€ë¡œ, ê°€ê¸‰ì  ì„¤ì¹˜ í›„ ì„¸ì…˜ ì¬ì‹œì‘ì„ ìˆ˜í–‰í•´ ì£¼ì„¸ìš”."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBhW7vFc14aO"
      },
      "source": [
        "í—ˆê¹…í˜ì´ìŠ¤ í† í°ì„ ì¸ì¦í•©ë‹ˆë‹¤.    \n",
        "\n",
        "https://huggingface.co/settings/tokens ì—ì„œ Read ê¶Œí•œ í† í°ì„ ë°œê¸‰ë°›ìŠµë‹ˆë‹¤.   \n",
        "ì•„ë˜ ì½”ë“œì—ì„œëŠ”, env íŒŒì¼ì— `HF_READ_TOKEN`ìœ¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKRxTZ5n14aO"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVFaB-AA14aO",
        "outputId": "047ac881-646b-4d8f-93d6-a848a719e239"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "load_dotenv('env') #.env íŒŒì¼ì¸ ê²½ìš° .envë¡œ ë³€ê²½"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "siIa3Voy14aO",
        "outputId": "04a4ff69-9a66-4687-fb4f-b3733570a545"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['HF_TOKEN']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJEt8RpX14aO",
        "outputId": "0194c900-3ae1-479a-f954-2e64d8a5d1e8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
            "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "# í—ˆê¹…í˜ì´ìŠ¤ í† í° ë¡œê·¸ì¸\n",
        "login(token=os.environ['HF_TOKEN'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d__aPk5_14aO"
      },
      "source": [
        "### 1. Gemma 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrzEKTP514aP"
      },
      "source": [
        "ë¶ˆëŸ¬ì˜¬ ëª¨ë¸ì€ êµ¬ê¸€ì˜ Gemma-3-12b-it ì…ë‹ˆë‹¤.   \n",
        "ì‹¤ì œ í¬ê¸°ëŠ” 24GB ì •ë„ì´ì§€ë§Œ, 4Bit ì–‘ìí™”ë¥¼ í†µí•´ 8~9GB í¬ê¸°ë¡œ ë¡œë“œí•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6QY8RHv14aP",
        "outputId": "a5be359d-d34b-48ba-c51e-99da2394aa45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# Model ID: google/gemma-3-12b-it\n"
          ]
        }
      ],
      "source": [
        "model_id = \"google/gemma-3-12b-it\"\n",
        "print(f\"# Model ID: {model_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qE5W5Bat14aP"
      },
      "source": [
        "íŠ¸ëœìŠ¤í¬ë¨¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í•´ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z592IDzIDaqM"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "# ì–‘ìí™” ì˜µì…˜: 4ë¹„íŠ¸\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    # nf4 ì–‘ìí™”\n",
        "\n",
        "    bnb_4bit_compute_dtype=\"bfloat16\", # ê³„ì‚°ì‹œì—ëŠ” ê¸°ì¡´ ìë£Œí˜• bf16 ì‚¬ìš©\n",
        "    bnb_4bit_use_double_quant=True # ì´ì¤‘ ì–‘ìí™”\n",
        ")\n",
        "\n",
        "\n",
        "# í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¡œë“œ\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "\n",
        "    torch_dtype=torch.bfloat16, # Gemma 3ì€ bf16 ëª¨ë¸ì…ë‹ˆë‹¤!\n",
        "    device_map={\"\":0},  # 0ë²ˆ GPUì— í• ë‹¹\n",
        "\n",
        "    quantization_config = quantization_config,\n",
        "    # ì–‘ìí™” ë¶ˆí•„ìš”ì‹œ ì œê±°\n",
        "\n",
        "    attn_implementation = 'eager'\n",
        "    # flash attention ì„¤ì •\n",
        "    # Gemma Seriesê°€ ì•„ë‹Œ ê²½ìš° 'eager'ë¥¼ \"flash_attention_2\" ë¡œ ë³€ê²½\n",
        "    # ë¯¸ì§€ì›/ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš° ì œê±° (T4 ë“±ì˜ êµ¬ë²„ì „ GPU ë“±)\n",
        "\n",
        ")\n",
        "\n",
        "# if hasattr(model, 'language_model'):\n",
        "#     model = model.language_model\n",
        "# Multimodal ëª¨ë¸ì˜ ê²½ìš°, Language Modelë§Œ ì„ íƒí•  ìˆ˜ë„ ìˆìœ¼ë‚˜\n",
        "# ë¹„ì „ ëª¨ë¸ì˜ ì‚¬ì´ì¦ˆê°€ ë§¤ìš° ì‘ê¸° ë•Œë¬¸ì— í° ì°¨ì´ëŠ” ì—†ìŒ\n",
        "\n",
        "# Train X (eval)\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRYqBlyQ14aP"
      },
      "source": [
        "ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¨ ë’¤ì—ëŠ” Text-Generation íŒŒì´í”„ë¼ì¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤.   \n",
        "ì´ ë•Œ, ì ì ˆí•œ ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•˜ê¸° ìœ„í•´ ëª¨ë¸ í™ˆí˜ì´ì§€ë¥¼ ì°¸ê³ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gFbGhuK14aP",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace\n",
        "\n",
        "# Sampling íŒŒë¼ë¯¸í„° ì„¤ì •: https://huggingface.co/google/gemma-3-12b-it\n",
        "# í™ˆí˜ì´ì§€ ê¶Œì¥ ì‚¬ì–‘ì´ì§€ë§Œ, ì ì ˆí•˜ê²Œ ë°”ê¿”ë„ ë¨\n",
        "gen_config = dict(\n",
        "    do_sample=True,\n",
        "    max_new_tokens=2048,\n",
        "    repetition_penalty = 1.05,\n",
        "\n",
        "    temperature = 1.0,\n",
        "    top_p = 0.95,\n",
        "    top_k = 64\n",
        ")\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False,\n",
        "    # return_full_text=True <-- ì…ë ¥ í”„ë¡¬í”„íŠ¸ë¥¼ í¬í•¨í•œ ì „ì²´ ì¶œë ¥í•˜ê¸°\n",
        "    **gen_config\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a-z0Pbt14aP"
      },
      "source": [
        "ëª¨ë¸ì˜ í† í¬ë‚˜ì´ì €ë¥¼ í™•ì¸í•˜ì—¬, ì±„íŒ… í…œí”Œë¦¿ êµ¬ì„±ì„ í™•ì¸í•©ë‹ˆë‹¤.   \n",
        "ì „ì²´ë¥¼ ì´í•´í•  í•„ìš”ëŠ” ì—†ì§€ë§Œ, Toolì´ë‚˜ Functionì´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRHzfqUk14aP"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.chat_template)\n",
        "# Tool ë¯¸ì§€ì› ğŸ˜£"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqWVrzLe14aP"
      },
      "source": [
        "ëª¨ë¸ì„ ë­ì²´ì¸ê³¼ ì—°ë™í•©ë‹ˆë‹¤.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZsarwsm14aP"
      },
      "outputs": [],
      "source": [
        "base_llm = HuggingFacePipeline(pipeline=pipe, pipeline_kwargs=gen_config)\n",
        "# base_llm: ì…ë ¥ ì „ Tokenizerë¡œ í…œí”Œë¦¿ ë³€í™˜ í•„ìˆ˜, ìŠ¤íŠ¸ë¦¬ë° ê°€ëŠ¥\n",
        "\n",
        "llm = ChatHuggingFace(llm=base_llm, tokenizer=tokenizer)\n",
        "# llm: ìë™ í…œí”Œë¦¿ ë³€í™˜(ChatGoogleGenAIì™€ ë™ì¼), ìŠ¤íŠ¸ë¦¬ë° ë¶ˆê°€"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPs0pT9z14aP"
      },
      "outputs": [],
      "source": [
        "def convert_chat(messages, add_generation_prompt = True):\n",
        "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=add_generation_prompt)\n",
        "\n",
        "example = [{'role':'user', 'content':'ì•ˆë…•'}]\n",
        "convert_chat(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tf93bsuV14aQ"
      },
      "outputs": [],
      "source": [
        "for s in base_llm.stream(convert_chat(example)):\n",
        "    print(s, end='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lG9dLxzP14aQ"
      },
      "outputs": [],
      "source": [
        "llm.invoke(\"ì•ˆë…•?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwnCxMeP14aQ"
      },
      "source": [
        "## 2. HuggingFace LLMê³¼ íˆ´ ì—°ë™í•˜ê¸°"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc0lmLMh14aQ"
      },
      "source": [
        "ë¨¼ì € íˆ´ì„ ì„¤ì •í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxxyvxZn14aQ"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_core.tools import tool\n",
        "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
        "from datetime import datetime\n",
        "\n",
        "tavily_search = TavilySearchResults(\n",
        "    max_results=3)\n",
        "\n",
        "repl_tool = PythonREPLTool()\n",
        "repl_tool.invoke(\"for i in range(10): print(i)\")\n",
        "\n",
        "\n",
        "@tool\n",
        "def current_date() -> str:\n",
        "    \"í˜„ì¬ ë‚ ì§œë¥¼ %y-%m-%d í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.\"\n",
        "    return datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "\n",
        "\n",
        "tools = [tavily_search, repl_tool, current_date]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPAdZG1H14aQ"
      },
      "source": [
        "`ChatHuggingFace()`ì—ë„ `bind_tools()`ì´ êµ¬í˜„ë˜ì–´ ìˆìœ¼ë‚˜,   \n",
        "ì‹¤ì œë¡œ ì‹¤í–‰ë˜ì§€ ì•ŠëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZoPFhiN14aQ"
      },
      "outputs": [],
      "source": [
        "llm_with_tools = llm.bind_tools(tools)\n",
        "llm_with_tools.invoke(\"ì˜¤ëŠ˜ ë‚ ì§œê°€ ì–´ë–»ê²Œ ë˜ë‹ˆ?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwvRgFfO14aQ"
      },
      "outputs": [],
      "source": [
        "llm_with_tools.invoke(\"ë„ˆëŠ” ì–´ë–¤ íˆ´ì´ë‚˜ í•¨ìˆ˜ë¥¼ ê°€ì§€ê³  ìˆë‹ˆ?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rP-lWBN14aQ"
      },
      "source": [
        "ì´ëŸ° ê²½ìš°, íˆ´ì„ ì‹¤í–‰í•˜ê¸° ìœ„í•´ì„œëŠ” ì§ì ‘ ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ êµ¬ì„±í•´ì•¼ í•©ë‹ˆë‹¤.   \n",
        "Tool ì—­í•  ë˜í•œ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ User ì—­í• ì„ íˆ´ ëŒ€ìš©ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBkBjlRE14aQ",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "llm_with_tools.kwargs['tools']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pirXIlE14aQ"
      },
      "source": [
        "íˆ´ ì„¤ëª…ì´ ë‹´ê¸´ ë¬¸ìì—´ì„ êµ¬ì„±í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAvTnfq614aQ"
      },
      "outputs": [],
      "source": [
        "tool_desc = str('\\n---\\n'.join([str(x) for x in llm_with_tools.kwargs['tools']]))\n",
        "print(tool_desc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AaY3Syl14aQ"
      },
      "source": [
        "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.   \n",
        "ì„±ëŠ¥ì— ë§¤ìš° ì¤‘ìš”í•œ ì˜í–¥ì„ ë¯¸ì¹˜ë¯€ë¡œ, ì˜ì–´ë¡œ ì‘ì„±í–ˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMNnrs6s14aQ"
      },
      "outputs": [],
      "source": [
        "system_prompt = f'''\n",
        "You are a helpful assistant with tools below.\n",
        "You can decide whether to invoke any functions or not.\n",
        "If you decide to use any of tools.\n",
        "print name and required parameters of the tool within a json blob correctly.\n",
        "For python code, Return the object as a raw dictionary, without escaping quotes or newlines.\n",
        "\n",
        "for tool use: wrap your output within ```tool_code```.\n",
        "\n",
        "Example:\n",
        "```tool_code\n",
        "{{\"name\":'name of tool', \"arguments\":{{List of apparent argument and parameters}}}}\n",
        "```\n",
        "\n",
        "When the output of the tool is provided, it will be wrapped within ``tool_output```\n",
        "Answer accordingly from the result of the tool output.\n",
        "\n",
        "The question might need some sequential, multiple tool execution.\n",
        "Think Step by Step.\n",
        "\n",
        "The following tools are available:\n",
        "{tool_desc}'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zl0MkeqQ14aR"
      },
      "source": [
        "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— ë“¤ì–´ê°€ì•¼ í•˜ëŠ” ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
        "- Tool Format\n",
        "- Tool Call Format\n",
        "- Tool Result Format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLS6guf414aV"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, ToolMessage\n",
        "\n",
        "\n",
        "messages = [SystemMessage(system_prompt),\n",
        "            HumanMessage('ì˜¤ëŠ˜ ë‚ ì§œê°€ ë©°ì¹ ì´ë‹ˆ?')]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izjgV8rS14aW"
      },
      "outputs": [],
      "source": [
        "\n",
        "response = llm.invoke(messages)\n",
        "messages.append(response)\n",
        "response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVRGa7X914aW"
      },
      "outputs": [],
      "source": [
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6okfSfZe14aW"
      },
      "source": [
        "tool_codeë¥¼ ë°›ì•˜ìœ¼ë‹ˆ, í•´ë‹¹ ë‚´ìš©ì„ íŒŒì‹±í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnDI-EaK14aW"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "\n",
        "\n",
        "def parse_tool(text):\n",
        "    try:\n",
        "        text = text.split('```tool_code\\n')[1].split('\\n```')[0]\n",
        "        # tool_codeë¡œ wrapëœ ì¤‘ê°„ ì½”ë“œ ì¶”ì¶œ\n",
        "\n",
        "        parsed = ast.literal_eval(text)\n",
        "        # Dict í˜•íƒœì˜ ê°’ ë³€í™˜ (json loadì™€ ìœ ì‚¬)\n",
        "\n",
        "        name = parsed.get('name')\n",
        "        arguments = parsed.get('arguments', {})\n",
        "        # nameê³¼ argument return\n",
        "        return {'name':name, 'arguments':arguments}\n",
        "    except (ValueError, SyntaxError):\n",
        "        return None\n",
        "\n",
        "result = parse_tool(response.content)\n",
        "name,arguments = result['name'], result['arguments']\n",
        "name, arguments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgwuDSRE14aW"
      },
      "source": [
        "íˆ´ ì‹¤í–‰ì„ ì—°ê²°í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfNnf7x514aW"
      },
      "outputs": [],
      "source": [
        "# íˆ´ ì´ë¦„ê³¼ íˆ´ ì—°ê²°\n",
        "tool_dict = {tool.name: tool for tool in tools}\n",
        "\n",
        "def execute_tool(name, arguments):\n",
        "    # íˆ´ ì‹¤í–‰í•œ ë’¤ tool_outputìœ¼ë¡œ wrap\n",
        "    result = f'''```tool_output\n",
        "{tool_dict[name].invoke(arguments)}\n",
        "```'''\n",
        "    return result\n",
        "\n",
        "tool_result = execute_tool(**parse_tool(response.content))\n",
        "\n",
        "print(tool_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImCgtT-714aW"
      },
      "outputs": [],
      "source": [
        "messages.append(HumanMessage(tool_result))\n",
        "messages[1:]\n",
        "# ì§ˆë¬¸ + Tool ìš”ì²­ + Tool ê²°ê³¼"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIPtqA1514aW"
      },
      "outputs": [],
      "source": [
        "# ê²°ê³¼ í•´ì„\n",
        "response = llm.invoke(messages)\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPtixRcX14aW"
      },
      "outputs": [],
      "source": [
        "messages = [SystemMessage(system_prompt),\n",
        "            HumanMessage('2025ë…„ 4ì›” ë°œí‘œëœ GPT-4.1 ëª¨ë¸ì´ ì–´ë–¤ ëª¨ë¸ì´ì•¼? í•œêµ­ì–´ë¡œ ì„¤ëª…í•´ì¤˜.')]\n",
        "response = llm.invoke(messages)\n",
        "messages.append(response)\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdLc7MVO14aW"
      },
      "outputs": [],
      "source": [
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIOlRjoY14aW"
      },
      "outputs": [],
      "source": [
        "tool_result = execute_tool(**parse_tool(response.content))\n",
        "tool_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dg51FbMB14aW"
      },
      "outputs": [],
      "source": [
        "messages[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwbD7jk214aX"
      },
      "outputs": [],
      "source": [
        "messages.append(HumanMessage(tool_result))\n",
        "response = llm.invoke(messages)\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gojp1ieM14aX"
      },
      "source": [
        "ì¼ë°˜ì ì¸ ì…ì¶œë ¥ ê´€ê³„ì˜ íˆ´ì€ ì´ì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ê°„ë‹¨í•˜ê²Œ êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.   \n",
        "(ë§Œì•½, Python_REPLê³¼ ê°™ì´ argumentê°€ ë³µì¡í•œ íˆ´ì„ ìˆ˜í–‰í•˜ëŠ” ê²½ìš°ì—ëŠ”   \n",
        "ë³„ë„ì˜ í•¨ìˆ˜ë¡œ ë³€í™˜í•˜ê±°ë‚˜ ê²°ê³¼ë¬¼ì„ ìˆ˜ì •í•˜ëŠ” ì‘ì—…ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEPVMsmk14aX"
      },
      "source": [
        "í•´ë‹¹ êµ¬í˜„ì„ í†µí•´, ReAct Agent êµ¬ì¡°ë¥¼ ë§Œë“¤ì–´ ë³´ê² ìŠµë‹ˆë‹¤.    \n",
        "bind_toolsê°€ ì—†ê¸° ë•Œë¬¸ì—, ê¸°ì¡´ì˜ Tool Messageë¥¼ ì‚¬ìš©í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vP38zKqB14aX"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages : Annotated[list, add_messages]   # ë©”ì‹œì§€ ë§¥ë½ì„ ì €ì¥í•˜ëŠ” ë¦¬ìŠ¤íŠ¸\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ur7uj1AG14aX"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import ToolMessage\n",
        "\n",
        "tool_list = {tool.name: tool for tool in tools}\n",
        "# tool ëª©ë¡ dictë¡œ ìƒì„±\n",
        "\n",
        "def tool_node(state):\n",
        "    tool_outputs = []\n",
        "    tool_call_msgs = state['messages'][-1]\n",
        "    # ë§ˆì§€ë§‰ ë©”ì‹œì§€: íˆ´ ì½œë§ ë©”ì‹œì§€\n",
        "    if '```tool_code' in tool_call_msgs.content:\n",
        "        tool_result = execute_tool(**parse_tool(tool_call_msgs.content))\n",
        "        # tool ì‹¤í–‰ ê²°ê³¼ ì–»ê¸° (ê²°ê³¼ëŠ” ```tool_output```)\n",
        "        tool_outputs.append(HumanMessage(tool_result))\n",
        "\n",
        "    return {'messages': tool_outputs}\n",
        "\n",
        "def agent(state):\n",
        "    system_prompt = SystemMessage(f'''\n",
        "You are a helpful assistant with tools below.\n",
        "You can decide whether to invoke any functions or not.\n",
        "If you decide to use any of tools.\n",
        "print name and required parameters of the tool within a json blob correctly.\n",
        "For python code, Return the object as a raw dictionary, without escaping quotes or newlines.\n",
        "\n",
        "for tool use: wrap your output within ```tool_code```.\n",
        "\n",
        "Example:\n",
        "```tool_code\n",
        "{{\"name\":'name of tool', \"arguments\":{{List of apparent argument and parameters}}}}\n",
        "```\n",
        "\n",
        "When the output of the tool is provided, it will be wrapped within ``tool_output```\n",
        "Answer accordingly from the result of the tool output.\n",
        "\n",
        "The question might need some sequential, multiple tool execution.\n",
        "Think Step by Step.\n",
        "\n",
        "The following tools are available:\n",
        "{tool_desc}\n",
        "\n",
        "\n",
        "Answer in Korean.''')\n",
        "\n",
        "\n",
        "    response = llm.invoke([system_prompt] + state[\"messages\"])\n",
        "    return {'messages': response}\n",
        "\n",
        "def tool_needed(state):\n",
        "\n",
        "    last_msg = state['messages'][-1]\n",
        "    if '```tool_code' in last_msg.content: # íˆ´ ì½œë§ì´ í•„ìš”í•˜ë©´\n",
        "        return \"continue\"\n",
        "    else:\n",
        "        return \"finish\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tgTNu8f14aX"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "builder = StateGraph(State)\n",
        "\n",
        "builder.add_node(\"agent\", agent)\n",
        "builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "builder.add_edge(START, 'agent'),\n",
        "builder.add_conditional_edges(\"agent\",\n",
        "                              tool_needed,\n",
        "                               {\"continue\": \"tools\",\"finish\": END})\n",
        "builder.add_edge(\"tools\", \"agent\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIaHRhJq14aX"
      },
      "outputs": [],
      "source": [
        "graph = builder.compile()\n",
        "graph # ìƒì„±í•œ ê·¸ë˜í”„ ì‹œê°í™”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwtyHK-C14aX"
      },
      "outputs": [],
      "source": [
        "response = graph.invoke({'messages':[HumanMessage(content=\"ì˜¤ëŠ˜ ë‚ ì§œì— íƒœì–´ë‚œ ìœ ëª…ì¸ë“¤ ì¡°ì‚¬í•´ì„œ ì•Œë ¤ì¤˜.\")]})\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gDQQHN_14aX"
      },
      "source": [
        "Gemma 3ëŠ” Native Tool Callì„ ì§€ì›í•˜ëŠ” ëª¨ë¸ì€ ì•„ë‹™ë‹ˆë‹¤.  \n",
        "í•˜ì§€ë§Œ, ì–´ëŠ ì •ë„ í•¨ìˆ˜ ì‹¤í–‰ ëŠ¥ë ¥ì„ ë³´ì´ëŠ”ë°ìš”.   \n",
        "\n",
        "ë§ˆì°¬ê°€ì§€ë¡œ, Gemma 3 ì´ì™¸ì˜ LLM ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ì—ë„,  \n",
        "ê°ê°ì˜ Tool ìŠ¤í™ì— ëŒ€í•œ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§„í–‰í•˜ì‹¤ ìˆ˜ ìˆìœ¼ë‚˜, ì„±ëŠ¥ì€ ì¡°ê¸ˆ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.   \n",
        "Ex) Phi-4 https://huggingface.co/microsoft/Phi-4-mini-instruct\n",
        "\n",
        "<br><br>\n",
        "ì´ë²ˆì—ëŠ” Qwen 2.5 7Bë¥¼ ì´ìš©í•´ ì§„í–‰í•´ ë³´ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "**ì„¸ì…˜ ì´ˆê¸°í™” í›„ ì—¬ê¸°ì„œë¶€í„° ë‹¤ì‹œ ì‹œì‘í•´ ì£¼ì„¸ìš”!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSelbmwB14aX",
        "outputId": "13751504-363e-4803-8251-dae0429caee1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv('env')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxqnxWpk14aX",
        "outputId": "0b5b7820-8e92-44a6-f548-9bd2f2042406"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# Model ID: Qwen/Qwen2.5-7B-Instruct\n"
          ]
        }
      ],
      "source": [
        "model_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "print(f\"# Model ID: {model_id}\")\n",
        "# Full Precision ë¡œë“œ: 15.6GB\n",
        "# GPU ë¶€ì¡±ì‹œ ì–‘ìí™”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KNPHUQ5_rKt"
      },
      "outputs": [],
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "# ì–‘ìí™” ì˜µì…˜: 4ë¹„íŠ¸\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    # nf4 ì–‘ìí™”\n",
        "\n",
        "    bnb_4bit_compute_dtype=\"bfloat16\", # ê³„ì‚°ì‹œì—ëŠ” ê¸°ì¡´ ìë£Œí˜• bf16 ì‚¬ìš©\n",
        "    bnb_4bit_use_double_quant=True # ì´ì¤‘ ì–‘ìí™”\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668,
          "referenced_widgets": [
            "98c5923efc05402fa4b06d32ad1588ef",
            "daf5a315673147779de0b999a7d2315b",
            "7878d799f8074aa3a8fc164c0daeab04",
            "26999a37e67148c299927987210755b4",
            "9de1e5c8a8b04dc1891ec35e400e3ced",
            "5369d4cdad1e48a696c4545b36023c75",
            "6bdaf67b2c3042f7a9058b0a7262114d",
            "7fcd670f11ba4e218f009ce0baa01eef",
            "33e9fdbd4cc94f7a9dad92a150046c8e",
            "588182a5c01f4f81883486dc74b6469f",
            "a1deaade948a4ff6acb34df5ac5f26dd"
          ]
        },
        "id": "RnflkPZR14aY",
        "outputId": "c5833b90-b1c1-495c-f1f0-121ed8fd24dd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "98c5923efc05402fa4b06d32ad1588ef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Qwen2ForCausalLM(\n",
              "  (model): Qwen2Model(\n",
              "    (embed_tokens): Embedding(152064, 3584)\n",
              "    (layers): ModuleList(\n",
              "      (0-27): 28 x Qwen2DecoderLayer(\n",
              "        (self_attn): Qwen2Attention(\n",
              "          (q_proj): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
              "          (k_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
              "          (v_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
              "          (o_proj): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
              "        )\n",
              "        (mlp): Qwen2MLP(\n",
              "          (gate_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=18944, out_features=3584, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "    (rotary_emb): Qwen2RotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¡œë“œ\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "\n",
        "    torch_dtype='auto',\n",
        "    device_map={\"\":0},  # 0ë²ˆ GPUì— í• ë‹¹\n",
        "\n",
        "    quantization_config  = quantization_config,\n",
        "\n",
        "    # attn_implementation = 'flash_attention_2'\n",
        "    # flash attention ì„¤ì •\n",
        "    # ë¯¸ì§€ì›/ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš° ì œê±° (T4 ë“±ì˜ êµ¬ë²„ì „ GPU ë“±)\n",
        "\n",
        ")\n",
        "\n",
        "# Train X (eval)\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZshkwSU14aY",
        "outputId": "565ac542-e095-4101-a1ec-45ab0fd9848b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace\n",
        "\n",
        "# Sampling íŒŒë¼ë¯¸í„° ì„¤ì •: https://huggingface.co/Qwen/Qwen2.5-7B-Instruct\n",
        "# í™ˆí˜ì´ì§€ ê¶Œì¥ ì‚¬ì–‘ì´ì§€ë§Œ, ì ì ˆí•˜ê²Œ ë°”ê¿”ë„ ë¨\n",
        "gen_config = dict(\n",
        "    do_sample=True,\n",
        "    max_new_tokens=2048,\n",
        "    repetition_penalty = 1.05,\n",
        "\n",
        "    temperature = 0.1,\n",
        "    top_p = 0.8,\n",
        "    top_k = 20\n",
        ")\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False,\n",
        "    # return_full_text=True <-- ì…ë ¥ í”„ë¡¬í”„íŠ¸ë¥¼ í¬í•¨í•œ ì „ì²´ ì¶œë ¥í•˜ê¸°\n",
        "    **gen_config\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpGHbrzF14aY"
      },
      "source": [
        "Qwen 2.5 ëŠ” Tool ê¸°ëŠ¥ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsDwcaHq14aY",
        "outputId": "5f4f271d-638b-4816-f2af-ac46cad9a71f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{%- if tools %}\n",
            "    {{- '<|im_start|>system\\n' }}\n",
            "    {%- if messages[0]['role'] == 'system' %}\n",
            "        {{- messages[0]['content'] }}\n",
            "    {%- else %}\n",
            "        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n",
            "    {%- endif %}\n",
            "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
            "    {%- for tool in tools %}\n",
            "        {{- \"\\n\" }}\n",
            "        {{- tool | tojson }}\n",
            "    {%- endfor %}\n",
            "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
            "{%- else %}\n",
            "    {%- if messages[0]['role'] == 'system' %}\n",
            "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
            "    {%- else %}\n",
            "        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n",
            "    {%- endif %}\n",
            "{%- endif %}\n",
            "{%- for message in messages %}\n",
            "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
            "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
            "    {%- elif message.role == \"assistant\" %}\n",
            "        {{- '<|im_start|>' + message.role }}\n",
            "        {%- if message.content %}\n",
            "            {{- '\\n' + message.content }}\n",
            "        {%- endif %}\n",
            "        {%- for tool_call in message.tool_calls %}\n",
            "            {%- if tool_call.function is defined %}\n",
            "                {%- set tool_call = tool_call.function %}\n",
            "            {%- endif %}\n",
            "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
            "            {{- tool_call.name }}\n",
            "            {{- '\", \"arguments\": ' }}\n",
            "            {{- tool_call.arguments | tojson }}\n",
            "            {{- '}\\n</tool_call>' }}\n",
            "        {%- endfor %}\n",
            "        {{- '<|im_end|>\\n' }}\n",
            "    {%- elif message.role == \"tool\" %}\n",
            "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
            "            {{- '<|im_start|>user' }}\n",
            "        {%- endif %}\n",
            "        {{- '\\n<tool_response>\\n' }}\n",
            "        {{- message.content }}\n",
            "        {{- '\\n</tool_response>' }}\n",
            "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
            "            {{- '<|im_end|>\\n' }}\n",
            "        {%- endif %}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "{%- if add_generation_prompt %}\n",
            "    {{- '<|im_start|>assistant\\n' }}\n",
            "{%- endif %}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.chat_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDoXXYMK14aY"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_core.tools import tool\n",
        "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
        "from datetime import datetime\n",
        "\n",
        "tavily_search = TavilySearchResults(\n",
        "    max_results=3)\n",
        "\n",
        "repl_tool = PythonREPLTool()\n",
        "\n",
        "\n",
        "@tool\n",
        "def current_date() -> str:\n",
        "    \"í˜„ì¬ ë‚ ì§œë¥¼ %y-%m-%d í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.\"\n",
        "    return datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "\n",
        "\n",
        "tools = [tavily_search, repl_tool, current_date]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTcLxWoK14aY"
      },
      "outputs": [],
      "source": [
        "base_llm = HuggingFacePipeline(pipeline=pipe, pipeline_kwargs=gen_config)\n",
        "# base_llm: ì…ë ¥ ì „ Tokenizerë¡œ í…œí”Œë¦¿ ë³€í™˜ í•„ìˆ˜, ìŠ¤íŠ¸ë¦¬ë° ê°€ëŠ¥\n",
        "\n",
        "llm = ChatHuggingFace(llm=base_llm, tokenizer=tokenizer)\n",
        "# llm: ìë™ í…œí”Œë¦¿ ë³€í™˜(ChatGoogleGenAIì™€ ë™ì¼), ìŠ¤íŠ¸ë¦¬ë° ë¶ˆê°€"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZhrctni14aY"
      },
      "source": [
        "íˆ´ ê¸°ëŠ¥ì´ ìˆì–´ë„, bind_toolsëŠ” ì‚¬ìš©ì´ ì–´ë µìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMKVBn3-14aY",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "llm_with_tools = llm.bind_tools(tools)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BXWUzvg14aY",
        "outputId": "5ce776cf-867d-4083-ff71-cc350f3df970"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='ì£„ì†¡í•©ë‹ˆë‹¤, ì €ëŠ” ì§ì ‘ ì˜¤ëŠ˜ì˜ ë‚ ì§œë¥¼ í™•ì¸í•  ìˆ˜ ìˆëŠ” ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì¼ë°˜ì ìœ¼ë¡œ ì»´í“¨í„°ë‚˜ ìŠ¤ë§ˆíŠ¸í°ì˜ ì‹œìŠ¤í…œ ë‚ ì§œê°€ ì •í™•í•œ ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ë˜ëŠ” ì›¹ì‚¬ì´íŠ¸ë‚˜ ì•±ì„ í†µí•´ í˜„ì¬ ë‚ ì§œì™€ ì‹œê°„ì„ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë¸Œë¼ìš°ì €ì˜ ë‚ ì§œì™€ ì‹œê°„ì„ í™•ì¸í•˜ê±°ë‚˜, Google ê²€ìƒ‰ì°½ì— \"í˜„ì¬ ë‚ ì§œì™€ ì‹œê°„\"ì„ ì…ë ¥í•˜ë©´ í˜„ì¬ ë‚ ì§œì™€ ì‹œê°„ì´ í‘œì‹œë©ë‹ˆë‹¤.', additional_kwargs={}, response_metadata={}, id='run-bf22927c-f7f0-409f-bede-53b21cb2e57c-0')"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm_with_tools.invoke(\"ì˜¤ëŠ˜ ë‚ ì§œê°€ ì–´ë–»ê²Œ ë˜ë‹ˆ? íˆ´ì„ ì‚¬ìš©í•´ì„œ ì•Œë ¤ì¤˜.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXByDlQD14aY"
      },
      "source": [
        "íˆ´ì„ json listë¡œ ì „ë‹¬í•˜ì—¬ í…œí”Œë¦¿ì— ì ìš©í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HT-XCdOo14aY",
        "outputId": "53c65edf-1a1a-4325-d7a3-36370a8dbeba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'type': 'function', 'function': {'name': 'tavily_search_results_json', 'description': 'A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'Python_REPL', 'description': 'A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.', 'parameters': {'properties': {'query': {'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'current_date', 'description': 'í˜„ì¬ ë‚ ì§œë¥¼ %y-%m-%d í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.', 'parameters': {'properties': {}, 'type': 'object'}}}]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "tool_desc = llm_with_tools.kwargs['tools']\n",
        "print(tool_desc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQ_N2uSH14aY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sayymRP14aY"
      },
      "source": [
        "í† í¬ë‚˜ì´ì €ì— toolì„ ì „ë‹¬í•˜ì—¬, ì „ë°˜ì ì¸ í…œí”Œë¦¿ì„ í™•ì¸í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAYKx-ca14aZ",
        "outputId": "44114933-9fc2-4a97-fdf4-8d4382831644"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|im_start|>system\n",
            "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
            "\n",
            "# Tools\n",
            "\n",
            "You may call one or more functions to assist with the user query.\n",
            "\n",
            "You are provided with function signatures within <tools></tools> XML tags:\n",
            "<tools>\n",
            "{\"type\": \"function\", \"function\": {\"name\": \"tavily_search_results_json\", \"description\": \"A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.\", \"parameters\": {\"properties\": {\"query\": {\"description\": \"search query to look up\", \"type\": \"string\"}}, \"required\": [\"query\"], \"type\": \"object\"}}}\n",
            "{\"type\": \"function\", \"function\": {\"name\": \"Python_REPL\", \"description\": \"A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\", \"parameters\": {\"properties\": {\"query\": {\"type\": \"string\"}}, \"required\": [\"query\"], \"type\": \"object\"}}}\n",
            "{\"type\": \"function\", \"function\": {\"name\": \"current_date\", \"description\": \"í˜„ì¬ ë‚ ì§œë¥¼ %y-%m-%d í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.\", \"parameters\": {\"properties\": {}, \"type\": \"object\"}}}\n",
            "</tools>\n",
            "\n",
            "For each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n",
            "<tool_call>\n",
            "{\"name\": <function-name>, \"arguments\": <args-json-object>}\n",
            "</tool_call><|im_end|>\n",
            "<|im_start|>user\n",
            "ì˜¤ëŠ˜ ë‚ ì§œê°€ ì–´ë–»ê²Œ ë¼?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def convert_chat_with_tools(messages, tools=None, add_generation_prompt = True):\n",
        "    return tokenizer.apply_chat_template(messages, tools=tools, tokenize=False, add_generation_prompt=add_generation_prompt)\n",
        "\n",
        "example = [{'role':'user', 'content':'ì˜¤ëŠ˜ ë‚ ì§œê°€ ì–´ë–»ê²Œ ë¼?'}]\n",
        "print(convert_chat_with_tools(example, tools=tool_desc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PwXIVXY14aZ",
        "outputId": "dbd3b369-ce06-4b75-8750-2e4f7b432db4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<tool_call>\n",
            "{\"name\": \"current_date\", \"arguments\": {}}\n",
            "</tool_call>"
          ]
        }
      ],
      "source": [
        "for s in base_llm.stream(convert_chat_with_tools(example, tools=tool_desc)):\n",
        "    print(s, end='')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoA3jD4d14aZ"
      },
      "source": [
        "í•´ë‹¹ ë‚´ìš©ì„ ê°€ì ¸ê°€ì„œ, ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNMMxgw414aZ"
      },
      "outputs": [],
      "source": [
        "# ìµœìƒì˜ ê²°ê³¼ë¥¼ ìœ„í•´, í† í¬ë‚˜ì´ì €ì˜ í…œí”Œë¦¿ì„ ìµœëŒ€í•œ ë”°ë¦…ë‹ˆë‹¤.\n",
        "system_prompt = f'''\n",
        "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
        "\n",
        "# Tools\n",
        "\n",
        "You may call one or more functions to assist with the user query.\n",
        "\n",
        "You are provided with function signatures within <tools></tools> XML tags:\n",
        "<tools>\n",
        "{tool_desc}\n",
        "</tools>\n",
        "\n",
        "For each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n",
        "<tool_call>\n",
        "{{\"name\": <function-name>, \"arguments\": <args-json-object>}}\n",
        "</tool_call>\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqCmg9EF14aZ"
      },
      "source": [
        "Gemmaì™€ ë™ì¼í•˜ê²Œ ì‹¤í–‰í•´ ë³´ê² ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gdb8FThu14aZ",
        "outputId": "633da78d-c5e7-40a4-d085-e6064965ad78"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='<tool_call>\\n{\"name\": \"current_date\", \"arguments\": {}}\\n</tool_call>', additional_kwargs={}, response_metadata={}, id='run-35009295-26a0-4904-bedb-67dd569ee9b9-0')"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, ToolMessage\n",
        "\n",
        "messages = [SystemMessage(system_prompt),\n",
        "            HumanMessage('ì˜¤ëŠ˜ ë‚ ì§œê°€ ë©°ì¹ ì´ë‹ˆ?')]\n",
        "\n",
        "response = llm.invoke(messages)\n",
        "messages.append(response)\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRZQrsfB14aZ"
      },
      "source": [
        "tool_callì˜ í˜•ì‹ì— ë§ì¶° ì½”ë“œë¥¼ ìˆ˜ì •í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b-20HsO14aZ",
        "outputId": "8a5fc277-17d8-46c9-a483-1e4e86307f69"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('current_date', {})"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import ast\n",
        "\n",
        "def parse_tool(text):\n",
        "    try:\n",
        "        text = text.split('<tool_call>\\n')[1].split('\\n</tool_call>')[0]\n",
        "        # tool_codeë¡œ wrapëœ ì¤‘ê°„ ì½”ë“œ ì¶”ì¶œ\n",
        "\n",
        "        parsed = ast.literal_eval(text)\n",
        "        # Dict í˜•íƒœì˜ ê°’ ë³€í™˜ (json loadì™€ ìœ ì‚¬)\n",
        "        name = parsed.get('name')\n",
        "        arguments = parsed.get('arguments', {})\n",
        "        # nameê³¼ argument return\n",
        "        return {'name':name, 'arguments':arguments}\n",
        "    except (ValueError, SyntaxError):\n",
        "        return None\n",
        "\n",
        "result = parse_tool(response.content)\n",
        "name,arguments = result['name'], result['arguments']\n",
        "name, arguments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSUaIpCr14aZ"
      },
      "source": [
        "íˆ´ì„ ì‹¤í–‰í•˜ê³ , ê·¸ ê²°ê³¼ëŠ” ToolMessageë¡œ ì „ë‹¬í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5tRRRDpr14aZ",
        "outputId": "dd872cd3-fbf3-43a7-aa08-a22feec9536a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2025-04-16'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# íˆ´ ì´ë¦„ê³¼ íˆ´ ì—°ê²°\n",
        "tool_dict = {tool.name: tool for tool in tools}\n",
        "\n",
        "def execute_tool(name, arguments):\n",
        "    # íˆ´ ì‹¤í–‰í•œ ë’¤ tool_outputìœ¼ë¡œ wrap\n",
        "    result = tool_dict[name].invoke(arguments)\n",
        "    return str(result)\n",
        "\n",
        "tool_result = execute_tool(**parse_tool(response.content))\n",
        "\n",
        "tool_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ShknVlD14aZ",
        "outputId": "08d8b086-480d-47b7-c8b0-daaca71e5e1c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='ì˜¤ëŠ˜ ë‚ ì§œê°€ ë©°ì¹ ì´ë‹ˆ?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='<tool_call>\\n{\"name\": \"current_date\", \"arguments\": {}}\\n</tool_call>\\n<tool_call>\\n{\"name\": \"current_date\", \"arguments\": {}}\\n</tool_call>\\n<tool_call>\\n{\"name\": \"tavily_search_results_json\", \"arguments\": {\"query\": \"ë‚ ì§œ í™•ì¸\"}}\\n</tool_call>\\n<tool_call>\\n{\"name\": \"current_date\", \"arguments\": {}}\\n</tool_call>\\nä»Šå¤©çš„æ—¥æœŸæ˜¯2024å¹´3æœˆ1æ—¥ã€‚', additional_kwargs={}, response_metadata={}, id='run-342498ab-cb42-4790-855a-4dea89ff229b-0'),\n",
              " HumanMessage(content='2025-04-16', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "messages.append(HumanMessage(tool_result))\n",
        "messages[1:]\n",
        "# ì§ˆë¬¸ + Tool ìš”ì²­ + Tool ê²°ê³¼"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3UQ30AX14aZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfiQVBqt14aZ",
        "outputId": "fbdfd2e8-709e-416c-8234-d6962986cb32"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='æ‚¨æä¾›çš„æ—¥æœŸæ˜¯2025å¹´4æœˆ16æ—¥ã€‚å¦‚æœæ‚¨éœ€è¦ç¡®è®¤è¿™ä¸ªæ—¥æœŸæ˜¯å¦æ­£ç¡®ï¼Œæˆ–è€…éœ€è¦ä¸å½“å‰æ—¥æœŸè¿›è¡Œæ¯”è¾ƒï¼Œè¯·å‘Šè¯‰æˆ‘æ‚¨çš„å…·ä½“éœ€æ±‚ã€‚å¦‚æœåªæ˜¯ç®€å•åœ°æä¾›ä¸€ä¸ªæ—¥æœŸï¼Œé‚£ä¹ˆè¿™å°±æ˜¯è¯¥æ—¥æœŸï¼š2025å¹´4æœˆ16æ—¥ã€‚', additional_kwargs={}, response_metadata={}, id='run-b6eac70c-99e1-4b47-87d5-9538b8119113-0')"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = llm.invoke(messages)\n",
        "messages.append(response)\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZM__QJI14aa"
      },
      "source": [
        "ë™ì¼í•œ êµ¬ì¡°ë¡œ, í”„ë¡¬í”„íŠ¸ë¥¼ ìˆ˜ì •í•˜ì—¬ ì „ì²´ ì—ì´ì „íŠ¸ë¥¼ êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-giuavz14aa"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages : Annotated[list, add_messages]   # ë©”ì‹œì§€ ë§¥ë½ì„ ì €ì¥í•˜ëŠ” ë¦¬ìŠ¤íŠ¸\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_ihlip114aa"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import ToolMessage\n",
        "\n",
        "tool_list = {tool.name: tool for tool in tools}\n",
        "# tool ëª©ë¡ dictë¡œ ìƒì„±\n",
        "\n",
        "def tool_node(state):\n",
        "    tool_outputs = []\n",
        "    tool_call_msgs = state['messages'][-1]\n",
        "    # ë§ˆì§€ë§‰ ë©”ì‹œì§€: íˆ´ ì½œë§ ë©”ì‹œì§€\n",
        "    if '<tool_call>' in tool_call_msgs.content:\n",
        "        tool_result = execute_tool(**parse_tool(tool_call_msgs.content))\n",
        "        # tool ì‹¤í–‰ ê²°ê³¼ ì–»ê¸° (ê²°ê³¼ëŠ” ```tool_output```)\n",
        "        tool_outputs.append(HumanMessage(tool_result))\n",
        "\n",
        "    return {'messages': tool_outputs}\n",
        "\n",
        "def agent(state):\n",
        "    system_prompt = SystemMessage(f'''\n",
        "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
        "\n",
        "# Tools\n",
        "\n",
        "You may call a function to assist with the user query.\n",
        "\n",
        "You are provided with function signatures within <tools></tools> XML tags:\n",
        "<tools>\n",
        "{tool_desc}\n",
        "</tools>\n",
        "\n",
        "For function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n",
        "<tool_call>\n",
        "{{\"name\": <function-name>, \"arguments\": <args-json-object>}}\n",
        "</tool_call>\n",
        "\n",
        "# Tool Usage Rules - VERY IMPORTANT!\n",
        "\n",
        "1.  **Analyze the Task:** Carefully read the user's request and determine if any tools are needed.\n",
        "2.  **One Tool At A Time:** If you need to use a tool, you **MUST** choose and call **only ONE** tool in your response. Do **NOT** issue multiple `<tool_call>` tags in a single turn.\n",
        "3.  **Sequential Execution:** If the user's request requires information from multiple tool calls (e.g., searching for information and then getting the weather based on the search result), you **MUST** perform these calls sequentially.\n",
        "    * First, call the **single** tool needed for the first step.\n",
        "    * Wait for the result of that tool.\n",
        "    * Then, based on the result, decide if another **single** tool call is necessary for the next step. Issue that call in a *new* response turn.\n",
        "\n",
        "Answer in Korean.''')\n",
        "\n",
        "\n",
        "    response = llm.invoke([system_prompt] + state[\"messages\"])\n",
        "    return {'messages': response}\n",
        "\n",
        "def tool_needed(state):\n",
        "\n",
        "    last_msg = state['messages'][-1]\n",
        "    if '<tool_call>' in last_msg.content: # íˆ´ ì½œë§ì´ í•„ìš”í•˜ë©´\n",
        "        return \"continue\"\n",
        "    else:\n",
        "        return \"finish\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpYW7TQ314aa",
        "outputId": "81f36951-15e5-475e-dfc3-73e9285e04cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7f95063d8250>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ë©”ëª¨ë¦¬ ì„¸íŒ…\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "builder = StateGraph(State)\n",
        "\n",
        "builder.add_node(\"agent\", agent)\n",
        "builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "builder.add_edge(START, 'agent'),\n",
        "builder.add_conditional_edges(\"agent\",\n",
        "                              tool_needed,\n",
        "                               {\"continue\": \"tools\",\"finish\": END})\n",
        "builder.add_edge(\"tools\", \"agent\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsuR8tZd14aa",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "memory = MemorySaver()\n",
        "graph = builder.compile(checkpointer = memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIHSRouc14aa",
        "outputId": "01b6ca20-d74d-4805-ce85-d4c849a7d87c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='ì˜¤ëŠ˜ì´ ë©°ì¹ ì´ì•¼?', additional_kwargs={}, response_metadata={}, id='f1228164-6d8e-417d-b298-18d0edb03cb2'),\n",
              "  AIMessage(content='<tool_call>\\n{\"name\": \"current_date\", \"arguments\": {}}\\n</tool_call>', additional_kwargs={}, response_metadata={}, id='run-556125ef-c872-4fab-9fd4-55b031e06221-0'),\n",
              "  HumanMessage(content='2025-04-16', additional_kwargs={}, response_metadata={}, id='d48dce25-fcad-47ef-adaf-cdbf7e8cd902'),\n",
              "  AIMessage(content='ì˜¤ëŠ˜ì€ 2025ë…„ 4ì›” 16ì¼ì…ë‹ˆë‹¤.', additional_kwargs={}, response_metadata={}, id='run-10dd8e0c-fe3c-40ee-862f-8df3def4ecca-0')]}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "\n",
        "response = graph.invoke({'messages':[HumanMessage(content=\"ì˜¤ëŠ˜ì´ ë©°ì¹ ì´ì•¼?\")]}, config)\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llXZkL_714aa",
        "outputId": "6a15ace9-f62a-4a6e-f8bf-c8d6cb176a76"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='ì˜¤ëŠ˜ê³¼ ê°™ì€ ë‚ ì§œì— íƒœì–´ë‚œ ìœ ëª…ì¸ë“¤ ì¡°ì‚¬í•´ì„œ ì•Œë ¤ì¤˜.', additional_kwargs={}, response_metadata={}, id='a78cddde-97c6-40a4-b514-00d38827ed92'),\n",
              "  AIMessage(content='ronics\\n{\"name\": \"tavily_search_results_json\", \"arguments\": {\"query\": \"ìœ ëª…ì¸ ì˜¤ëŠ˜ ê°™ì€ ë‚ ì§œì— íƒœì–´ë‚¨\"}}', additional_kwargs={}, response_metadata={}, id='run-7606bcad-bd2f-4fee-b35e-1e979a6ef3e6-0'),\n",
              "  HumanMessage(content='ì˜¤ëŠ˜ê³¼ ê°™ì€ ë‚ ì§œì— íƒœì–´ë‚œ ìœ ëª…ì¸ë“¤ ì¡°ì‚¬í•´ì„œ ì•Œë ¤ì¤˜.', additional_kwargs={}, response_metadata={}, id='de1b72f4-d897-46a4-92e9-10162db785d7'),\n",
              "  AIMessage(content='ronics\\n{\"name\": \"tavily_search_results_json\", \"arguments\": {\"query\": \"ìœ ëª…ì¸ ì˜¤ëŠ˜ ê°™ì€ ë‚ ì§œì— íƒœì–´ë‚œ ì‚¬ëŒ\"}}', additional_kwargs={}, response_metadata={}, id='run-903fce71-d0fa-4d57-b390-a5582c23cc85-0'),\n",
              "  HumanMessage(content='ì˜¤ëŠ˜ê³¼ ê°™ì€ ë‚ ì§œì— íƒœì–´ë‚œ ìœ ëª…ì¸ë“¤ ì¡°ì‚¬í•´ì„œ ì•Œë ¤ì¤˜.', additional_kwargs={}, response_metadata={}, id='c5684997-c263-402e-9396-e47ece92e422'),\n",
              "  AIMessage(content='ronics\\n{\"name\": \"tavily_search_results_json\", \"arguments\": {\"query\": \"ìœ ëª…ì¸ ì˜¤ëŠ˜ ìƒì¼\"}}', additional_kwargs={}, response_metadata={}, id='run-7fba29d1-44eb-418f-a070-e734c744a7f7-0')]}"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# sLLM: ê²°ê³¼ê°€ Inconsistent...\n",
        "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
        "\n",
        "response = graph.invoke({'messages':[HumanMessage(content=\"ì˜¤ëŠ˜ê³¼ ê°™ì€ ë‚ ì§œì— íƒœì–´ë‚œ ìœ ëª…ì¸ë“¤ ì¡°ì‚¬í•´ì„œ ì•Œë ¤ì¤˜.\")]}, config)\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3opOZPT-14aa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlt75bEw14aa",
        "outputId": "f81775e1-b011-4b3c-df11-488e584d7656"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='2025ë…„ 4ì›”ì— ì¶œì‹œëœ GPT-4.1 ëª¨ë¸ì— ëŒ€í•´ ì†Œê°œí•´ì¤˜.', additional_kwargs={}, response_metadata={}, id='8d9abf31-7608-4d03-9afb-706d2afcac5a'),\n",
              "  AIMessage(content='ë¨¼ì €, í˜„ì¬ ì‹œì ì—ì„œ GPT-4.1ì´ë¼ëŠ” ëª¨ë¸ì€ ê³µê°œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. GPT-4ëŠ” OpenAIì—ì„œ 2023ë…„ 9ì›”ì— ë°œí‘œí•œ ìµœì‹  ëª¨ë¸ì´ë©°, ê·¸ ì´í›„ë¡œ ë” ì—…ë°ì´íŠ¸ëœ ë²„ì „ì€ ì•„ì§ ì•Œë ¤ì ¸ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ë”°ë¼ì„œ, 2025ë…„ 4ì›”ì— ì¶œì‹œëœ GPT-4.1ì— ëŒ€í•œ ì •ë³´ëŠ” ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\\n\\ní•˜ì§€ë§Œ, GPT-4ì˜ íŠ¹ì§•ê³¼ ê¸°ëŒ€ë˜ëŠ” ê°œì„ ì ì„ ë°”íƒ•ìœ¼ë¡œ ì¶”ì¸¡í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. GPT-4ëŠ” ì´ë¯¸ ëŒ€í™”í˜• AIì˜ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œì¼°ê³ , ë”ìš± ì •í™•í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ê°€ ê°€ëŠ¥í•˜ë„ë¡ í–ˆìŠµë‹ˆë‹¤. ë§Œì•½ GPT-4.1ì´ ìˆë‹¤ë©´, ì•„ë§ˆë„ ë‹¤ìŒê³¼ ê°™ì€ ê°œì„ ì ë“¤ì´ ìˆì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤:\\n\\n1. **ë” ë†’ì€ ì •í™•ì„±**: ëª¨ë¸ì´ í•™ìŠµí•œ ë°ì´í„°ì˜ ì–‘ì´ ëŠ˜ì–´ë‚˜ë©´ì„œ, ë”ìš± ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ë³´ë¥¼ ì œê³µí•  ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤.\\n2. **ë” ë‚˜ì€ ì´í•´ë ¥**: ëŒ€í™”í˜• AIì˜ ì´í•´ë ¥ì´ ë”ìš± í–¥ìƒë˜ì–´, ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë” ì˜ ëŒ€ë‹µí•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.\\n3. **ë” ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”**: ëŒ€í™”ì˜ ìì—°ìŠ¤ëŸ¬ì›€ì´ ë”ìš± í–¥ìƒë˜ì–´, ì‚¬ìš©ìì™€ì˜ ëŒ€í™”ê°€ ë”ìš± ì›í™œí•´ì§ˆ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.\\n\\nìœ„ ë‚´ìš©ì€ ì¶”ì •ì‚¬í•­ì´ë©°, ì‹¤ì œ GPT-4.1ì˜ ê²½ìš° ê³µì‹ì ì¸ ë°œí‘œë‚˜ ë°œí‘œìê°€ ì œê³µí•˜ëŠ” ì •ë³´ë¥¼ ì°¸ê³ í•´ì•¼ í•©ë‹ˆë‹¤.', additional_kwargs={}, response_metadata={}, id='run-4933bc55-b51d-4971-b003-d383b30c1cd3-0'),\n",
              "  HumanMessage(content='2025ë…„ 4ì›”ì— ì¶œì‹œëœ GPT-4.1 ëª¨ë¸ì— ëŒ€í•´ ì†Œê°œí•´ì¤˜.', additional_kwargs={}, response_metadata={}, id='1591623c-b6f0-4973-ad70-314b0973ce7a'),\n",
              "  AIMessage(content='í˜„ì¬ê¹Œì§€ ê³µê°œëœ ì •ë³´ì— ë”°ë¥´ë©´, GPT-4.1ì´ë¼ëŠ” ëª¨ë¸ì€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. GPT-4ëŠ” 2023ë…„ 9ì›”ì— OpenAIì—ì„œ ë°œí‘œí•œ ìµœì‹  ëª¨ë¸ì´ë©°, ì´í›„ ë” ì—…ë°ì´íŠ¸ëœ ë²„ì „ì€ ì•„ì§ ì•Œë ¤ì ¸ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.\\n\\nê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³ , 2025ë…„ 4ì›”ì— ì¶œì‹œëœ GPT-4.1 ëª¨ë¸ì— ëŒ€í•œ ê°€ìƒì˜ ì†Œê°œë¥¼ í•´ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ì¶”ì¸¡ì ì¸ ë‚´ìš©ì´ë¯€ë¡œ ì‹¤ì œ ì œí’ˆê³¼ëŠ” ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\n### GPT-4.1 ëª¨ë¸ ì†Œê°œ\\n\\n**1. ê°œìš”**\\nGPT-4.1ì€ 2025ë…„ 4ì›”ì— ì¶œì‹œëœ ìµœì‹  ëŒ€í™”í˜• AI ëª¨ë¸ì…ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ GPT-4ì˜ ì„±ëŠ¥ì„ ê¸°ë°˜ìœ¼ë¡œ ë”ìš± ê°•í™”ëœ ê¸°ëŠ¥ê³¼ ì„±ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.\\n\\n**2. ì£¼ìš” íŠ¹ì§•**\\n\\n- **ë” ë†’ì€ ì •í™•ì„±**: GPT-4.1ì€ ë” ë§ì€ ë°ì´í„°ì™€ ë” ê°•ë ¥í•œ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ ë”ìš± ì •í™•í•œ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ëŠ” ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œì˜ ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.\\n  \\n- **ë” ë‚˜ì€ ì´í•´ë ¥**: ëª¨ë¸ì˜ ì´í•´ë ¥ì´ ë”ìš± í–¥ìƒë˜ì–´, ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë”ìš± ì •í™•í•˜ê²Œ ëŒ€ë‹µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ëŒ€í™”í˜• AIì˜ ì‚¬ìš©ì„±ì„ í¬ê²Œ í–¥ìƒì‹œí‚µë‹ˆë‹¤.\\n  \\n- **ë” ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”**: ëŒ€í™”ì˜ ìì—°ìŠ¤ëŸ¬ì›€ì´ ë”ìš± í–¥ìƒë˜ì–´, ì‚¬ìš©ìì™€ì˜ ëŒ€í™”ê°€ ë”ìš± ì›í™œí•´ì§‘ë‹ˆë‹¤. ì´ëŠ” ì‚¬ìš©ì ê²½í—˜ì„ í¬ê²Œ ê°œì„ í•©ë‹ˆë‹¤.\\n  \\n- **ë” ë¹ ë¥¸ ì‘ë‹µ ì‹œê°„**: GPT-4.1ì€ ë” ë¹ ë¥¸ ì²˜ë¦¬ ì†ë„ë¥¼ í†µí•´ ì‚¬ìš©ìì—ê²Œ ì¦‰ê°ì ì¸ ì‘ë‹µì„ ì œê³µí•©ë‹ˆë‹¤. ì´ëŠ” ì‚¬ìš©ìì™€ì˜ ìƒí˜¸ì‘ìš©ì„ ë”ìš± ì›í™œí•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.\\n  \\n- **ë” ì•ˆì „í•œ ìš´ì˜**: ëª¨ë¸ì˜ ì•ˆì „ì„±ì´ ë”ìš± ê°•í™”ë˜ì–´, ì‚¬ìš©ì ê°œì¸ì •ë³´ ë³´í˜¸ì™€ ë°ì´í„° ë³´ì•ˆì„ ë”ìš± ê°•í™”í•©ë‹ˆë‹¤. ì´ëŠ” ì‚¬ìš©ì ì‹ ë¢°ë¥¼ í¬ê²Œ ì¦ê°€ì‹œí‚µë‹ˆë‹¤.\\n\\n**3. í™œìš© ì‚¬ë¡€**\\n\\n- **êµìœ¡**: êµìœ¡ ë¶„ì•¼ì—ì„œëŠ” GPT-4.1ì„ í†µí•´ í•™ìƒë“¤ì˜ ì§ˆë¬¸ì— ë”ìš± ì •í™•í•˜ê²Œ ëŒ€ë‹µí•˜ê³ , ê°œì¸í™”ëœ í•™ìŠµ ê²½í—˜ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n  \\n- **ê±´ê°• ê´€ë¦¬**: ì˜ë£Œ ë¶„ì•¼ì—ì„œëŠ” GPT-4.1ì„ í†µí•´ í™˜ìì˜ ê±´ê°• ìƒíƒœë¥¼ ëª¨ë‹ˆí„°ë§í•˜ê³ , ê°œì¸í™”ëœ ì¹˜ë£Œ ê³„íšì„ ì œì•ˆí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n  \\n- **ë¹„ì¦ˆë‹ˆìŠ¤ ì§€ì›**: ë¹„ì¦ˆë‹ˆìŠ¤ì—ì„œëŠ” GPT-4.1ì„ í†µí•´ ê³ ê° ì„œë¹„ìŠ¤ë¥¼ ê°œì„ í•˜ê³ , íš¨ìœ¨ì ì¸ ì˜ì‚¬ê²°ì •ì„ ì§€ì›í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\nì´ì²˜ëŸ¼, GPT-4.1ì€ ëŒ€í™”í˜• AIì˜ ì„±ëŠ¥ì„ ë”ìš± í–¥ìƒì‹œí‚¤ë©°, ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œì˜ í™œìš©ë„ë¥¼ í¬ê²Œ í™•ì¥í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤.', additional_kwargs={}, response_metadata={}, id='run-f3e25724-b5ea-4ea6-b5ec-ef2b13b8b1fa-0')]}"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ê²€ìƒ‰ 2\n",
        "config = {\"configurable\": {\"thread_id\": \"3\"}}\n",
        "\n",
        "response = graph.invoke({'messages':[HumanMessage(content=\"2025ë…„ 4ì›”ì— ì¶œì‹œëœ GPT-4.1 ëª¨ë¸ì— ëŒ€í•´ ì†Œê°œí•´ì¤˜.\")]},\n",
        "                       config)\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRbTmsmI14aa"
      },
      "source": [
        "Toolì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” Agentic Workì˜ ê²½ìš°ì—ëŠ” ê¸°ì¡´ LLMê³¼ ë™ì¼í•˜ê²Œ ì‹¤í–‰ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruxvj95514aa"
      },
      "source": [
        "ë‹¨, Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ê²½ìš° ë™ì‹œ ì‹¤í–‰ ë“±ì˜ ë©”ì»¤ë‹ˆì¦˜ì´ ìµœì í™”ë˜ì–´ ìˆì§€ ì•Šê¸° ë•Œë¬¸ì—,   \n",
        "ì´ì „ì˜ Send()ì™€ ê°™ì´ ë³‘ë ¬ ì‹¤í–‰ì´ í•„ìš”í•œ ë¬¸ì œì—ì„œëŠ” ê²°ê³¼ê°€ ì œëŒ€ë¡œ ë‚˜ì˜¤ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.    \n",
        "\n",
        "ì´ë¥¼ í•´ê²°í•˜ëŠ” ë°©ë²•ì€, Ollamaë‚˜ vLLMê³¼ ê°™ì€ ì„œë¹™ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "26999a37e67148c299927987210755b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_588182a5c01f4f81883486dc74b6469f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a1deaade948a4ff6acb34df5ac5f26dd",
            "value": "â€‡4/4â€‡[01:22&lt;00:00,â€‡19.82s/it]"
          }
        },
        "33e9fdbd4cc94f7a9dad92a150046c8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5369d4cdad1e48a696c4545b36023c75": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "588182a5c01f4f81883486dc74b6469f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bdaf67b2c3042f7a9058b0a7262114d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7878d799f8074aa3a8fc164c0daeab04": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7fcd670f11ba4e218f009ce0baa01eef",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_33e9fdbd4cc94f7a9dad92a150046c8e",
            "value": 4
          }
        },
        "7fcd670f11ba4e218f009ce0baa01eef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98c5923efc05402fa4b06d32ad1588ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_daf5a315673147779de0b999a7d2315b",
              "IPY_MODEL_7878d799f8074aa3a8fc164c0daeab04",
              "IPY_MODEL_26999a37e67148c299927987210755b4"
            ],
            "layout": "IPY_MODEL_9de1e5c8a8b04dc1891ec35e400e3ced"
          }
        },
        "9de1e5c8a8b04dc1891ec35e400e3ced": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1deaade948a4ff6acb34df5ac5f26dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "daf5a315673147779de0b999a7d2315b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5369d4cdad1e48a696c4545b36023c75",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6bdaf67b2c3042f7a9058b0a7262114d",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

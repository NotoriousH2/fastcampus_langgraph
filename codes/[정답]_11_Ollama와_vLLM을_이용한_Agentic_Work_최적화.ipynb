{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsYrzxX5DaqK"
      },
      "source": [
        "# [실습] Ollama과 vLLM을 이용한 Agentic Work 만들기   \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBGUmtU0ChST"
      },
      "source": [
        "## 라이브러리 설치\n",
        "\n",
        "\n",
        "이번 실습은 무료 코랩(T4 GPU)이 아닌 고성능 GPU 라이브러리에서 진행합니다.   \n",
        "무료 코랩으로 진행하시는 경우, GPU 성능의 한계로 vLLM 실행이 어렵습니다.\n",
        "\n",
        "<br><br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lPVjc9mChST",
        "outputId": "531f969b-be62-4110-ed22-d3180797a843",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting vllm\n",
            "  Downloading vllm-0.8.4-cp38-abi3-manylinux1_x86_64.whl.metadata (27 kB)\n",
            "Collecting flash-attn\n",
            "  Downloading flash_attn-2.7.4.post1.tar.gz (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m426.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: pyzmq in /usr/local/lib/python3.11/dist-packages (24.0.1)\n",
            "Collecting flashinfer-python\n",
            "  Downloading flashinfer_python-0.2.5.tar.gz (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m176.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting openai\n",
            "  Downloading openai-1.74.0-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.12-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.13.1)\n",
            "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
            "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Collecting regex!=2019.12.17 (from transformers)\n",
            "  Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
            "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting safetensors>=0.4.3 (from transformers)\n",
            "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting tqdm>=4.27 (from transformers)\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.4.1+cu124)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.0)\n",
            "Collecting cachetools (from vllm)\n",
            "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting sentencepiece (from vllm)\n",
            "  Downloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting blake3 (from vllm)\n",
            "  Downloading blake3-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Collecting py-cpuinfo (from vllm)\n",
            "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
            "Collecting protobuf (from vllm)\n",
            "  Downloading protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
            "Collecting fastapi>=0.115.0 (from fastapi[standard]>=0.115.0->vllm)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting aiohttp (from vllm)\n",
            "  Downloading aiohttp-3.11.16-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting pydantic>=2.9 (from vllm)\n",
            "  Downloading pydantic-2.11.3-py3-none-any.whl.metadata (65 kB)\n",
            "Requirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.21.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from vllm) (10.2.0)\n",
            "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm)\n",
            "  Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting tiktoken>=0.6.0 (from vllm)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting lm-format-enforcer<0.11,>=0.10.11 (from vllm)\n",
            "  Downloading lm_format_enforcer-0.10.11-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting llguidance<0.8.0,>=0.7.9 (from vllm)\n",
            "  Downloading llguidance-0.7.14-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting outlines==0.1.11 (from vllm)\n",
            "  Downloading outlines-0.1.11-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting lark==1.2.2 (from vllm)\n",
            "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting xgrammar==0.1.18 (from vllm)\n",
            "  Downloading xgrammar-0.1.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting typing_extensions>=4.10 (from vllm)\n",
            "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting filelock (from transformers)\n",
            "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting partial-json-parser (from vllm)\n",
            "  Downloading partial_json_parser-0.2.1.1.post5-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting msgspec (from vllm)\n",
            "  Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting gguf>=0.13.0 (from vllm)\n",
            "  Downloading gguf-0.14.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: importlib_metadata in /usr/lib/python3/dist-packages (from vllm) (4.6.4)\n",
            "Collecting mistral_common>=1.5.4 (from mistral_common[opencv]>=1.5.4->vllm)\n",
            "  Downloading mistral_common-1.5.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting opencv-python-headless>=4.11.0 (from vllm)\n",
            "  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting einops (from vllm)\n",
            "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting compressed-tensors==0.9.3 (from vllm)\n",
            "  Downloading compressed_tensors-0.9.3-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting depyf==0.18.0 (from vllm)\n",
            "  Downloading depyf-0.18.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting cloudpickle (from vllm)\n",
            "  Downloading cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting watchfiles (from vllm)\n",
            "  Downloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: python-json-logger in /usr/local/lib/python3.11/dist-packages (from vllm) (2.0.7)\n",
            "Collecting scipy (from vllm)\n",
            "  Downloading scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting ninja (from vllm)\n",
            "  Using cached ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
            "Collecting opentelemetry-sdk<1.27.0,>=1.26.0 (from vllm)\n",
            "  Downloading opentelemetry_sdk-1.26.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-api<1.27.0,>=1.26.0 (from vllm)\n",
            "  Downloading opentelemetry_api-1.26.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp<1.27.0,>=1.26.0 (from vllm)\n",
            "  Downloading opentelemetry_exporter_otlp-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-semantic-conventions-ai<0.5.0,>=0.4.1 (from vllm)\n",
            "  Downloading opentelemetry_semantic_conventions_ai-0.4.3-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting numba==0.61.2 (from vllm)\n",
            "  Downloading numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
            "Collecting ray!=2.44.*,>=2.43.0 (from ray[cgraph]!=2.44.*,>=2.43.0->vllm)\n",
            "  Downloading ray-2.43.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Collecting torch<3,>=2.0 (from bitsandbytes)\n",
            "  Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting torchaudio==2.6.0 (from vllm)\n",
            "  Downloading torchaudio-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting torchvision==0.21.0 (from vllm)\n",
            "  Downloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting xformers==0.0.29.post2 (from vllm)\n",
            "  Downloading xformers-0.0.29.post2-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting astor (from depyf==0.18.0->vllm)\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting dill (from depyf==0.18.0->vllm)\n",
            "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba==0.61.2->vllm)\n",
            "  Downloading llvmlite-0.44.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
            "Collecting interegular (from outlines==0.1.11->vllm)\n",
            "  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (3.1.3)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (1.6.0)\n",
            "Collecting diskcache (from outlines==0.1.11->vllm)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: referencing in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (0.35.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (4.23.0)\n",
            "Collecting pycountry (from outlines==0.1.11->vllm)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting airportsdata (from outlines==0.1.11->vllm)\n",
            "  Downloading airportsdata-20250224-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting outlines_core==0.1.26 (from outlines==0.1.11->vllm)\n",
            "  Downloading outlines_core-0.1.26-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.2.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.2.0 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Collecting sympy==1.13.1 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.6.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.27.2)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Collecting langchain-core<1.0.0,>=0.3.49 (from langchain_openai)\n",
            "  Downloading langchain_core-0.3.51-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting fastapi-cli>=0.0.5 (from fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
            "  Downloading fastapi_cli-0.0.7-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting jinja2 (from outlines==0.1.11->vllm)\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting python-multipart>=0.0.18 (from fastapi[standard]>=0.115.0->vllm)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting email-validator>=2.0.0 (from fastapi[standard]>=0.115.0->vllm)\n",
            "  Downloading email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting uvicorn>=0.12.0 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
            "  Downloading uvicorn-0.34.1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Collecting hf-xet>=0.1.4 (from huggingface-hub[hf_xet]>=0.30.0->vllm)\n",
            "  Downloading hf_xet-1.0.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (494 bytes)\n",
            "Collecting langsmith<0.4,>=0.1.125 (from langchain-core<1.0.0,>=0.3.49->langchain_openai)\n",
            "  Downloading langsmith-0.3.31-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<1.0.0,>=0.3.49->langchain_openai)\n",
            "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.49->langchain_openai)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting pillow (from vllm)\n",
            "  Downloading pillow-11.2.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api<1.27.0,>=1.26.0->vllm)\n",
            "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting importlib_metadata (from vllm)\n",
            "  Downloading importlib_metadata-8.0.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/lib/python3/dist-packages (from importlib_metadata->vllm) (1.0.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc==1.26.0 (from opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http==1.26.0 (from opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm)\n",
            "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting grpcio<2.0.0,>=1.0.0 (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm)\n",
            "  Downloading grpcio-1.71.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.26.0 (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.26.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.26.0 (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm)\n",
            "  Downloading opentelemetry_proto-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting protobuf (from vllm)\n",
            "  Downloading protobuf-4.25.6-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting opentelemetry-semantic-conventions==0.47b0 (from opentelemetry-sdk<1.27.0,>=1.26.0->vllm)\n",
            "  Downloading opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic>=2.9->vllm)\n",
            "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.33.1 (from pydantic>=2.9->vllm)\n",
            "  Downloading pydantic_core-2.33.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting typing-inspection>=0.4.0 (from pydantic>=2.9->vllm)\n",
            "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting click>=7.0 (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm)\n",
            "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting msgpack<2.0.0,>=1.0.0 (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm)\n",
            "  Downloading msgpack-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
            "Collecting aiosignal (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm)\n",
            "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting frozenlist (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm)\n",
            "  Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting cupy-cuda12x (from ray[cgraph]!=2.44.*,>=2.43.0->vllm)\n",
            "  Downloading cupy_cuda12x-13.4.1-cp311-cp311-manylinux2014_x86_64.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.2.3)\n",
            "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->vllm)\n",
            "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (24.2.0)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->vllm)\n",
            "  Downloading multidict-6.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting propcache>=0.2.0 (from aiohttp->vllm)\n",
            "  Downloading propcache-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp->vllm)\n",
            "  Downloading yarl-1.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (71 kB)\n",
            "Collecting wrapt<2,>=1.10 (from deprecated>=1.2.6->opentelemetry-api<1.27.0,>=1.26.0->vllm)\n",
            "  Downloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting dnspython>=2.0.0 (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting typer>=0.12.3 (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
            "  Downloading typer-0.15.2-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting rich-toolkit>=0.11.1 (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
            "  Downloading rich_toolkit-0.14.1-py3-none-any.whl.metadata (999 bytes)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->outlines==0.1.11->vllm) (2.1.5)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.49->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm) (2023.12.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm) (0.20.0)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain_openai)\n",
            "  Downloading orjson-3.10.16-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
            "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain_openai)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain_openai)\n",
            "  Downloading zstandard-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting websockets>=10.4 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
            "  Downloading websockets-15.0.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting fastrlock>=0.5 (from cupy-cuda12x->ray[cgraph]!=2.44.*,>=2.43.0->vllm)\n",
            "  Downloading fastrlock-0.8.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting rich>=13.7.1 (from rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
            "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting shellingham>=1.3.0 (from typer>=0.12.3->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.18.0)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m351.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m348.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
            "Downloading vllm-0.8.4-cp38-abi3-manylinux1_x86_64.whl (294.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.1/294.1 MB\u001b[0m \u001b[31m166.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading compressed_tensors-0.9.3-py3-none-any.whl (98 kB)\n",
            "Downloading depyf-0.18.0-py3-none-any.whl (38 kB)\n",
            "Downloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
            "Downloading numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m347.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outlines-0.1.11-py3-none-any.whl (87 kB)\n",
            "Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m219.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.6.0-cp311-cp311-manylinux1_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m423.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m458.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.29.post2-cp311-cp311-manylinux_2_28_x86_64.whl (44.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 MB\u001b[0m \u001b[31m186.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading xgrammar-0.1.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m221.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m341.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m392.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m417.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m267.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m358.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m313.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m355.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m265.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m288.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m232.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m453.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "Downloading outlines_core-0.1.26-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (343 kB)\n",
            "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m226.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m233.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.74.0-py3-none-any.whl (644 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.8/644.8 kB\u001b[0m \u001b[31m204.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.12-py3-none-any.whl (61 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
            "Downloading gguf-0.14.0-py3-none-any.whl (76 kB)\n",
            "Downloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
            "Downloading jiter-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (351 kB)\n",
            "Downloading langchain_core-0.3.51-py3-none-any.whl (423 kB)\n",
            "Downloading llguidance-0.7.14-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m207.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lm_format_enforcer-0.10.11-py3-none-any.whl (44 kB)\n",
            "Downloading mistral_common-1.5.4-py3-none-any.whl (6.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m402.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 MB\u001b[0m \u001b[31m379.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.26.0-py3-none-any.whl (61 kB)\n",
            "Downloading importlib_metadata-8.0.0-py3-none-any.whl (24 kB)\n",
            "Downloading opentelemetry_exporter_otlp-1.26.0-py3-none-any.whl (7.0 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_grpc-1.26.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_http-1.26.0-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.26.0-py3-none-any.whl (17 kB)\n",
            "Downloading opentelemetry_proto-1.26.0-py3-none-any.whl (52 kB)\n",
            "Downloading opentelemetry_sdk-1.26.0-py3-none-any.whl (109 kB)\n",
            "Downloading opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl (138 kB)\n",
            "Downloading opentelemetry_semantic_conventions_ai-0.4.3-py3-none-any.whl (5.4 kB)\n",
            "Downloading pillow-11.2.1-cp311-cp311-manylinux_2_28_x86_64.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m404.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (19 kB)\n",
            "Downloading protobuf-4.25.6-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "Downloading pydantic-2.11.3-py3-none-any.whl (443 kB)\n",
            "Downloading pydantic_core-2.33.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m398.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ray-2.43.0-cp311-cp311-manylinux2014_x86_64.whl (67.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 MB\u001b[0m \u001b[31m175.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.7/792.7 kB\u001b[0m \u001b[31m151.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
            "Downloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m208.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m199.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "Downloading aiohttp-3.11.16-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m202.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blake3-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (376 kB)\n",
            "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
            "Downloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
            "Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n",
            "Using cached ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "Downloading partial_json_parser-0.2.1.1.post5-py3-none-any.whl (10 kB)\n",
            "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
            "Downloading scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m251.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (454 kB)\n",
            "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
            "Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
            "Downloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
            "Downloading fastapi_cli-0.0.7-py3-none-any.whl (10 kB)\n",
            "Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (274 kB)\n",
            "Downloading hf_xet-1.0.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 MB\u001b[0m \u001b[31m183.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading interegular-0.3.3-py37-none-any.whl (23 kB)\n",
            "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langsmith-0.3.31-py3-none-any.whl (358 kB)\n",
            "Downloading llvmlite-0.44.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m228.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msgpack-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (403 kB)\n",
            "Downloading multidict-6.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
            "Downloading propcache-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (232 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
            "Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
            "Downloading uvicorn-0.34.1-py3-none-any.whl (62 kB)\n",
            "Downloading yarl-1.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (358 kB)\n",
            "Downloading airportsdata-20250224-py3-none-any.whl (913 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m913.7/913.7 kB\u001b[0m \u001b[31m178.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Downloading cupy_cuda12x-13.4.1-cp311-cp311-manylinux2014_x86_64.whl (105.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 MB\u001b[0m \u001b[31m363.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.9-py3-none-any.whl (119 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "Downloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m492.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "Downloading fastrlock-0.8.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl (54 kB)\n",
            "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
            "Downloading grpcio-1.71.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m517.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "Downloading orjson-3.10.16-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (132 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "Downloading rich_toolkit-0.14.1-py3-none-any.whl (24 kB)\n",
            "Downloading typer-0.15.2-py3-none-any.whl (45 kB)\n",
            "Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m360.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-15.0.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (182 kB)\n",
            "Downloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
            "Downloading zstandard-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m361.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
            "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Building wheels for collected packages: flash-attn, flashinfer-python\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for flash-attn: filename=flash_attn-2.7.4.post1-cp311-cp311-linux_x86_64.whl size=187805408 sha256=92cf49e6f66795b6934cec0cba526ed6e45d3313de3f905d45df8773f19092a9\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/88/d8/284b89f56af7d5bf366b10d6b8e251ac8a7c7bf3f04203fb4f\n",
            "  Building wheel for flashinfer-python (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for flashinfer-python: filename=flashinfer_python-0.2.5-py3-none-any.whl size=4124487 sha256=20693644e08ea1e239c6a19962116d5e4604b109ec20a589f388fb28446768a9\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/8c/d5/d58e45afac78c48fbc57aac3e97ef653228738b35ca7947a62\n",
            "Successfully built flash-attn flashinfer-python\n",
            "Installing collected packages: triton, sentencepiece, py-cpuinfo, nvidia-cusparselt-cu12, fastrlock, blake3, zstandard, wrapt, websockets, uvloop, typing_extensions, tqdm, tenacity, sympy, shellingham, scipy, safetensors, regex, python-multipart, python-dotenv, pycountry, protobuf, propcache, pillow, partial-json-parser, orjson, opentelemetry-semantic-conventions-ai, opencv-python-headless, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ninja, multidict, msgspec, msgpack, mdurl, llvmlite, llguidance, lark, jsonpatch, jiter, jinja2, interegular, importlib_metadata, httptools, hf-xet, grpcio, frozenlist, filelock, einops, dnspython, diskcache, dill, cupy-cuda12x, cloudpickle, click, cachetools, astor, annotated-types, airportsdata, aiohappyeyeballs, yarl, watchfiles, uvicorn, typing-inspection, tiktoken, starlette, requests-toolbelt, pydantic-core, opentelemetry-proto, nvidia-cusparse-cu12, numba, markdown-it-py, huggingface-hub, googleapis-common-protos, gguf, email-validator, depyf, deprecated, aiosignal, tokenizers, rich, pydantic, prometheus-fastapi-instrumentator, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, nvidia-cusolver-cu12, aiohttp, typer, transformers, torch, rich-toolkit, ray, outlines_core, opentelemetry-semantic-conventions, openai, mistral_common, lm-format-enforcer, langsmith, fastapi, xgrammar, xformers, torchvision, torchaudio, outlines, opentelemetry-sdk, langchain-core, flashinfer-python, flash-attn, fastapi-cli, compressed-tensors, bitsandbytes, accelerate, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, langchain_openai, opentelemetry-exporter-otlp, vllm\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.0.0\n",
            "    Uninstalling triton-3.0.0:\n",
            "      Successfully uninstalled triton-3.0.0\n",
            "  Attempting uninstall: typing_extensions\n",
            "    Found existing installation: typing_extensions 4.9.0\n",
            "    Uninstalling typing_extensions-4.9.0:\n",
            "      Successfully uninstalled typing_extensions-4.9.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.12\n",
            "    Uninstalling sympy-1.12:\n",
            "      Successfully uninstalled sympy-1.12\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 10.2.0\n",
            "    Uninstalling pillow-10.2.0:\n",
            "      Successfully uninstalled pillow-10.2.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.99\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.99:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.99\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.4.99\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.4.99:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.4.99\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.20.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.20.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.20.5\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.5.119\n",
            "    Uninstalling nvidia-curand-cu12-10.3.5.119:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.5.119\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.0.44\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.0.44:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.0.44\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.4.99\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.4.99:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.99\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.99\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.99:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.99\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.4.99\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.4.99:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.99\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.4.2.65\n",
            "    Uninstalling nvidia-cublas-cu12-12.4.2.65:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.4.2.65\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.3\n",
            "    Uninstalling Jinja2-3.1.3:\n",
            "      Successfully uninstalled Jinja2-3.1.3\n",
            "  Attempting uninstall: importlib_metadata\n",
            "    Found existing installation: importlib-metadata 4.6.4\n",
            "    Uninstalling importlib-metadata-4.6.4:\n",
            "      Successfully uninstalled importlib-metadata-4.6.4\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.13.1\n",
            "    Uninstalling filelock-3.13.1:\n",
            "      Successfully uninstalled filelock-3.13.1\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.3.0.142\n",
            "    Uninstalling nvidia-cusparse-cu12-12.3.0.142:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.3.0.142\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.0.99\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.0.99:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.0.99\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.4.1+cu124\n",
            "    Uninstalling torch-2.4.1+cu124:\n",
            "      Successfully uninstalled torch-2.4.1+cu124\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.19.1+cu124\n",
            "    Uninstalling torchvision-0.19.1+cu124:\n",
            "      Successfully uninstalled torchvision-0.19.1+cu124\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.4.1+cu124\n",
            "    Uninstalling torchaudio-2.4.1+cu124:\n",
            "      Successfully uninstalled torchaudio-2.4.1+cu124\n",
            "Successfully installed accelerate-1.6.0 aiohappyeyeballs-2.6.1 aiohttp-3.11.16 aiosignal-1.3.2 airportsdata-20250224 annotated-types-0.7.0 astor-0.8.1 bitsandbytes-0.45.5 blake3-1.0.4 cachetools-5.5.2 click-8.1.8 cloudpickle-3.1.1 compressed-tensors-0.9.3 cupy-cuda12x-13.4.1 deprecated-1.2.18 depyf-0.18.0 dill-0.3.9 diskcache-5.6.3 dnspython-2.7.0 einops-0.8.1 email-validator-2.2.0 fastapi-0.115.12 fastapi-cli-0.0.7 fastrlock-0.8.3 filelock-3.18.0 flash-attn-2.7.4.post1 flashinfer-python-0.2.5 frozenlist-1.5.0 gguf-0.14.0 googleapis-common-protos-1.70.0 grpcio-1.71.0 hf-xet-1.0.3 httptools-0.6.4 huggingface-hub-0.30.2 importlib_metadata-8.0.0 interegular-0.3.3 jinja2-3.1.6 jiter-0.9.0 jsonpatch-1.33 langchain-core-0.3.51 langchain_openai-0.3.12 langsmith-0.3.31 lark-1.2.2 llguidance-0.7.14 llvmlite-0.44.0 lm-format-enforcer-0.10.11 markdown-it-py-3.0.0 mdurl-0.1.2 mistral_common-1.5.4 msgpack-1.1.0 msgspec-0.19.0 multidict-6.4.3 ninja-1.11.1.4 numba-0.61.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 openai-1.74.0 opencv-python-headless-4.11.0.86 opentelemetry-api-1.26.0 opentelemetry-exporter-otlp-1.26.0 opentelemetry-exporter-otlp-proto-common-1.26.0 opentelemetry-exporter-otlp-proto-grpc-1.26.0 opentelemetry-exporter-otlp-proto-http-1.26.0 opentelemetry-proto-1.26.0 opentelemetry-sdk-1.26.0 opentelemetry-semantic-conventions-0.47b0 opentelemetry-semantic-conventions-ai-0.4.3 orjson-3.10.16 outlines-0.1.11 outlines_core-0.1.26 partial-json-parser-0.2.1.1.post5 pillow-11.2.1 prometheus-fastapi-instrumentator-7.1.0 propcache-0.3.1 protobuf-4.25.6 py-cpuinfo-9.0.0 pycountry-24.6.1 pydantic-2.11.3 pydantic-core-2.33.1 python-dotenv-1.1.0 python-multipart-0.0.20 ray-2.43.0 regex-2024.11.6 requests-toolbelt-1.0.0 rich-14.0.0 rich-toolkit-0.14.1 safetensors-0.5.3 scipy-1.15.2 sentencepiece-0.2.0 shellingham-1.5.4 starlette-0.46.2 sympy-1.13.1 tenacity-9.1.2 tiktoken-0.9.0 tokenizers-0.21.1 torch-2.6.0 torchaudio-2.6.0 torchvision-0.21.0 tqdm-4.67.1 transformers-4.51.3 triton-3.2.0 typer-0.15.2 typing-inspection-0.4.0 typing_extensions-4.13.2 uvicorn-0.34.1 uvloop-0.21.0 vllm-0.8.4 watchfiles-1.0.5 websockets-15.0.1 wrapt-1.17.2 xformers-0.0.29.post2 xgrammar-0.1.18 yarl-1.19.0 zstandard-0.23.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers bitsandbytes openai langchain_openai -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1JC3bK9DaqL",
        "outputId": "d3e122a4-4b57-4818-d530-0fd63a205973"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain_experimental langgraph langchain langchain_community langchain_huggingface dotenv -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvXDl6a-ChSU",
        "outputId": "fd5b1af6-fa00-441f-803c-7fa6c5b88e85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: flashinfer-python in /usr/local/lib/python3.11/dist-packages (0.2.5)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.11/dist-packages (24.0.1)\n",
            "Collecting pyzmq\n",
            "  Downloading pyzmq-26.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: vllm in /usr/local/lib/python3.11/dist-packages (0.8.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from flashinfer-python) (1.26.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flashinfer-python) (2.6.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from flashinfer-python) (1.11.1.4)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from vllm) (5.5.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from vllm) (6.0.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from vllm) (0.2.0)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from vllm) (4.67.1)\n",
            "Requirement already satisfied: blake3 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.0.4)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from vllm) (9.0.0)\n",
            "Requirement already satisfied: transformers>=4.51.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.51.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub[hf_xet]>=0.30.0->vllm) (0.30.2)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.21.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from vllm) (4.25.6)\n",
            "Requirement already satisfied: fastapi>=0.115.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.115.12)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from vllm) (3.11.16)\n",
            "Requirement already satisfied: openai>=1.52.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.74.0)\n",
            "Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.11.3)\n",
            "Requirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.21.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from vllm) (11.2.1)\n",
            "Requirement already satisfied: prometheus-fastapi-instrumentator>=7.0.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (7.1.0)\n",
            "Requirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.9.0)\n",
            "Requirement already satisfied: lm-format-enforcer<0.11,>=0.10.11 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.10.11)\n",
            "Requirement already satisfied: llguidance<0.8.0,>=0.7.9 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.7.14)\n",
            "Requirement already satisfied: outlines==0.1.11 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.1.11)\n",
            "Requirement already satisfied: lark==1.2.2 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.2.2)\n",
            "Requirement already satisfied: xgrammar==0.1.18 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.1.18)\n",
            "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.13.2)\n",
            "Requirement already satisfied: filelock>=3.16.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (3.18.0)\n",
            "Requirement already satisfied: partial-json-parser in /usr/local/lib/python3.11/dist-packages (from vllm) (0.2.1.1.post5)\n",
            "Requirement already satisfied: msgspec in /usr/local/lib/python3.11/dist-packages (from vllm) (0.19.0)\n",
            "Requirement already satisfied: gguf>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.14.0)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/dist-packages (from vllm) (8.0.0)\n",
            "Requirement already satisfied: mistral_common>=1.5.4 in /usr/local/lib/python3.11/dist-packages (from mistral_common[opencv]>=1.5.4->vllm) (1.5.4)\n",
            "Requirement already satisfied: opencv-python-headless>=4.11.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.11.0.86)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from vllm) (6.0.2)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from vllm) (0.8.1)\n",
            "Requirement already satisfied: compressed-tensors==0.9.3 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.9.3)\n",
            "Requirement already satisfied: depyf==0.18.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.18.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from vllm) (3.1.1)\n",
            "Requirement already satisfied: watchfiles in /usr/local/lib/python3.11/dist-packages (from vllm) (1.0.5)\n",
            "Requirement already satisfied: python-json-logger in /usr/local/lib/python3.11/dist-packages (from vllm) (2.0.7)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from vllm) (1.15.2)\n",
            "Requirement already satisfied: opentelemetry-sdk<1.27.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.26.0)\n",
            "Requirement already satisfied: opentelemetry-api<1.27.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.26.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp<1.27.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.26.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions-ai<0.5.0,>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.4.3)\n",
            "Requirement already satisfied: numba==0.61.2 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.61.2)\n",
            "Requirement already satisfied: ray!=2.44.*,>=2.43.0 in /usr/local/lib/python3.11/dist-packages (from ray[cgraph]!=2.44.*,>=2.43.0->vllm) (2.43.0)\n",
            "Requirement already satisfied: torchaudio==2.6.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.6.0)\n",
            "Requirement already satisfied: torchvision==0.21.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.21.0)\n",
            "Requirement already satisfied: xformers==0.0.29.post2 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.0.29.post2)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.11/dist-packages (from depyf==0.18.0->vllm) (0.8.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from depyf==0.18.0->vllm) (0.3.9)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba==0.61.2->vllm) (0.44.0)\n",
            "Requirement already satisfied: interegular in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (0.3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (3.1.6)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (1.6.0)\n",
            "Requirement already satisfied: diskcache in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (5.6.3)\n",
            "Requirement already satisfied: referencing in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (0.35.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (4.23.0)\n",
            "Requirement already satisfied: pycountry in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (24.6.1)\n",
            "Requirement already satisfied: airportsdata in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (20250224)\n",
            "Requirement already satisfied: outlines_core==0.1.26 in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (0.1.26)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python) (3.2.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python) (2024.2.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->flashinfer-python) (1.3.0)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (0.46.2)\n",
            "Requirement already satisfied: fastapi-cli>=0.0.5 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.0.7)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.27.2)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.0.20)\n",
            "Requirement already satisfied: email-validator>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (2.2.0)\n",
            "Requirement already satisfied: uvicorn>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.34.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.0->huggingface-hub[hf_xet]>=0.30.0->vllm) (24.1)\n",
            "Requirement already satisfied: hf-xet>=0.1.4 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub[hf_xet]>=0.30.0->vllm) (1.0.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (4.6.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.52.0->vllm) (1.7.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (1.3.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api<1.27.0,>=1.26.0->vllm) (1.2.18)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/lib/python3/dist-packages (from importlib_metadata->vllm) (1.0.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc==1.26.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm) (1.26.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http==1.26.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm) (1.26.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm) (1.71.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.26.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm) (1.26.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.26.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm) (1.26.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.47b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk<1.27.0,>=1.26.0->vllm) (0.47b0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm) (0.4.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (8.1.8)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (1.1.0)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.11/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (1.3.2)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.11/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (1.5.0)\n",
            "Requirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.11/dist-packages (from ray[cgraph]!=2.44.*,>=2.43.0->vllm) (13.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (2024.8.30)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.6.0->vllm) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.51.1->vllm) (0.5.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (2.6.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (24.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (1.19.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api<1.27.0,>=1.26.0->vllm) (1.17.2)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm) (2.7.0)\n",
            "Requirement already satisfied: typer>=0.12.3 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.15.2)\n",
            "Requirement already satisfied: rich-toolkit>=0.11.1 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.14.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (0.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->outlines==0.1.11->vllm) (2.1.5)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm) (2023.12.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm) (0.20.0)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.1.0)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.21.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (15.0.1)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.11/dist-packages (from cupy-cuda12x->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (0.8.3)\n",
            "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.11/dist-packages (from rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (14.0.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.5.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.1.2)\n",
            "Downloading pyzmq-26.4.0-cp311-cp311-manylinux_2_28_x86_64.whl (862 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m862.4/862.4 kB\u001b[0m \u001b[31m193.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyzmq\n",
            "  Attempting uninstall: pyzmq\n",
            "    Found existing installation: pyzmq 24.0.1\n",
            "    Uninstalling pyzmq-24.0.1:\n",
            "      Successfully uninstalled pyzmq-24.0.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "notebook 6.5.5 requires pyzmq<25,>=17, but you have pyzmq 26.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pyzmq-26.4.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Pytorch 버전 호환\n",
        "# vllm 0.10 : Pytorch 2.7.1\n",
        "# vllm 0.9.x == 2.7.0\n",
        "# vllm 0.8.x == 2.6.0\n",
        "# vllm 0.6.x == 2.5.1\n",
        "\n",
        "!pip install pyzmq flashinfer-python==0.2.10 vllm==0.10.1 -q\n",
        "!pip install  flashinfer-python pyzmq vllm -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-K10yruCvnu"
      },
      "source": [
        "설치 후에는 세션을 재시작해 주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-Q0yOegChSU",
        "outputId": "3dbaf4e0-c1ca-4268-aaf9-6f39fd3614f5",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Flash Attention: 리눅스 전용 설치방법\n",
        "# Windows 설치는 https://github.com/kingbri1/flash-attention/releases 참고\n",
        "!pip install flash-attn --no-build-isolation -q\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwLiGVjiChSV"
      },
      "source": [
        "설치할 라이브러리가 많으므로, 가급적 설치 후 세션 재시작을 수행해 주세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfDjPosVChSV"
      },
      "source": [
        "vLLM은 캐싱을 통해 효과적인 추론과 동시 실행을 지원합니다.   \n",
        "아래 코드를 터미널에서 실행하세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIRl33XXChSV"
      },
      "source": [
        "```\n",
        "vllm serve unsloth/gemma-3-12b-it-bnb-4bit --dtype auto --max_model_len 32768 --quantization bitsandbytes --served_model_name gemma3 --max_num_seqs 1\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uiptF1yChSW"
      },
      "source": [
        "어느 정도 시간이 지나면, vLLM 서빙이 완료됩니다.   \n",
        "vllm Serve가 정상적으로 완료되면, 8000번 포트에서 모델을 확인하실 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7RLw4ynDEqa"
      },
      "outputs": [],
      "source": [
        "# 모델 주소 확인\n",
        "!curl http://0.0.0.0:8000/v1/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huewOgXWChSW"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI, OpenAI\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    base_url=\"http://localhost:8000/v1\",\n",
        "    api_key=\"token-abc123\",\n",
        "    model=\"gemma3\",\n",
        "    temperature=0.5,\n",
        "    max_tokens=1024\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKU1bgY9ChSW",
        "outputId": "aba1396f-2e7f-406b-bcbb-8d67d590a0e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vLLM은 **고성능 LLM(Large Language Model) 추론을 위한 오픈소스 엔진**입니다. 쉽게 말해, LLM을 더 빠르고 효율적으로 실행할 수 있도록 도와주는 도구라고 생각하시면 됩니다.\n",
            "\n",
            "**vLLM의 주요 특징 및 장점:**\n",
            "\n",
            "*   **PagedAttention:** vLLM의 핵심 기술입니다. LLM 추론 시 메모리 사용량을 크게 줄여줍니다. LLM은 텍스트를 처리하면서 중간 결과를 메모리에 저장해야 하는데, PagedAttention은 이 과정을 효율적으로 관리하여 더 많은 요청을 동시에 처리할 수 있도록 합니다. 마치 컴퓨터의 가상 메모리처럼, 필요한 만큼만 메모리를 할당하고 관리하는 방식입니다.\n",
            "*   **빠른 추론 속도:** PagedAttention 덕분에 기존 LLM 추론 엔진보다 훨씬 빠른 속도로 LLM을 실행할 수 있습니다.\n",
            "*   **높은 처리량:** 더 많은 요청을 동시에 처리할 수 있어, LLM 서비스를 운영할 때 비용 효율성을 높일 수 있습니다.\n",
            "*   **다양한 LLM 지원:** Llama 2, Mistral, Falcon 등 다양한 LLM을 지원합니다.\n",
            "*   **쉬운 사용법:** API를 통해 쉽게 통합할 수 있으며, Hugging Face Transformers와 호환됩니다.\n",
            "\n",
            "**vLLM이 왜 중요할까요?**\n",
            "\n",
            "LLM은 많은 연산 자원을 필요로 하며, 특히 추론(inference) 과정에서 많은 메모리를 사용합니다. vLLM은 이러한 문제를 해결하여 LLM을 더 많은 사람들이 더 쉽게 사용할 수 있도록 합니다. LLM 서비스를 운영하는 기업에게는 비용 절감과 성능 향상이라는 이점을 제공합니다.\n",
            "\n",
            "**vLLM에 대해 더 자세히 알아보고 싶다면:**\n",
            "\n",
            "*   **vLLM 공식 웹사이트:** [https://vllm.ai/](https://vllm.ai/)\n",
            "*   **GitHub 저장소:** [https://github.com/vllm-project/vllm](https://github.com/vllm-project/vllm)\n",
            "\n",
            "궁금한 점이 있다면 언제든지 다시 질문해주세요."
          ]
        }
      ],
      "source": [
        "for s in llm.stream(\"vLLM이 뭐야?\"):\n",
        "    print(s.content, end='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXqmbS-lChSW",
        "outputId": "d6187fef-523f-445b-da64-ae2452328047"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<bos><start_of_turn>user\\n안녕<end_of_turn>\\n<start_of_turn>model\\n'"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def convert_chat(messages, add_generation_prompt = True):\n",
        "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=add_generation_prompt)\n",
        "\n",
        "example = [{'role':'user', 'content':'안녕'}]\n",
        "convert_chat(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_P4V-vgChSW"
      },
      "source": [
        "## 2. HuggingFace LLM과 툴 연동하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwiBEnDuChSW"
      },
      "source": [
        "먼저 툴을 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIOEJGlIChSW",
        "outputId": "68a464c6-3f6c-4bd4-a350-ca836db149c0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Python REPL can execute arbitrary code. Use with caution.\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_core.tools import tool\n",
        "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
        "from datetime import datetime\n",
        "\n",
        "tavily_search = TavilySearchResults(\n",
        "    max_results=3)\n",
        "\n",
        "repl_tool = PythonREPLTool()\n",
        "repl_tool.invoke(\"for i in range(10): print(i)\")\n",
        "\n",
        "\n",
        "@tool\n",
        "def current_date() -> str:\n",
        "    \"현재 날짜를 %y-%m-%d 형식으로 반환합니다.\"\n",
        "    return datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "\n",
        "\n",
        "tools = [tavily_search, repl_tool, current_date]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY-T4e9hChSW"
      },
      "source": [
        "vLLM에서는 Tool Binding을 수행하려면   \n",
        "--enable-auto-tool-choice 를 추가한 뒤, 전용 파서를 연결해야 합니다.   \n",
        "Llama, Qwen, Hermes 등의 모델이 가능합니다.   \n",
        "https://docs.vllm.ai/en/stable/features/tool_calling.htm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcZfOJvTChSX",
        "outputId": "fef8a96b-f6d8-4ac2-db98-3fe102ab580a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'type': 'function',\n",
              " 'function': {'name': 'current_date',\n",
              "  'description': '현재 날짜를 %y-%m-%d 형식으로 반환합니다.',\n",
              "  'parameters': {'properties': {}, 'type': 'object'}}}"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
        "\n",
        "\n",
        "convert_to_openai_tool(tools[2])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Av22_b_eChSX"
      },
      "source": [
        "툴 설명이 담긴 문자열을 구성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFAAolWtChSX",
        "outputId": "794c42db-e9af-4108-90f8-1325ea4db5f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'type': 'function', 'function': {'name': 'tavily_search_results_json', 'description': 'A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}\n",
            "---\n",
            "{'type': 'function', 'function': {'name': 'Python_REPL', 'description': 'A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.', 'parameters': {'properties': {'query': {'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}\n",
            "---\n",
            "{'type': 'function', 'function': {'name': 'current_date', 'description': '현재 날짜를 %y-%m-%d 형식으로 반환합니다.', 'parameters': {'properties': {}, 'type': 'object'}}}\n"
          ]
        }
      ],
      "source": [
        "tool_desc = str('\\n---\\n'.join([str(convert_to_openai_tool(tool)) for tool in tools]))\n",
        "print(tool_desc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxPl5cfWChSX"
      },
      "source": [
        "시스템 프롬프트를 구성합니다.   \n",
        "성능에 매우 중요한 영향을 미치므로, 영어로 작성했습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLF16NzcChSX"
      },
      "outputs": [],
      "source": [
        "system_prompt = f'''\n",
        "You are a helpful assistant with tools below.\n",
        "You can decide whether to invoke any functions or not.\n",
        "If you decide to use any of tools.\n",
        "print name and required parameters of the tool within a json blob correctly.\n",
        "For python code, Return the object as a raw dictionary, without escaping quotes or newlines.\n",
        "\n",
        "for tool use: wrap your output within ```tool_code```.\n",
        "\n",
        "Example:\n",
        "```tool_code\n",
        "{{\"name\":'name of tool', \"arguments\":{{List of apparent argument and parameters}}}}\n",
        "```\n",
        "\n",
        "When the output of the tool is provided, it will be wrapped within ``tool_output```\n",
        "Answer accordingly from the result of the tool output.\n",
        "\n",
        "The question might need some sequential, multiple tool execution.\n",
        "Think Step by Step.\n",
        "\n",
        "The following tools are available:\n",
        "{tool_desc}'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGNheWMdChSX"
      },
      "source": [
        "시스템 프롬프트에 들어가야 하는 내용은 다음과 같습니다.\n",
        "- Tool Format\n",
        "- Tool Call Format\n",
        "- Tool Result Format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6hbNYp0ChSY"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, ToolMessage\n",
        "\n",
        "\n",
        "messages = [SystemMessage(system_prompt),\n",
        "            HumanMessage('오늘 날짜가 며칠이니?')]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcxyBL1bChSY",
        "outputId": "53dd0fab-5a69-4966-aa7b-819ada117091"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='```tool_code\\n{\"name\": \"current_date\", \"arguments\": {}}\\n```', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 436, 'total_tokens': 457, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gemma3', 'system_fingerprint': None, 'id': 'chatcmpl-a25d13c9f0ea437cb82dd75e1b46e70c', 'finish_reason': 'stop', 'logprobs': None}, id='run-42102c77-bc44-4e88-9c7e-6f894df8067c-0', usage_metadata={'input_tokens': 436, 'output_tokens': 21, 'total_tokens': 457, 'input_token_details': {}, 'output_token_details': {}})"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = llm.invoke(messages)\n",
        "messages.append(response)\n",
        "response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHphfjKGChSY",
        "outputId": "ae9cf9c4-35bd-4ae4-de1c-7b33608b0a70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```tool_code\n",
            "{\"name\": \"current_date\", \"arguments\": {}}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hf3ikmr5ChSY"
      },
      "source": [
        "tool_code를 받았으니, 해당 내용을 파싱합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Mg_NaBOChSY",
        "outputId": "e09d54bf-c3ca-450a-c116-1aee57844edf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('current_date', {})"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import ast\n",
        "\n",
        "\n",
        "def parse_tool(text):\n",
        "    try:\n",
        "        text = text.split('```tool_code\\n')[1].split('\\n```')[0]\n",
        "        # tool_code로 wrap된 중간 코드 추출\n",
        "\n",
        "        parsed = ast.literal_eval(text)\n",
        "        # Dict 형태의 값 변환 (json load와 유사)\n",
        "\n",
        "        name = parsed.get('name')\n",
        "        arguments = parsed.get('arguments', {})\n",
        "        # name과 argument return\n",
        "        return {'name':name, 'arguments':arguments}\n",
        "    except (ValueError, SyntaxError):\n",
        "        return None\n",
        "\n",
        "result = parse_tool(response.content)\n",
        "name,arguments = result['name'], result['arguments']\n",
        "name, arguments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaTQK_seChSZ"
      },
      "source": [
        "툴 실행을 연결합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plDwh8fNChSZ",
        "outputId": "7bd96f8b-a81a-41c7-ed88-9858f1a7214e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```tool_output\n",
            "2025-04-15\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "# 툴 이름과 툴 연결\n",
        "tool_dict = {tool.name: tool for tool in tools}\n",
        "\n",
        "def execute_tool(name, arguments):\n",
        "    # 툴 실행한 뒤 tool_output으로 wrap\n",
        "    result = f'''```tool_output\n",
        "{tool_dict[name].invoke(arguments)}\n",
        "```'''\n",
        "    return result\n",
        "\n",
        "tool_result = execute_tool(**parse_tool(response.content))\n",
        "\n",
        "print(tool_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vALn3uIChSZ",
        "outputId": "33d48f10-c301-4709-9a28-5da925fbcc59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='오늘 날짜가 며칠이니?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='```tool_code\\n{\"name\": \"current_date\", \"arguments\": {}}\\n```', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 436, 'total_tokens': 457, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gemma3', 'system_fingerprint': None, 'id': 'chatcmpl-a25d13c9f0ea437cb82dd75e1b46e70c', 'finish_reason': 'stop', 'logprobs': None}, id='run-42102c77-bc44-4e88-9c7e-6f894df8067c-0', usage_metadata={'input_tokens': 436, 'output_tokens': 21, 'total_tokens': 457, 'input_token_details': {}, 'output_token_details': {}}),\n",
              " HumanMessage(content='```tool_output\\n2025-04-15\\n```', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "messages.append(HumanMessage(tool_result))\n",
        "messages[1:]\n",
        "# 질문 + Tool 요청 + Tool 결과"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-00rE8xChSc",
        "outputId": "63010bcb-b25f-4c1a-8bbf-d783ec3af7b4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='오늘 날짜는 2025년 4월 15일입니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 483, 'total_tokens': 503, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gemma3', 'system_fingerprint': None, 'id': 'chatcmpl-2ea92a48c466474d95f92496514bbb88', 'finish_reason': 'stop', 'logprobs': None}, id='run-f2417c07-718e-4a31-a8d2-aec6ce066b08-0', usage_metadata={'input_tokens': 483, 'output_tokens': 20, 'total_tokens': 503, 'input_token_details': {}, 'output_token_details': {}})"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 결과 해석\n",
        "response = llm.invoke(messages)\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuoZLPE-ChSc",
        "outputId": "f74077dd-1109-4c04-941e-15c3afe3345e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='2025년 4월에 발표된 GPT-4.1 모델에 대한 정보를 찾기 위해 검색을 수행하겠습니다.\\n\\n```tool_code\\n{\"name\": \"tavily_search_results_json\", \"arguments\": {\"query\": \"2025년 4월 GPT-4.1 모델\"}}\\n```', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 73, 'prompt_tokens': 454, 'total_tokens': 527, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gemma3', 'system_fingerprint': None, 'id': 'chatcmpl-1a93a67679514213bf8ea2dc6f860ac7', 'finish_reason': 'stop', 'logprobs': None}, id='run-3b356dfd-b74c-4f7a-ae65-61cf75a60530-0', usage_metadata={'input_tokens': 454, 'output_tokens': 73, 'total_tokens': 527, 'input_token_details': {}, 'output_token_details': {}})"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "messages = [SystemMessage(system_prompt),\n",
        "            HumanMessage('2025년 4월 발표된 GPT-4.1 모델이 어떤 모델이야? 한국어로 설명해줘.')]\n",
        "response = llm.invoke(messages)\n",
        "messages.append(response)\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4UwJtbSChSc",
        "outputId": "050fa880-3c6a-4132-da28-d012773a7389"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025년 4월에 발표된 GPT-4.1 모델에 대한 정보를 찾기 위해 검색을 수행하겠습니다.\n",
            "\n",
            "```tool_code\n",
            "{\"name\": \"tavily_search_results_json\", \"arguments\": {\"query\": \"2025년 4월 GPT-4.1 모델\"}}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVIJ0E3gChSc",
        "outputId": "c96088d6-c498-4269-fe59-d71584106b1b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'```tool_output\\n[{\\'title\\': \"오픈AI의 새로운 진화, \\'GPT-4.1\\'과 온디바이스 시대의 서막\", \\'url\\': \\'https://m.blog.naver.com/uripeoyn_/223830042974?recommendCode=2&recommendTrackingCode=2\\', \\'content\\': \\'블로그\\\\n\\\\n카테고리 이동\\\\n\\\\n\\\\n\\\\n\\\\n1200만 시니어 AI 교육1인자 가장 쉬운 인공지능 강의\\\\n\\\\n오픈AI의 새로운 진화, ‘GPT-4.1’과 온디바이스 시대의 서막 – 경량화 ‘o4-미니’, ‘나노’, 그리고 ‘o3’ 모델까지 동시 출격 예고\\\\n\\\\n2025. 4. 11. 19:36\\\\n\\\\n2025년 4월, AI 기술의 진화는 다시 한 번 중요한 분기점을 맞이하고 있습니다. 바로 오픈AI(OpenAI)가 GPT-4o의 상위 버전으로 추정되는 **‘GPT-4.1’**과 ‘o4-미니(Mini)’, ‘나노(Nano)’, 그리고 ‘o3’ 모델까지 다음 주 공개를 예고하면서입니다.\\\\n\\\\n\\\\u200b\\\\n\\\\n\\\\u200b\\\\n\\\\n이 새로운 라인업은 단순한 업그레이드가 아닌, AI 모델의 경량화와 온디바이스(기기 내 탑재) 실행을 가능하게 하는 본격적인 이정표가 될 전망입니다.\\\\n\\\\n\\\\u200b\\\\n\\\\n1. GPT-4.1 – 더 강력한 멀티모달, 더 똑똑한 AVM 엔진\\\\n\\\\n\\\\u200b [...] GPT-4o는 오픈AI가 2024년 처음 선보였던 플래그십 멀티모달 모델로, 텍스트, 이미지, 오디오를 통합 처리하며 실시간 상호작용이 가능한 고성능 AI였습니다. 특히 ChatGPT의 **‘고급 음성 모드(Advanced Voice Mode, AVM)’**의 핵심 엔진으로 주목받았습니다.\\\\n\\\\n\\\\u200b\\\\n\\\\n이번에 예고된 GPT-4.1은 그 GPT-4o의 성능 향상 버전으로, 더 빠른 응답, 개선된 논리적 추론, 안정적인 멀티모달 연산을 목표로 설계된 것으로 알려졌습니다.\\\\n\\\\n이로써 ChatGPT 사용자는 더 현실적인 음성 대화, 더 정확한 시각 정보 해석, 더 자연스러운 멀티턴 대화를 경험할 수 있게 될 것입니다.\\\\n\\\\n\\\\u200b\\\\n\\\\n2. GPT-4.1 Mini & Nano – ‘온디바이스 AI’ 시대 개막\\\\n\\\\n\\\\u200b\\\\n\\\\n이번 출시에서 가장 주목할 점은 ‘GPT-4.1 미니(Mini)’와 ‘나노(Nano)’ 모델입니다. [...] 이는 오픈AI 최초의 **경량형 LLM(Large Language Model)**이며, 로컬 디바이스에서도 실행 가능하다는 점에서 스마트폰·웨어러블·로봇 등 엣지 컴퓨팅 환경의 AI 통합을 가속화할 핵심 요소입니다.\\\\n\\\\n\\\\u200b\\\\n\\\\nGPT-4.1 Mini: 클라우드 중심 모델 대비 10배 이상 가벼운 구조. 고속 추론 가능.\\\\n\\\\n\\\\u200b\\\\n\\\\nNano: IoT, 스마트워치, 차량용 시스템 등 초소형 환경에 최적화된 AI 모델.\\\\n\\\\n\\\\u200b\\\\n\\\\n이러한 온디바이스 AI는 개인정보 보호, 실시간 반응성, 오프라인 환경에서도 사용 가능하다는 점에서 AI 서비스의 보편화에 획기적인 변화를 예고합니다.\\\\n\\\\n\\\\u200b\\\\n\\\\n\\\\u200b\\\\n\\\\n3. GPT-o3 – 고도화된 추론 엔진\\\\n\\\\n\\\\u200b\\\\n\\\\n또 다른 주목할 신형 모델은 GPT-o3. AI 개발자 및 분석가들 사이에서 ‘논리적 추론(Logical Reasoning)’ 특화 모델로 평가받고 있으며, 오픈AI가 기존 GPT 시리즈와는 차별화된 방향으로 실험 중이던 모델입니다.\\\\n\\\\n\\\\u200b\\', \\'score\\': 0.8708723}, {\\'title\\': \\'GPT-4 - 나무위키\\', \\'url\\': \\'https://namu.wiki/w/GPT-4\\', \\'content\\': \\'# 이후 2월 말 출시했다. 2025년 4월 15일, GPT-4.1 모델을 API 전용으로 출시했다. # 또한 GPT-4는 4월 30일부로\\', \\'score\\': 0.8684817}, {\\'title\\': \\'Introducing GPT-4.1 in the API - OpenAI\\', \\'url\\': \\'https://openai.com/index/gpt-4-1/\\', \\'content\\': \\'April 14, 2025\\\\nProduct\\\\nIntroducing GPT-4.1 in the API\\\\nA new series of GPT models featuring major improvements on coding, instruction following, and long context—plus our first-ever nano model.\\\\nTry in Playground\\\\nListen to article\\\\nShare [...] We will also begin deprecating GPT‑4.5 Preview in the API, as GPT‑4.1 offers improved or similar performance on many key capabilities at much lower cost and latency. GPT‑4.5 Preview will be turned off in three months, on July 14, 2025, to allow time for developers to transition. GPT‑4.5 was introduced as a research preview to explore and experiment with a large, compute-intensive model, and we’ve learned a lot from developer feedback. We’ll continue to carry forward the creativity, writing [...] Today, we’re launching three new models in the API: GPT‑4.1, GPT‑4.1 mini, and GPT‑4.1 nano. These models outperform GPT‑4o and GPT‑4o mini across the board, with major gains in coding and instruction following. They also have larger context windows—supporting up to 1 million tokens of context—and are able to better use that context with improved long-context comprehension. They feature a refreshed knowledge cutoff of June 2024.\\\\nGPT‑4.1 excels at the following industry standard measures:\\', \\'score\\': 0.79041743}]\\n```'"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tool_result = execute_tool(**parse_tool(response.content))\n",
        "tool_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbPDgbu7ChSd",
        "outputId": "04963625-442d-433e-f51e-1f717b666b2c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='2025년 4월에 발표된 GPT-4.1 모델에 대한 정보를 찾기 위해 검색을 수행하겠습니다.\\n\\n```tool_code\\n{\"name\": \"tavily_search_results_json\", \"arguments\": {\"query\": \"2025년 4월 GPT-4.1 모델\"}}\\n```', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 73, 'prompt_tokens': 454, 'total_tokens': 527, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gemma3', 'system_fingerprint': None, 'id': 'chatcmpl-1a93a67679514213bf8ea2dc6f860ac7', 'finish_reason': 'stop', 'logprobs': None}, id='run-3b356dfd-b74c-4f7a-ae65-61cf75a60530-0', usage_metadata={'input_tokens': 454, 'output_tokens': 73, 'total_tokens': 527, 'input_token_details': {}, 'output_token_details': {}})"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "messages[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G636d4DkChSd",
        "outputId": "420c583c-2335-4bfe-dcc1-81d7aca04675"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"GPT-4.1은 2025년 4월에 발표된 GPT-4o의 상위 버전으로 추정되는 모델입니다. 오픈AI는 GPT-4.1과 함께 'o4-미니', '나노', 'o3' 모델도 공개했습니다.\\n\\nGPT-4.1은 더 강력한 멀티모달 기능, 빠른 응답 속도, 개선된 논리적 추론 능력을 갖추고 있습니다. 특히, '나노' 모델은 오픈AI 최초의 경량형 LLM으로, 기기 내에서 실행이 가능하여 온디바이스 AI 시대를 예고합니다. 또한, GPT-4.1은 최대 100만 토큰의 문맥을 지원하며, 2024년 6월까지의 지식을 포함하고 있습니다. API를 통해 먼저 출시되었으며, GPT-4.5 Preview를 대체할 예정입니다.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 198, 'prompt_tokens': 2002, 'total_tokens': 2200, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gemma3', 'system_fingerprint': None, 'id': 'chatcmpl-5ce90a24f23f4cfb9e63e1abded5d423', 'finish_reason': 'stop', 'logprobs': None}, id='run-248c17b2-0db8-4e2e-aee9-14b8fc1da6b5-0', usage_metadata={'input_tokens': 2002, 'output_tokens': 198, 'total_tokens': 2200, 'input_token_details': {}, 'output_token_details': {}})"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "messages.append(HumanMessage(tool_result))\n",
        "response = llm.invoke(messages)\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zM4gli-1ChSd"
      },
      "source": [
        "일반적인 입출력 관계의 툴은 이와 같은 방식으로 간단하게 구성할 수 있습니다.   \n",
        "(만약, Python_REPL과 같이 argument가 복잡한 툴을 수행하는 경우에는   \n",
        "별도의 함수로 변환하거나 결과물을 수정하는 작업이 필요할 수 있습니다.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qx8blhUwChSd"
      },
      "source": [
        "해당 구현을 통해, ReAct Agent 구조를 만들어 보겠습니다.    \n",
        "bind_tools가 없기 때문에, 기존의 Tool Message를 사용하기 어렵습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJuCpEQ5ChSd"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages : Annotated[list, add_messages]   # 메시지 맥락을 저장하는 리스트\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9bMAts4ChSd"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import ToolMessage\n",
        "\n",
        "tool_list = {tool.name: tool for tool in tools}\n",
        "# tool 목록 dict로 생성\n",
        "\n",
        "def tool_node(state):\n",
        "    tool_outputs = []\n",
        "    tool_call_msgs = state['messages'][-1]\n",
        "    # 마지막 메시지: 툴 콜링 메시지\n",
        "    if '```tool_code' in tool_call_msgs.content:\n",
        "        tool_result = execute_tool(**parse_tool(tool_call_msgs.content))\n",
        "        # tool 실행 결과 얻기 (결과는 ```tool_output```)\n",
        "        tool_outputs.append(HumanMessage(tool_result))\n",
        "\n",
        "    return {'messages': tool_outputs}\n",
        "\n",
        "def agent(state):\n",
        "    system_prompt = SystemMessage(f'''\n",
        "You are a helpful assistant with tools below.\n",
        "You can decide whether to invoke any functions or not.\n",
        "If you decide to use any of tools.\n",
        "print name and required parameters of the tool within a json blob correctly.\n",
        "For python code, Return the object as a raw dictionary, without escaping quotes or newlines.\n",
        "\n",
        "for tool use: wrap your output within ```tool_code```.\n",
        "\n",
        "Example:\n",
        "```tool_code\n",
        "{{\"name\":'name of tool', \"arguments\":{{List of apparent argument and parameters}}}}\n",
        "```\n",
        "\n",
        "When the output of the tool is provided, it will be wrapped within ``tool_output```\n",
        "Answer accordingly from the result of the tool output.\n",
        "\n",
        "The question might need some sequential, multiple tool execution.\n",
        "Think Step by Step.\n",
        "\n",
        "The following tools are available:\n",
        "{tool_desc}\n",
        "\n",
        "\n",
        "Answer in Korean.''')\n",
        "\n",
        "\n",
        "    response = llm.invoke([system_prompt] + state[\"messages\"])\n",
        "    return {'messages': response}\n",
        "\n",
        "def tool_needed(state):\n",
        "\n",
        "    last_msg = state['messages'][-1]\n",
        "    if '```tool_code' in last_msg.content: # 툴 콜링이 필요하면\n",
        "        return \"continue\"\n",
        "    else:\n",
        "        return \"finish\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3OJlZQ4ChSd",
        "outputId": "f713f4bb-3594-45d7-f567-e022a68665cc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x791d2cd45b50>"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "builder = StateGraph(State)\n",
        "\n",
        "builder.add_node(\"agent\", agent)\n",
        "builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "builder.add_edge(START, 'agent'),\n",
        "builder.add_conditional_edges(\"agent\",\n",
        "                              tool_needed,\n",
        "                               {\"continue\": \"tools\",\"finish\": END})\n",
        "builder.add_edge(\"tools\", \"agent\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6snqe_OChSd",
        "outputId": "4765f078-2d0a-421b-99fa-205c093de54b"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<langgraph.graph.state.CompiledStateGraph object at 0x791d2c596990>"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph = builder.compile()\n",
        "graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUzr8D4cChSd",
        "outputId": "998f8f3e-8b5e-4d54-ef0e-756d9ac43b3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='오늘 날짜에 태어난 유명인들 조사해서 알려줘.', additional_kwargs={}, response_metadata={}, id='5e436e3d-4033-4a9d-b1c4-41810f76d617'),\n",
              "  AIMessage(content='오늘 날짜를 알아야 오늘 날짜에 태어난 유명인을 찾을 수 있습니다. 먼저 `current_date` 도구를 사용하여 오늘 날짜를 알아봅니다.\\n```tool_code\\n{\"name\": \"current_date\", \"arguments\": {}}\\n```', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 59, 'prompt_tokens': 447, 'total_tokens': 506, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gemma3', 'system_fingerprint': None, 'id': 'chatcmpl-add7d72a92b74226891bfea8cb997829', 'finish_reason': 'stop', 'logprobs': None}, id='run-21398e0f-c2a0-4c82-9bcf-ca58ec0dfc06-0', usage_metadata={'input_tokens': 447, 'output_tokens': 59, 'total_tokens': 506, 'input_token_details': {}, 'output_token_details': {}}),\n",
              "  HumanMessage(content='```tool_output\\n2025-04-15\\n```', additional_kwargs={}, response_metadata={}, id='dc9312c8-3082-414b-91e6-13b7b833d571'),\n",
              "  AIMessage(content='2025년 4월 15일에 태어난 유명인을 찾기 위해 `tavily_search_results_json` 도구를 사용하겠습니다.\\n\\n```tool_code\\n{\"name\": \"tavily_search_results_json\", \"arguments\": {\"query\": \"2025년 4월 15일 생 유명인\"}}\\n```', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 82, 'prompt_tokens': 532, 'total_tokens': 614, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gemma3', 'system_fingerprint': None, 'id': 'chatcmpl-9f6c8955106347078bd2e09c4587c53a', 'finish_reason': 'stop', 'logprobs': None}, id='run-25c31696-49fc-47c8-b34a-839075dfe85f-0', usage_metadata={'input_tokens': 532, 'output_tokens': 82, 'total_tokens': 614, 'input_token_details': {}, 'output_token_details': {}}),\n",
              "  HumanMessage(content=\"```tool_output\\n[{'title': '2025년 4월 15일, 오늘은 누군가의 생일이다 - YouTube', 'url': 'https://www.youtube.com/watch?v=3m6vHCKYUvQ', 'content': '... 유명인 생일 출처 : 나무위키 ~오늘 생일인 유명인 목록~ 클라우디아 카르디날레님(배우) 윤무부님(조류학자) 엠마 톰슨님(배우) 크레싣 ㅏ코웰님', 'score': 0.875031}, {'title': '[오늘의 운세] 2025년 4월 15일 띠별 운세 - 네이트 뉴스', 'url': 'https://news.nate.com/view/20250415n00015', 'content': '2025년 4월 15일 (음력 3월 18일) 정동근 (한국역술인협회 부회장) 쥐띠 36년 병자생 꽃을 가꾸어 보십시오. 48년 무자생 성장을 하겠습니다.', 'score': 0.7869018}, {'title': '4월 15일 생일 연예인 - superkts.com', 'url': 'https://superkts.com/people/birth/day/0415', 'content': '4월 15일 생일 연예인 ; 정애란, 배우, 1927.04.15(금) ; 이동원, 가수, 1951.04.15(일) ; 김태우, 배우, 1971.04.15(목) ; 김석훈, 배우, 1972.04.15(토)', 'score': 0.5402697}]\\n```\", additional_kwargs={}, response_metadata={}, id='c68c5411-9f4b-482d-9581-459c48756cb6'),\n",
              "  AIMessage(content='2025년 4월 15일에 태어난 유명인은 아직 없지만, 4월 15일에 태어난 다른 유명인들을 찾았습니다.\\n\\n*   **클라우디아 카르디날레:** 배우\\n*   **윤무부:** 조류학자\\n*   **엠마 톰슨:** 배우\\n*   **정애란:** 배우 (1927년)\\n*   **이동원:** 가수 (1951년)\\n*   **김태우:** 배우 (1971년)\\n*   **김석훈:** 배우 (1972년)\\n\\n이 정보가 도움이 되었기를 바랍니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 150, 'prompt_tokens': 1064, 'total_tokens': 1214, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gemma3', 'system_fingerprint': None, 'id': 'chatcmpl-69a7039ddd4e4230a3b69d08a32bee0c', 'finish_reason': 'stop', 'logprobs': None}, id='run-d04d28f9-5572-42c9-b75c-1fe452dd0c94-0', usage_metadata={'input_tokens': 1064, 'output_tokens': 150, 'total_tokens': 1214, 'input_token_details': {}, 'output_token_details': {}})]}"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = graph.invoke({'messages':[HumanMessage(content=\"오늘 날짜에 태어난 유명인들 조사해서 알려줘.\")]})\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-Amj3waChSd"
      },
      "source": [
        "이번에는 병렬 실행이 가능한 vLLM을 이용해, 리포트 작성 모듈을 구성해 봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkVkmgviChSd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZhYoD0qDaqN"
      },
      "outputs": [],
      "source": [
        "from typing_extensions import TypedDict, Annotated, Literal, List\n",
        "from pydantic import BaseModel, Field\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "\n",
        "# 전체 섹션의 구획: Contents (Chapter List)\n",
        "# Chapter: name, outline\n",
        "class Chapter(BaseModel):\n",
        "    name: str = Field(description=\"챕터의 이름\")\n",
        "    outline: str = Field(description=\"챕터의 주요 내용, 1문장 길이로\")\n",
        "\n",
        "\n",
        "class Contents(BaseModel):\n",
        "    contents: List[Chapter] = Field(description=\"전체 리포트의 섹션 구성\")\n",
        "\n",
        "parser = PydanticOutputParser(pydantic_object=Contents)\n",
        "\n",
        "format_str = parser.get_format_instructions()\n",
        "\n",
        "planner = llm | parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWO6Hp-sDaqN",
        "outputId": "b2f1a9b0-c88f-492f-fecd-c7b3ba688a73"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Chapter(name='서론: LLM의 등장과 중요성', outline='LLM의 정의, 발전 배경, 사회적 영향 및 본 보고서의 목적을 간략히 소개한다.'),\n",
              " Chapter(name='1세대 LLM: 초기 모델과 한계점 (2018-2020)', outline='GPT-1, BERT 등 초기 LLM의 등장 배경, 구조, 성능을 분석하고, 파라미터 규모 제한, 문맥 이해 부족, 생성 품질 저하 등의 한계를 지적한다.'),\n",
              " Chapter(name='2세대 LLM: 규모 확장과 Transformer 아키텍처의 발전 (2020-2022)', outline='GPT-2, GPT-3, PaLM 등 파라미터 규모가 급격히 증가하며 성능 향상을 이룬 모델들을 살펴보고, Transformer 아키텍처의 발전이 LLM 성능에 미친 영향을 분석한다.'),\n",
              " Chapter(name='3세대 LLM: Instruction Tuning과 강화 학습 (2022-2023)', outline='Instruction Tuning, RLHF (Reinforcement Learning from Human Feedback) 등 인간 피드백을 활용하여 모델 성능을 향상시키는 기법들을 소개하고, ChatGPT, Bard 등 대화형 LLM의 등장과 발전 과정을 설명한다.'),\n",
              " Chapter(name='4세대 LLM: 멀티모달, 에이전트, 그리고 미래 (2023-현재)', outline='이미지, 오디오 등 다양한 데이터를 처리하는 멀티모달 LLM, 자율적으로 작업을 수행하는 LLM 에이전트의 등장과 발전 현황을 살펴보고, LLM의 미래 발전 방향과 과제를 제시한다.'),\n",
              " Chapter(name='LLM 발전의 기술적 핵심 요소', outline='Transformer 아키텍처, Self-Attention 메커니즘, 대규모 데이터셋, 분산 학습 기술 등 LLM 발전의 핵심 기술 요소들을 상세히 설명한다.'),\n",
              " Chapter(name='LLM의 윤리적, 사회적 과제', outline='LLM의 편향성, 허위 정보 생성, 저작권 문제, 일자리 감소 등 윤리적, 사회적 과제를 분석하고, 해결 방안을 모색한다.'),\n",
              " Chapter(name='결론: LLM의 현재와 미래', outline='LLM 발전의 주요 내용들을 요약하고, LLM이 사회에 미치는 영향과 미래 전망을 제시하며 보고서를 마무리한다.')]"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "with torch.inference_mode():\n",
        "    example = planner.invoke(f\"LLM의 발전 과정에 대한 보고서 구획을 작성해 주세요. \\n{format_str}\")\n",
        "example.contents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iNjCpVeChSh",
        "outputId": "c9ef2648-481c-45c5-ebd2-e91465c57b0c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Chapter(name='1장: LLM의 태동과 초기 발전 (2018-2020)', outline='Transformer 아키텍처의 등장과 GPT-1, BERT와 같은 초기 LLM들이 등장하며 자연어 처리 분야에 혁신을 가져왔다.'),\n",
              " Chapter(name='2장: 규모 확장과 발전 (2020-2022)', outline='GPT-2, GPT-3, LaMDA 등 모델 규모가 기하급수적으로 증가하며 few-shot learning 능력이 향상되고, 다양한 task에서 뛰어난 성능을 보였다.'),\n",
              " Chapter(name='3장: 최신 동향과 미래 전망 (2022-현재)', outline='Instruction tuning, Reinforcement Learning from Human Feedback (RLHF) 등의 기법을 통해 LLM의 성능과 제어 가능성이 향상되었고, 멀티모달 LLM, 생성적 AI와의 융합 등 미래 발전 가능성이 주목받고 있다.')]"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.output_parsers import OutputFixingParser\n",
        "\n",
        "# parse 불가능한 출력이 주어지면, llm을 통해 교정하는 파서\n",
        "new_parser = OutputFixingParser.from_llm(parser=parser, llm=llm)\n",
        "\n",
        "planner = llm | new_parser\n",
        "\n",
        "with torch.inference_mode():\n",
        "    example = planner.invoke(f\"LLM의 발전 과정에 대한 3챕터 구성의 보고서 구획을 작성해 주세요. \\n{format_str}\")\n",
        "example.contents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z02IWYAIDaqN"
      },
      "source": [
        "그래프에서 사용할 State를 정의합니다.   \n",
        "\n",
        "이번에는 중간 Writer LLM이 사용할 State를 별도로 만들어 보겠습니다.   \n",
        "이렇게 구성하면 최종 State에서 필요한 부분만 저장할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_MIbKB8DaqN"
      },
      "outputs": [],
      "source": [
        "import operator\n",
        "\n",
        "# reducer 구조: operator.add\n",
        "# 단순 + 연산 구조 (리스트의 + 연산이므로 append)\n",
        "\n",
        "class State(TypedDict):\n",
        "    topic: str\n",
        "    contents: list[Chapter]\n",
        "    completed_sections: Annotated[list, operator.add]\n",
        "    final_report: str\n",
        "\n",
        "\n",
        "# 섹션 Writer가 사용할 State\n",
        "class SubState(TypedDict):\n",
        "    chapter: Chapter\n",
        "    completed_sections: Annotated[list, operator.add]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn9oVR_xDaqO"
      },
      "source": [
        "섹션을 생성하는 노드를 구성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VIpcYLzDaqO"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "def orchestrator(state: State):\n",
        "\n",
        "    prompt = ChatPromptTemplate([\n",
        "        ('system', \"주제에 대한 전문가 수준의 깊이 있는 한국어 보고서를 쓰려고 합니다. 보고서의 섹션 구성과, 각 섹션의 간단한 설명을 작성해 주세요.\"),\n",
        "        ('user', \"\"\"주제: {topic}\n",
        "---\n",
        "\n",
        "{instruction}\n",
        "\n",
        "\"\"\")\n",
        "    ])\n",
        "    chain = prompt.partial(instruction = {format_str}) | planner\n",
        "\n",
        "    # chain 결과물: Contents (contents: List[Chapter])\n",
        "\n",
        "    return {\"contents\": chain.invoke(state).contents}\n",
        "    # state: topic --> topic\n",
        "    # Return: List[Chapter]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IC0OpL4NDaqO"
      },
      "source": [
        "섹션별 내용을 처리하는 노드를 구성합니다.   \n",
        "State에는 각각의 Chapter가 아닌 Chapter의 리스트인 Contents가 들어 있는데요.   \n",
        "\n",
        "`SubState`를 이용해, 각각의 Chapter를 처리하도록 정의하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6S5AwvDbDaqO"
      },
      "outputs": [],
      "source": [
        "def llm_call(state: SubState):\n",
        "    # SubState :  chapter, completed_sections 2개 property\n",
        "\n",
        "    chapter = state['chapter']\n",
        "\n",
        "    prompt = ChatPromptTemplate([\n",
        "        ('system',\"아래 섹션에 대한 상세한 한국어 보고서를 작성하세요.\" ),\n",
        "        ('user', \"섹션 이름과 주제는 다음과 같습니다: {name} --> {outline}\")\n",
        "    ])\n",
        "\n",
        "    chain = prompt | llm\n",
        "    with torch.inference_mode():\n",
        "        return {\"completed_sections\": [chain.invoke({'name':chapter.name, 'outline':chapter.outline}).content]}\n",
        "    # 리스트로 Wrap하는 이유 중요(Reduce Operator 합치기 위해서)\n",
        "\n",
        "\n",
        "# 생성된 섹션별 결과들을 결합\n",
        "def synthesizer(state: State):\n",
        "\n",
        "    completed_sections = state[\"completed_sections\"]\n",
        "\n",
        "    completed_report_sections = \"\\n\\n---\\n\\n\".join(completed_sections)\n",
        "    # join: 전체 리스트 스트링으로 결합하기\n",
        "\n",
        "    return {\"final_report\": completed_report_sections}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gH9uKHSGDaqO"
      },
      "source": [
        "**가장 중요한 부분입니다😁😁**   \n",
        "langgraph의 Send()를 이용하면, 리스트의 원소 개수만큼 서브모듈을 호출할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuqkIRicDaqO"
      },
      "outputs": [],
      "source": [
        "from langgraph.constants import Send\n",
        "\n",
        "def assign_workers(state: State):\n",
        "    # Send: 노드를 호출하며, 값을 전달해 준다\n",
        "    # state['contents']의 개수를 기본적으로 알 수 없는데,\n",
        "    # 이를 통해 개수만큼 llm_call을 생성하여 호출할 수 있음\n",
        "    with torch.inference_mode():\n",
        "        return [Send(\"llm_call\", {\"chapter\": s}) for s in state[\"contents\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEdmwAR1DaqO"
      },
      "source": [
        "그래프를 구성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruDhpkxEDaqO"
      },
      "outputs": [],
      "source": [
        "builder = StateGraph(State)\n",
        "\n",
        "builder.add_node(\"orchestrator\", orchestrator) # 구획 짜고\n",
        "builder.add_node(\"llm_call\", llm_call) # 섹션별 글쓰고\n",
        "builder.add_node(\"synthesizer\", synthesizer) # 합치고\n",
        "\n",
        "\n",
        "builder.add_edge(START, \"orchestrator\")\n",
        "\n",
        "builder.add_conditional_edges(\"orchestrator\", assign_workers, [\"llm_call\"])\n",
        "# assign_workers의 결과에 따라 llm_call을 호출\n",
        "\n",
        "builder.add_edge(\"llm_call\", \"synthesizer\")\n",
        "# 생성된 섹션들은 synthesizer로 이동\n",
        "\n",
        "builder.add_edge(\"synthesizer\", END) # 끝\n",
        "\n",
        "\n",
        "graph = builder.compile()\n",
        "# graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4_faj1ODaqO",
        "outputId": "b29d6cd7-6c1f-46bf-e806-704e16a06c7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'orchestrator': {'contents': [Chapter(name='1장: GPT-1의 등장과 초기 발전 (2018-2019)', outline='GPT-1의 등장 배경, 아키텍처 특징, 초기 성능 및 한계를 분석하고, 이후 GPT-2로의 진화 과정을 살펴본다.'), Chapter(name='1.1 GPT-1의 탄생과 Transformer 아키텍처', outline='GPT-1의 개발 배경과 OpenAI의 목표를 설명하고, Transformer 기반 아키텍처의 핵심 개념과 GPT-1에 적용된 방식을 상세히 설명한다.'), Chapter(name='1.2 GPT-1의 성능 및 한계점', outline='GPT-1의 초기 성능을 평가하고, 텍스트 생성의 품질, 일관성, 창의성 측면에서 나타난 한계점을 분석한다.'), Chapter(name='1.3 GPT-2로의 진화: 규모 확장과 새로운 기능', outline='GPT-2의 등장 배경과 GPT-1 대비 규모 확장, 새로운 기능 (zero-shot learning) 도입의 의미를 분석하고, GPT-2의 성능 향상을 살펴본다.'), Chapter(name='2장: GPT-3와 Beyond: 규모의 확장과 새로운 패러다임 (2020-2022)', outline='GPT-3의 등장과 1750억 개의 파라미터 규모 확장, Few-shot learning 능력 향상, 그리고 다양한 활용 사례를 분석하고, GPT-3 이후 등장한 모델들의 특징을 살펴본다.'), Chapter(name='2.1 GPT-3의 등장: 규모의 혁신과 Few-shot Learning', outline='GPT-3의 개발 배경과 1750억 개의 파라미터 규모 확장의 의미를 설명하고, Few-shot learning 능력의 발전과 그 효과를 분석한다.'), Chapter(name='2.2 GPT-3의 활용 사례 및 사회적 영향', outline='GPT-3를 활용한 다양한 서비스 및 애플리케이션 사례를 소개하고, GPT-3가 사회에 미치는 긍정적/부정적 영향을 분석한다.'), Chapter(name='2.3 Codex, InstructGPT, ChatGPT 등 GPT-3 기반 모델의 등장', outline='Codex, InstructGPT, ChatGPT 등 GPT-3를 기반으로 개발된 모델들의 특징과 성능을 비교 분석하고, 각 모델이 가진 장단점을 평가한다.'), Chapter(name='3장: 최신 LLM의 발전 동향 및 미래 전망 (2023-현재)', outline='GPT-4, Gemini, Claude 등 최신 LLM의 등장 배경, 아키텍처 특징, 성능 향상 요인들을 분석하고, LLM의 미래 발전 방향과 윤리적 고려 사항을 제시한다.'), Chapter(name='3.1 GPT-4, Gemini, Claude 등 최신 LLM의 등장과 경쟁', outline='GPT-4, Gemini, Claude 등 최신 LLM의 등장 배경과 주요 특징을 소개하고, 모델 간 성능 비교 및 경쟁 구도를 분석한다.'), Chapter(name='3.2 멀티모달 LLM의 발전과 가능성', outline='텍스트뿐만 아니라 이미지, 오디오, 비디오 등 다양한 데이터를 처리할 수 있는 멀티모달 LLM의 발전 현황과 미래 가능성을 전망한다.'), Chapter(name='3.3 LLM의 윤리적 문제와 안전성 확보 방안', outline='LLM의 편향성, 허위 정보 생성, 악용 가능성 등 윤리적 문제점을 지적하고, LLM의 안전성을 확보하기 위한 기술적, 정책적 방안을 제시한다.'), Chapter(name='3.4 LLM의 미래 발전 방향: AGI(Artificial General Intelligence)를 향하여', outline='LLM의 미래 발전 방향을 제시하고, AGI(Artificial General Intelligence)를 향한 연구 개발 동향을 전망하며, LLM이 사회에 미칠 영향에 대해 논의한다.')]}}\n",
            "--------------\n",
            "{'llm_call': {'completed_sections': ['## 1장: GPT-1의 등장과 초기 발전 (2018-2019) - 거대 언어 모델의 서막과 가능성과 한계\\n\\n**1.1. 등장 배경: 언어 모델 발전의 흐름과 GPT-1의 필요성**\\n\\n2018년, OpenAI는 GPT-1 (Generative Pre-trained Transformer)을 공개하며 자연어 처리 (NLP) 분야에 큰 파장을 일으켰습니다. 그 이전까지 NLP 분야는 특정 작업 (Task-Specific)에 특화된 모델을 구축하는 데 집중되어 있었습니다. 예를 들어, 감성 분석, 기계 번역, 질의 응답 등 각 작업에 최적화된 모델을 별도로 학습시키는 방식이 일반적이었습니다. 이러한 방식은 각 작업마다 많은 양의 레이블링된 데이터를 필요로 했으며, 새로운 작업에 적용하기 위해서는 추가적인 학습 과정이 필요했습니다.\\n\\nGPT-1의 등장은 이러한 문제점을 해결하고자 하는 흐름의 결과였습니다. OpenAI는 **사전 학습 (Pre-training)**이라는 새로운 패러다임을 도입했습니다. 이는 방대한 텍스트 데이터 (Wikipedia, 웹 크롤링 데이터 등)를 사용하여 일반적인 언어 이해 능력을 먼저 학습시키고, 이후 특정 작업에 미세 조정 (Fine-tuning)하는 방식입니다. 즉, 다양한 작업을 수행하기 위한 기반 지식을 먼저 쌓은 후, 특정 작업에 필요한 지식을 추가하는 것입니다.\\n\\nGPT-1은 이러한 사전 학습 패러다임을 Transformer 아키텍처에 적용하여 구현되었습니다. Transformer는 Google에서 2017년에 발표한 \"Attention is All You Need\" 논문에서 소개된 새로운 신경망 아키텍처로, 기존의 순환 신경망 (RNN) 기반 모델의 병목 현상을 해결하고 병렬 처리를 가능하게 하여 학습 속도를 획기적으로 향상시켰습니다.\\n\\n**1.2. 아키텍처 특징: Transformer 기반의 언어 모델**\\n\\nGPT-1은 Transformer 아키텍처를 기반으로 구축되었으며, 다음과 같은 특징을 가집니다.\\n\\n* **Transformer Decoder:** GPT-1은 Transformer의 Decoder 부분만을 사용합니다. Decoder는 주어진 문맥을 바탕으로 다음 단어를 예측하는 역할을 수행합니다.\\n* **Self-Attention 메커니즘:** Transformer의 핵심 구성 요소인 Self-Attention 메커니즘은 문장 내의 단어 간의 관계를 파악하여 문맥을 이해하는 데 중요한 역할을 합니다. GPT-1은 Self-Attention 메커니즘을 통해 문장 내의 단어 간의 의존성을 효과적으로 학습했습니다.\\n* **레이어 수 및 파라미터 크기:** GPT-1은 다양한 크기로 구축되었으며, 가장 큰 모델은 11억 7천만 개의 파라미터를 가지고 있었습니다. 이는 당시로서는 매우 큰 규모의 파라미터 수였습니다.\\n* **Masked Self-Attention:** GPT-1은 미래의 단어를 참조하지 않고 과거의 단어만을 참조하도록 Masked Self-Attention 메커니즘을 사용했습니다. 이는 언어 모델링의 기본적인 가정인 \"다음 단어 예측\"을 가능하게 합니다.\\n\\n**1.3. 초기 성능 및 한계: 놀라운 생성 능력과 현실적인 제약**\\n\\nGPT-1은 사전 학습 후 다양한 NLP 작업에 미세 조정되었을 때 상당한 성능 향상을 보였습니다. 특히, 텍스트 생성 능력은 주목할 만했습니다. GPT-1은 주어진 문맥에 따라 자연스럽고 일관성 있는 텍스트를 생성할 수 있었으며, 때로는 인간이 작성한 것과 구별하기 어려울 정도의 텍스트를 생성하기도 했습니다.\\n\\n하지만 GPT-1은 다음과 같은 한계점 또한 가지고 있었습니다.\\n\\n* **문맥 이해 부족:** GPT-1은 문맥을 완벽하게 이해하지 못하고, 때로는 앞뒤가 맞지 않거나 비논리적인 텍스트를 생성하기도 했습니다. 특히, 장문의 텍스트를 생성할 때 이러한 문제가 더욱 두드러졌습니다.\\n* **사실 기반 지식 부족:** GPT-1은 방대한 텍스트 데이터를 학습했지만, 사실 기반 지식은 부족했습니다. 따라서, 사실에 기반한 질문에 정확하게 답변하지 못하거나, 잘못된 정보를 제공하기도 했습니다.\\n* **편향성 문제:** GPT-1은 학습 데이터에 존재하는 편향성을 그대로 반영했습니다. 따라서, 성별, 인종, 종교 등에 대한 편향적인 텍스트를 생성할 가능성이 있었습니다.\\n* **계산 비용:** GPT-1과 같은 거대 언어 모델을 학습하고 실행하는 데는 막대한 계산 비용이 소요되었습니다. 이는 GPT-1의 보급과 활용에 제약 요인으로 작용했습니다.\\n\\n**1.4. GPT']}}\n",
            "--------------\n",
            "{'llm_call': {'completed_sections': ['## 1.3 GPT-2로의 진화: 규모 확장과 새로운 기능\\n\\n본 섹션에서는 OpenAI에 의해 개발된 GPT-2의 등장 배경과 GPT-1 대비 괄목할 만한 규모 확장, 그리고 혁신적인 기능인 \\'zero-shot learning\\' 도입의 의미를 분석하고, GPT-2의 성능 향상을 심층적으로 살펴봅니다. GPT-2는 자연어 처리 (NLP) 분야에 큰 파장을 일으켰으며, 텍스트 생성 모델의 발전 방향을 제시하는 중요한 이정표가 되었습니다.\\n\\n**1. GPT-2 등장 배경: GPT-1의 한계와 OpenAI의 목표**\\n\\nGPT-2의 등장은 GPT-1의 성공적인 개발과 그 한계를 극복하고자 하는 OpenAI의 노력에서 비롯되었습니다. GPT-1은 트랜스포머(Transformer) 아키텍처를 기반으로 대규모 텍스트 데이터셋을 학습하여 인간과 유사한 텍스트를 생성하는 능력을 보여주었지만, 몇 가지 중요한 한계를 드러냈습니다.\\n\\n* **제한적인 생성 능력:** GPT-1은 짧은 텍스트 생성에는 능숙했지만, 긴 텍스트를 일관성 있게 생성하는 데 어려움을 겪었습니다. 맥락을 유지하고 논리적인 흐름을 이어가는 데 한계가 있었습니다.\\n* **특정 작업에 대한 의존성:** GPT-1은 특정 작업에 맞게 미세 조정(fine-tuning)을 거쳐야 좋은 성능을 보였습니다. 즉, 다양한 작업에 유연하게 대응하기 어려웠습니다.\\n* **윤리적 우려:** GPT-1이 생성하는 텍스트는 때때로 편향되거나 유해한 내용을 포함할 수 있다는 우려가 제기되었습니다.\\n\\n이러한 한계를 극복하고, 더욱 강력하고 유연하며 윤리적인 텍스트 생성 모델을 개발하고자 OpenAI는 GPT-2를 개발하게 된 것입니다.\\n\\n**2. GPT-1 대비 규모 확장: 파라미터 수 증가와 데이터셋 변화**\\n\\nGPT-2의 가장 큰 특징은 GPT-1 대비 규모가 대폭 확장되었다는 점입니다. GPT-1은 11억 7천만 개의 파라미터를 가지고 있었지만, GPT-2는 15억 개, 34억 개, 77억 개 등 다양한 규모의 모델을 제공했습니다. 특히 77억 개 파라미터를 가진 모델은 GPT-1의 규모를 훨씬 뛰어넘는 수준입니다.\\n\\n규모 확장은 단순히 파라미터 수 증가에 그치지 않고, 학습 데이터셋의 변화를 동반했습니다. GPT-2는 웹 크롤링을 통해 수집된 40GB의 텍스트 데이터셋을 사용했습니다. 이 데이터셋은 GPT-1에 사용된 데이터셋보다 훨씬 다양하고 방대한 내용을 담고 있어, GPT-2가 더 넓은 범위의 지식을 학습하고 다양한 스타일의 텍스트를 생성할 수 있도록 했습니다.\\n\\n* **데이터셋 구성:** GPT-2 학습 데이터셋은 웹에서 수집된 텍스트 데이터로, Reddit 웹사이트의 댓글과 링크를 통해 수집된 데이터를 포함합니다. 이러한 데이터는 다양한 주제, 스타일, 어조를 포함하고 있어 GPT-2의 일반적인 텍스트 생성 능력을 향상시키는 데 기여했습니다.\\n\\n**3. 새로운 기능: Zero-Shot Learning의 도입**\\n\\nGPT-2의 가장 중요한 혁신은 \\'zero-shot learning\\' 기능의 도입입니다. Zero-shot learning은 특정 작업에 대한 학습 데이터 없이도 해당 작업을 수행할 수 있는 능력을 의미합니다. GPT-2는 대규모 텍스트 데이터셋을 통해 일반적인 언어 이해 능력을 학습하고, 이를 바탕으로 다양한 작업을 수행할 수 있습니다.\\n\\n* **Zero-Shot Learning의 원리:** GPT-2는 텍스트 생성 모델로서, 주어진 프롬프트(prompt)에 따라 텍스트를 생성합니다. 사용자는 원하는 작업에 대한 설명을 프롬프트에 포함시키면, GPT-2는 해당 작업에 대한 학습 데이터 없이도 텍스트를 생성하여 작업을 수행합니다. 예를 들어, \"다음 문장을 프랑스어로 번역하세요:\" 라는 프롬프트를 제공하면, GPT-2는 해당 문장을 프랑스어로 번역한 텍스트를 생성합니다.\\n* **Zero-Shot Learning의 의미:** Zero-Shot Learning 기능은 GPT-2를 더욱 유연하고 활용 가능하게 만들었습니다. 특정 작업에 대한 미세 조정 없이도 다양한 작업을 수행할 수 있어, 다양한 분야에서 활용될 수 있는 가능성을 열었습니다.\\n\\n**4. GPT-2의 성능 향상: 텍스트 생성 품질 및 다양성**\\n\\nGPT-2는 규모 확장과 Zero-Shot Learning 기능 도입을 통해 GPT-1']}}\n",
            "--------------\n",
            "{'llm_call': {'completed_sections': ['## 1.2 GPT-1의 성능 및 한계점: 초기 성능 평가와 텍스트 생성의 한계 분석\\n\\nGPT-1 (Generative Pre-trained Transformer 1)은 OpenAI에서 2018년에 공개한 초기 대규모 언어 모델로, 딥러닝 기반의 자연어 처리 (NLP) 분야에 큰 영향을 미쳤습니다. GPT-1은 방대한 양의 텍스트 데이터 (웹 크롤링 데이터)를 사용하여 사전 훈련되었으며, 주어진 프롬프트 (prompt)에 기반하여 텍스트를 생성하는 능력을 보유했습니다. 본 보고서는 GPT-1의 초기 성능을 평가하고, 텍스트 생성의 품질, 일관성, 창의성 측면에서 나타난 주요 한계점을 상세히 분석합니다.\\n\\n**1. GPT-1의 초기 성능 평가**\\n\\nGPT-1은 공개 당시, 여러 NLP 작업에서 상당한 성능을 보였습니다. 특히, 다음과 같은 측면에서 주목할 만한 성과를 거두었습니다.\\n\\n* **문맥 이해 능력:** GPT-1은 비교적 긴 문맥을 이해하고 이를 기반으로 텍스트를 생성할 수 있었습니다. 이는 이전 모델에 비해 괄목할 만한 발전이었으며, 문맥에 따른 의미 변화를 어느 정도 파악할 수 있음을 시사했습니다.\\n* **다양한 스타일의 텍스트 생성:** GPT-1은 다양한 스타일의 텍스트를 생성할 수 있었습니다. 예를 들어, 뉴스 기사, 소설, 시 등 다양한 장르의 텍스트를 유사하게 모방하여 생성할 수 있었습니다. 이는 텍스트 생성 모델의 범용성을 입증하는 중요한 요소였습니다.\\n* **제로-샷 학습 (Zero-Shot Learning) 능력:** GPT-1은 특정 작업에 대한 명시적인 훈련 없이도, 주어진 프롬프트만으로 어느 정도의 작업을 수행할 수 있는 제로-샷 학습 능력을 보여주었습니다. 이는 모델이 일반적인 언어 패턴을 학습하여 새로운 작업에 적용할 수 있음을 의미했습니다.\\n\\n**2. 텍스트 생성의 품질 측면에서 나타난 한계점**\\n\\nGPT-1은 초기 모델로서, 텍스트 생성의 품질 측면에서 다음과 같은 한계점을 드러냈습니다.\\n\\n* **반복적인 텍스트 생성:** GPT-1은 텍스트 생성 과정에서 특정 단어나 구문을 반복적으로 사용하는 경향이 있었습니다. 이는 모델이 텍스트의 다양성을 확보하는 데 어려움을 겪고 있음을 시사합니다. 특히, 긴 텍스트를 생성할 때 이러한 문제가 더욱 두드러졌습니다.\\n* **의미 불명확성 및 비논리성:** GPT-1이 생성하는 텍스트는 때때로 의미가 불명확하거나 논리적으로 모순되는 경우가 있었습니다. 이는 모델이 텍스트의 의미를 정확하게 이해하고, 이를 바탕으로 일관성 있는 텍스트를 생성하는 데 한계가 있음을 보여줍니다.\\n* **사실 기반 오류 (Factuality Errors):** GPT-1은 훈련 데이터에 포함된 잘못된 정보를 그대로 반영하여 사실과 다른 텍스트를 생성하는 경우가 있었습니다. 이는 모델이 텍스트의 진실성을 검증하는 능력이 부족함을 의미하며, 생성된 텍스트의 신뢰성을 저해하는 요인이었습니다.\\n* **맥락 파악의 불완전성:** GPT-1은 문맥을 어느 정도 이해할 수 있었지만, 복잡하고 미묘한 맥락을 정확하게 파악하는 데 어려움을 겪었습니다. 특히, 문맥에 따라 의미가 달라지는 단어나 구문을 잘못 해석하여 부적절한 텍스트를 생성하는 경우가 있었습니다.\\n\\n**3. 텍스트 생성의 일관성 측면에서 나타난 한계점**\\n\\nGPT-1은 텍스트 생성의 일관성 측면에서도 다음과 같은 한계점을 보였습니다.\\n\\n* **주제 일관성 유지의 어려움:** GPT-1은 긴 텍스트를 생성할 때 주제 일관성을 유지하는 데 어려움을 겪었습니다. 텍스트의 초반부에는 특정 주제에 대해 논의했지만, 후반부에는 주제가 벗어나거나 관련 없는 내용을 담는 경우가 있었습니다.\\n* **캐릭터 일관성 유지의 어려움 (스토리텔링):** GPT-1을 사용하여 스토리를 생성할 때, 캐릭터의 성격이나 행동 패턴을 일관성 있게 유지하는 데 어려움을 겪었습니다. 캐릭터의 성격이 갑작스럽게 변하거나, 상황에 맞지 않는 행동을 하는 경우가 있었습니다.\\n* **문체 일관성 유지의 어려움:** GPT-1은 다양한 스타일의 텍스트를 생성할 수 있었지만, 생성된 텍스트 전체에 걸쳐']}}\n",
            "--------------\n",
            "{'llm_call': {'completed_sections': ['## 1.1 GPT-1의 탄생과 Transformer 아키텍처: 자연어 처리의 새로운 지평을 열다\\n\\n본 섹션에서는 OpenAI에 의해 개발된 GPT-1의 탄생 배경과 그 핵심 기술인 Transformer 아키텍처에 대해 상세히 설명합니다. GPT-1은 자연어 처리 (NLP) 분야에 혁명적인 변화를 가져왔으며, 특히 텍스트 생성 분야에서 괄목할 만한 성과를 보여주었습니다.\\n\\n**1. GPT-1 개발 배경 및 OpenAI의 목표**\\n\\nOpenAI는 인공지능 (AI)의 안전하고 긍정적인 발전을 목표로 하는 연구 기관입니다. GPT-1 개발 당시 OpenAI는 기존의 NLP 모델들이 특정 작업에 맞춰 설계되어 범용적인 문제 해결 능력은 부족하다는 점을 인지했습니다.  기존 모델들은 주로 지도 학습 (Supervised Learning) 방식으로, 특정 작업에 대한 레이블링된 데이터셋을 필요로 했습니다.  이는 데이터 수집 및 레이블링에 상당한 비용과 노력이 소요되었으며, 새로운 작업에 대한 적응성이 떨어지는 단점을 가지고 있었습니다.\\n\\nOpenAI는 이러한 문제점을 해결하고, **“제로샷 (Zero-shot) 학습”** 능력을 갖춘 모델을 개발하고자 했습니다. 제로샷 학습이란, 특정 작업에 대한 학습 데이터 없이도 해당 작업을 수행할 수 있는 능력을 의미합니다.  즉, 다양한 작업을 수행할 수 있는 범용적인 언어 모델을 구축하는 것을 목표로 했습니다.  GPT-1은 이러한 목표를 달성하기 위한 첫 번째 단계였으며, 방대한 텍스트 데이터셋을 통해 언어의 패턴을 학습하고, 이를 기반으로 다양한 텍스트 생성 작업을 수행할 수 있도록 설계되었습니다.\\n\\n**2. Transformer 아키텍처의 핵심 개념**\\n\\nGPT-1의 핵심 기술은 Google에서 2017년에 발표한 **Transformer 아키텍처**입니다.  Transformer는 기존의 순환 신경망 (RNN) 기반 모델들이 가지고 있던 문제점을 해결하고, 병렬 처리를 통해 학습 속도를 획기적으로 향상시킨 혁신적인 구조입니다.  Transformer의 핵심 개념은 다음과 같습니다.\\n\\n* **Attention 메커니즘:**  Transformer의 가장 중요한 특징은 Attention 메커니즘입니다.  Attention은 입력 시퀀스의 각 단어가 다른 단어와 얼마나 관련이 있는지 계산하여, 중요한 단어에 집중하도록 합니다.  이는 문맥을 파악하고, 장거리 의존성을 효과적으로 처리하는 데 도움이 됩니다.  기존 RNN 기반 모델들은 시퀀스의 앞부분에 있는 정보가 뒷부분으로 전달될수록 희석되는 문제 (Vanishing Gradient Problem)를 가지고 있었지만, Attention 메커니즘은 이러한 문제를 완화합니다.\\n* **Self-Attention:**  Transformer는 입력 시퀀스 내의 단어 간의 관계를 파악하기 위해 Self-Attention을 사용합니다.  Self-Attention은 입력 시퀀스의 각 단어를 다른 모든 단어와 비교하여 관련성을 평가하고, 이를 기반으로 각 단어의 표현을 업데이트합니다.\\n* **Encoder-Decoder 구조:**  Transformer는 Encoder와 Decoder라는 두 개의 모듈로 구성됩니다.  Encoder는 입력 시퀀스를 처리하여 문맥 정보를 담고 있는 표현을 생성하고, Decoder는 이 표현을 기반으로 출력 시퀀스를 생성합니다.  GPT-1은 Decoder만 사용한 구조입니다.\\n* **Positional Encoding:**  Transformer는 순서 정보를 처리하지 않기 때문에, Positional Encoding을 사용하여 단어의 위치 정보를 모델에 제공합니다.  Positional Encoding은 각 단어의 위치에 따라 다른 값을 부여하여, 모델이 단어의 순서를 인식할 수 있도록 합니다.\\n* **Multi-Head Attention:**  Transformer는 여러 개의 Attention 헤드를 사용하여 다양한 관점에서 단어 간의 관계를 파악합니다.  각 헤드는 서로 다른 방식으로 Attention을 계산하며, 이를 통해 모델은 더욱 풍부한 문맥 정보를 학습할 수 있습니다.\\n\\n**3. GPT-1에 적용된 Transformer 아키텍처**\\n\\nGPT-1은 Transformer 아키텍처의 **Decoder 부분만 사용**한 구조입니다.  이는 GPT-1의 목표가 특정 작업에 대한 학습 데이터 없이도 텍스트를 생성하는 것이었기 때문입니다.  GPT-1은 다음과 같은 특징을 가집니다.\\n\\n* **모델 크기:** GPT-1은 다양한 크기로 구축되었으며, 가장 큰 모델은 11억 7천만 개의 파라미터를 가지고 있습니다.\\n* **학습 데이터:** GPT-1은 Common Crawl 데이터셋을 포함한 방대한 텍스트 데이터셋을 사용하여 학습되었습니다.  Common Crawl은 웹 페이지의 텍스트 데이터를 수집한 데이터셋으로, GPT-1은 이 데이터셋을 통해']}}\n",
            "--------------\n",
            "{'llm_call': {'completed_sections': ['## 2장: GPT-3와 Beyond: 규모의 확장과 새로운 패러다임 (2020-2022)\\n\\n**1. GPT-3의 등장과 1750억 개의 파라미터 규모 확장: 게임 체인저의 탄생**\\n\\n2020년, OpenAI에서 발표한 GPT-3는 자연어 처리(NLP) 분야에 혁명적인 변화를 가져왔다. 기존 모델들과 비교했을 때 압도적인 규모, 즉 1750억 개의 파라미터를 가진 거대한 규모는 단순한 숫자를 넘어, 모델의 성능과 가능성을 비약적으로 끌어올리는 핵심 요인이었다.\\n\\n* **파라미터 규모의 의미:** 파라미터는 신경망의 연결 강도를 나타내는 값으로, 모델이 학습한 지식을 저장하는 역할을 한다. 파라미터 규모가 클수록 모델은 더 많은 정보를 기억하고 복잡한 패턴을 학습할 수 있으며, 이는 결과적으로 더 자연스럽고 유창한 텍스트 생성 능력으로 이어진다.\\n* **GPT-3의 학습 데이터:** GPT-3는 인터넷 상의 방대한 텍스트 데이터(Common Crawl, WebText2, Books1&2, Wikipedia)를 사용하여 학습되었다. 이러한 데이터의 다양성은 GPT-3가 다양한 주제와 스타일에 대한 이해도를 높이는 데 기여했다.\\n* **기존 모델과의 비교:** GPT-2 (15억 파라미터)와 비교했을 때, GPT-3는 파라미터 규모가 10배 이상 증가했으며, 이는 성능 향상으로 이어졌다. GPT-3는 이전 모델들이 어려움을 겪었던 복잡한 추론, 창의적인 글쓰기, 코딩 등 다양한 작업에서 뛰어난 성능을 보였다.\\n\\n**2. Few-shot Learning 능력 향상: 지시사항 이해의 진화**\\n\\nGPT-3의 가장 중요한 특징 중 하나는 Few-shot Learning 능력이다. Few-shot Learning은 모델에게 몇 개의 예시(few shots)만 제공하면 새로운 작업에 대한 이해도를 빠르게 높이고, 유사한 방식으로 작업을 수행할 수 있도록 하는 학습 방식이다.\\n\\n* **Zero-shot, One-shot, Few-shot 학습:**\\n    * **Zero-shot Learning:** 모델에게 어떤 예시도 제공하지 않고, 단순히 지시사항만으로 작업을 수행하도록 한다.\\n    * **One-shot Learning:** 모델에게 하나의 예시만 제공하고, 이를 바탕으로 작업을 수행하도록 한다.\\n    * **Few-shot Learning:** 모델에게 몇 개의 예시를 제공하고, 이를 바탕으로 작업을 수행하도록 한다.\\n* **GPT-3의 Few-shot Learning 능력:** GPT-3는 Few-shot Learning 능력이 뛰어나, 몇 개의 예시만으로도 다양한 작업(번역, 질문 답변, 요약, 코드 생성 등)을 수행할 수 있었다. 이는 모델이 텍스트의 패턴을 이해하고, 이를 일반화하여 새로운 상황에 적용할 수 있는 능력을 보여준다.\\n* **Few-shot Learning의 의미:** Few-shot Learning 능력은 모델의 학습 효율성을 높이고, 특정 작업에 대한 데이터 부족 문제를 해결하는 데 기여한다. 또한, 사용자가 모델에게 작업을 지시하는 방식을 더욱 직관적으로 만들 수 있다.\\n\\n**3. GPT-3의 다양한 활용 사례: 잠재력의 무한한 확장**\\n\\nGPT-3는 Few-shot Learning 능력을 바탕으로 다양한 분야에서 활용 가능성을 보여주었다.\\n\\n* **콘텐츠 생성:** 블로그 게시물, 소설, 시, 광고 문구 등 다양한 종류의 콘텐츠를 생성할 수 있다.\\n* **챗봇:** 자연스러운 대화가 가능한 챗봇을 구축하여 고객 서비스, 교육, 엔터테인먼트 등 다양한 분야에 적용할 수 있다.\\n* **코드 생성:** 자연어 설명을 기반으로 코드를 생성하여 개발 생산성을 향상시킬 수 있다.\\n* **번역:** 다양한 언어 간의 번역을 수행할 수 있다.\\n* **질문 답변:** 복잡한 질문에 대한 답변을 제공할 수 있다.\\n* **요약:** 긴 텍스트를 요약하여 핵심 내용을 전달할 수 있다.\\n* **교육:** 학생들에게 맞춤형 학습 자료를 제공하고, 질문에 답변하는 등 교육적인 목적으로 활용할 수 있다.\\n\\n**4. GPT-3 이후 등장한 모델들의 특징: 새로운 패러다임의 등장**\\n\\nGPT-3의 성공 이후, 더 크고 강력한 모델들이 등장하며 NLP 분야는 더욱 빠르게 발전했다.\\n\\n* **PaLM (Google):** 5400억 개의 파라미터를 가진 Google의 PaLM은 GPT-3보다 더 큰 규모를 자랑하며, 복잡한 추론']}}\n",
            "--------------\n",
            "{'llm_call': {'completed_sections': ['## 2.2 GPT-3의 활용 사례 및 사회적 영향\\n\\n**서론:**\\n\\nGPT-3 (Generative Pre-trained Transformer 3)는 OpenAI에서 개발한 거대 언어 모델로, 방대한 텍스트 데이터를 학습하여 인간과 유사한 텍스트를 생성하는 능력을 보유하고 있습니다. GPT-3의 등장으로 다양한 분야에서 혁신적인 서비스 및 애플리케이션 개발이 가능해졌으며, 이는 사회 전반에 걸쳐 긍정적 및 부정적 영향을 미치고 있습니다. 본 보고서는 GPT-3를 활용한 다양한 서비스 및 애플리케이션 사례를 소개하고, GPT-3가 사회에 미치는 긍정적/부정적 영향을 분석하여, GPT-3 기술의 발전과 함께 고려해야 할 윤리적, 사회적 과제를 제시합니다.\\n\\n**1. GPT-3를 활용한 다양한 서비스 및 애플리케이션 사례**\\n\\nGPT-3는 텍스트 생성 능력 외에도 다양한 기능을 통해 여러 분야에서 활용되고 있습니다. 주요 활용 사례는 다음과 같습니다.\\n\\n* **콘텐츠 생성:**\\n    * **블로그 게시물, 기사 작성:** 특정 주제에 대한 블로그 게시물이나 기사를 자동으로 생성하여 콘텐츠 제작 시간을 단축하고 비용을 절감합니다.\\n    * **마케팅 카피 작성:** 광고 문구, 제품 설명, 이메일 마케팅 콘텐츠 등 다양한 마케팅 카피를 생성하여 마케팅 효율성을 향상시킵니다.\\n    * **시나리오, 소설, 시 등 창작:** GPT-3는 창작 활동에도 활용될 수 있으며, 아이디어 발상, 초고 작성 등 창작 과정을 지원합니다.\\n    * **코드 생성:** 간단한 코드를 생성하거나, 기존 코드를 수정하는 데 활용될 수 있습니다. 개발자의 생산성을 향상시키고, 프로그래밍 지식이 부족한 사람도 코드를 생성할 수 있도록 돕습니다.\\n* **고객 서비스:**\\n    * **챗봇 구축:** GPT-3 기반 챗봇은 자연스러운 대화 능력을 통해 고객 문의에 응답하고 문제 해결을 지원합니다. 24시간 응대가 가능하며, 다양한 언어를 지원하여 글로벌 고객 서비스 제공에 용이합니다.\\n    * **FAQ 자동 생성:** 자주 묻는 질문(FAQ)을 자동으로 생성하여 고객 지원 효율성을 높입니다.\\n* **교육:**\\n    * **맞춤형 학습 자료 생성:** 학생의 수준과 관심사에 맞는 맞춤형 학습 자료를 생성하여 학습 효과를 높입니다.\\n    * **자동 채점 및 피드백:** 에세이, 논문 등 주관식 답안을 자동으로 채점하고 피드백을 제공하여 교사의 업무 부담을 줄입니다.\\n    * **언어 학습 지원:** 외국어 학습 시, GPT-3는 번역, 문법 교정, 대화 연습 등 다양한 방식으로 학습을 지원합니다.\\n* **연구 및 개발:**\\n    * **데이터 분석 및 요약:** 방대한 텍스트 데이터를 분석하고 핵심 내용을 요약하여 연구자의 분석 시간을 단축합니다.\\n    * **가설 생성 및 검증:** 연구 주제에 대한 가설을 생성하고 관련 정보를 검색하여 가설의 타당성을 검증하는 데 활용됩니다.\\n* **기타:**\\n    * **이메일 자동 작성:** 업무 관련 이메일을 자동으로 작성하여 업무 효율성을 높입니다.\\n    * **번역:** 다양한 언어 간 번역을 수행하며, 특히 문맥을 이해하고 자연스러운 번역을 제공하는 데 강점을 보입니다.\\n    * **게임 개발:** 게임 내 캐릭터의 대사를 생성하거나, 게임 스토리를 개발하는 데 활용됩니다.\\n\\n**2. GPT-3가 사회에 미치는 긍정적 영향**\\n\\nGPT-3 기술의 발전은 사회 전반에 걸쳐 다음과 같은 긍정적인 영향을 미칠 수 있습니다.\\n\\n* **생산성 향상:** 콘텐츠 생성, 코드 작성, 고객 서비스 등 다양한 분야에서 업무 효율성을 높여 생산성을 향상시킵니다.\\n* **접근성 향상:** 프로그래밍 지식이 부족한 사람도 코드를 생성하거나, 외국어 학습을 지원받을 수 있도록 하여 정보 접근성을 높입니다.\\n* **창의성 증진:** 아이디어 발상, 초고 작성 등 창작 과정을 지원하여 창의적인 활동을 촉진합니다.\\n* **새로운 비즈니스 기회 창출:** GPT-3를 활용한 새로운 서비스 및 애플리케이션 개발을 통해 새로운 비즈니스 기회를 창출합니다.\\n* **교육 기회 확대:** 맞춤형 학습 자료 제공, 자동 채점 등 교육 관련 서비스를 통해 교육 기회를 확대합니다.\\n\\n**3. GPT-3가 사회에 미치는 부정적 영향**']}}\n",
            "--------------\n",
            "{'llm_call': {'completed_sections': ['## 3장: 최신 LLM의 발전 동향 및 미래 전망 (2023-현재) - GPT-4, Gemini, Claude 등\\n\\n**1. 서론: LLM의 급격한 진화와 2023년 이후의 변화**\\n\\n2023년 이후, 대규모 언어 모델(Large Language Models, LLMs) 분야는 이전과는 비교할 수 없을 정도로 빠른 속도로 발전하고 있습니다. GPT-3의 등장 이후, OpenAI의 GPT-4, Google의 Gemini, Anthropic의 Claude 등 더욱 강력하고 다재다능한 LLM들이 잇따라 공개되면서 LLM 기술은 실용적인 수준으로 빠르게 도약했습니다. 본 장에서는 이러한 최신 LLM들의 등장 배경, 핵심 아키텍처 특징, 성능 향상 요인들을 분석하고, LLM의 미래 발전 방향과 함께 고려해야 할 윤리적 문제들을 심층적으로 논의합니다.\\n\\n**2. 주요 LLM 등장 배경 및 경쟁 구도**\\n\\n* **OpenAI - GPT-4:** GPT-3의 성공에 힘입어 OpenAI는 2023년 GPT-4를 공개했습니다. GPT-4는 텍스트뿐만 아니라 이미지 입력도 지원하는 멀티모달(Multimodal) 기능을 갖추고 있으며, GPT-3보다 더욱 복잡하고 미묘한 지시사항을 이해하고 수행할 수 있는 능력을 보여주었습니다. 이는 OpenAI의 독점적인 위치를 더욱 공고히 하는 요인이 되었습니다.\\n* **Google - Gemini:** Google은 GPT-4에 대응하기 위해 Gemini를 개발했습니다. Gemini는 Ultra, Pro, Nano 세 가지 버전으로 출시되었으며, 특히 Ultra 버전은 다양한 벤치마크 테스트에서 GPT-4를 능가하는 성능을 보여주며 LLM 경쟁에 새로운 동력을 불어넣었습니다. Gemini의 특징은 처음부터 멀티모달 모델로 설계되었다는 점이며, 이는 텍스트, 이미지, 오디오, 비디오 등 다양한 형태의 데이터를 통합적으로 처리할 수 있는 잠재력을 시사합니다.\\n* **Anthropic - Claude:** Anthropic은 안전하고 윤리적인 AI 개발을 목표로 Claude 모델을 개발했습니다. Claude는 긴 컨텍스트를 처리하는 능력과 인간과 유사한 대화 능력을 갖추고 있으며, 특히 기업 환경에서 활용될 수 있는 보안 및 규정 준수 기능을 강화했습니다. Anthropic은 ‘Constitutional AI’라는 독특한 접근 방식을 통해 모델의 행동을 제어하고 안전성을 확보하고 있습니다.\\n\\n이러한 LLM들의 등장은 AI 시장의 경쟁을 더욱 심화시키고 있으며, 각 기업들은 성능 향상뿐만 아니라 안전성, 윤리성, 비용 효율성 등 다양한 측면에서 경쟁하고 있습니다.\\n\\n**3. 아키텍처 특징 및 성능 향상 요인**\\n\\n* **모델 규모:** LLM의 성능 향상은 모델 규모의 증가와 밀접하게 관련되어 있습니다. GPT-4, Gemini, Claude 등 최신 LLM들은 수조 개 이상의 파라미터를 가지고 있으며, 이는 더욱 복잡한 패턴을 학습하고 이해할 수 있게 합니다. 하지만 모델 규모의 증가는 막대한 컴퓨팅 자원과 에너지 소비를 필요로 하는 문제점을 야기합니다.\\n* **트랜스포머 아키텍처의 발전:** LLM은 대부분 트랜스포머 아키텍처를 기반으로 합니다. 최신 LLM들은 트랜스포머 아키텍처의 효율성을 높이기 위해 다양한 기술들을 적용하고 있습니다. 예를 들어, Sparse Attention, FlashAttention 등은 계산 복잡도를 줄이고 메모리 사용량을 최적화하여 모델의 확장성을 높입니다.\\n* **멀티모달 학습:** GPT-4와 Gemini는 멀티모달 학습을 통해 텍스트뿐만 아니라 이미지, 오디오, 비디오 등 다양한 형태의 데이터를 처리할 수 있게 되었습니다. 이는 LLM의 활용 범위를 크게 확장시키고 있으며, 더욱 자연스럽고 지능적인 상호 작용을 가능하게 합니다.\\n* **강화 학습 (Reinforcement Learning):** Anthropic의 Claude는 ‘Constitutional AI’라는 독특한 접근 방식을 통해 강화 학습을 활용하여 모델의 행동을 제어하고 안전성을 확보하고 있습니다. 이는 LLM의 윤리적 문제 해결에 새로운 가능성을 제시합니다.\\n* **데이터 품질 및 양:** LLM의 성능은 학습 데이터의 품질과 양에 크게 의존합니다. 최신 LLM들은 더욱 방대한 양의 데이터를 수집하고, 데이터의 품질을 높이기 위한 노력을 기울이고 있습니다.\\n\\n**4. LLM의 미래 발전 방향**\\n\\n* **모델 경량화 (Model Compression):** LLM의 규모가 커짐에 따라, 모델 경량화 기술의 중요성이 더욱 커지고 있습니다.']}}\n",
            "--------------\n",
            "{'llm_call': {'completed_sections': ['## 2.1 GPT-3의 등장: 규모의 혁신과 Few-shot Learning\\n\\n본 섹션에서는 OpenAI에 의해 개발된 GPT-3 (Generative Pre-trained Transformer 3)의 등장 배경과 그 의미, 특히 파라미터 규모 확장에 따른 Few-shot Learning 능력의 발전과 그 효과를 심층적으로 분석하고자 한다. GPT-3는 자연어 처리 (NLP) 분야에 지대한 영향을 미쳤으며, 기존 모델들과 비교하여 괄목할 만한 성능 향상을 보여주었다.\\n\\n**1. GPT-3 개발 배경: Transformer 아키텍처와 사전학습의 진화**\\n\\nGPT-3의 등장은 자연스럽게 Transformer 아키텍처의 발전과 긴밀하게 연결되어 있다. 2017년 Google에서 발표한 Transformer는 기존 RNN (Recurrent Neural Network) 기반 모델의 단점을 극복하고, 병렬 처리를 통해 학습 속도를 획기적으로 향상시켰다.  Transformer는 Attention 메커니즘을 핵심으로 사용하여 문장 내 단어들 간의 관계를 효과적으로 파악하고, 문맥을 이해하는 능력을 크게 향상시켰다.\\n\\nGPT (Generative Pre-trained Transformer)는 이러한 Transformer 아키텍처를 기반으로, 방대한 텍스트 데이터셋을 활용하여 사전학습 (Pre-training)을 진행하는 모델이다. 사전학습 단계에서 GPT는 주어진 문맥을 기반으로 다음 단어를 예측하는 방식으로 언어의 통계적 패턴을 학습한다.  이후, 특정 작업 (Task)에 맞게 미세 조정 (Fine-tuning)을 통해 성능을 최적화한다.\\n\\nGPT-1, GPT-2를 거치면서 사전학습 데이터의 규모와 모델의 크기는 점진적으로 증가해왔다. GPT-1은 117M 파라미터를 사용했고, GPT-2는 1.5B 파라미터를 사용했다. 하지만 GPT-3는 이러한 추세를 넘어선 혁신적인 규모를 자랑한다.\\n\\n**2. 1750억 개의 파라미터 규모 확장의 의미: 언어 이해 능력의 비약적인 발전**\\n\\nGPT-3는 **1750억 개의 파라미터**를 갖는 거대한 규모의 언어 모델이다. 이는 GPT-2보다 약 100배 더 큰 규모이며, 기존 자연어 처리 모델들과 비교하여 압도적인 크기이다.  이러한 파라미터 규모의 증가는 GPT-3의 언어 이해 능력에 비약적인 발전을 가져왔다.\\n\\n* **더욱 풍부한 지식 습득:** 1750억 개의 파라미터는 방대한 양의 텍스트 데이터를 통해 더 많은 지식을 습득하고, 다양한 문맥과 어휘를 이해할 수 있게 한다.\\n* **미묘한 뉘앙스 이해:**  더욱 복잡한 패턴을 학습함으로써 문장의 미묘한 뉘앙스, 비유, 은유 등을 이해하는 능력이 향상되었다.\\n* **일반화 능력 향상:** 특정 작업에 대한 미세 조정 없이도 다양한 작업을 수행할 수 있는 일반화 능력 (Generalization)이 크게 향상되었다.\\n\\n**3. Few-shot Learning 능력의 발전과 그 효과: 제로샷, 원샷, 그리고 Few-shot**\\n\\nGPT-3의 가장 중요한 특징 중 하나는 **Few-shot Learning** 능력이다. Few-shot Learning은 모델이 극소량의 예시 (Example)만으로 새로운 작업을 수행할 수 있는 능력을 의미한다. GPT-3는 Few-shot Learning을 통해 기존 모델들이 필요로 했던 많은 양의 레이블링된 데이터 (Labeled Data) 없이도 다양한 작업을 수행할 수 있게 되었다.\\n\\nFew-shot Learning은 크게 다음과 같은 세 가지 단계로 나눌 수 있다:\\n\\n* **제로샷 (Zero-shot):**  예시 없이 바로 작업을 수행한다. GPT-3는 제로샷에서도 어느 정도의 성능을 보이지만, 기대만큼의 결과는 나오지 않는다.\\n* **원샷 (One-shot):**  하나의 예시만 제공하여 작업을 수행한다.  GPT-3는 원샷에서도 상당한 성능 향상을 보인다.\\n* **Few-shot:**  몇 개의 예시 (일반적으로 3~5개)를 제공하여 작업을 수행한다. GPT-3는 Few-shot 환경에서 가장 뛰어난 성능을 발휘하며, 인간 수준의 성능에 근접하는 결과를 보여준다.\\n\\nGPT-3의 Few-shot Learning 능력은 다음과 같은 효과를 가져왔다:\\n\\n* **데이터 부족 문제 해결:** 레이블링된 데이터가 부족한 작업에서도 효과적으로 수행할 수 있게 되었다.\\n* **빠른 적응력:** 새로운']}}\n",
            "--------------\n",
            "{'llm_call': {'completed_sections': ['## 3.2 멀티모달 LLM의 발전과 가능성: 텍스트를 넘어 다양한 데이터를 이해하고 활용하는 미래\\n\\n**1. 서론: 멀티모달 LLM의 등장 배경과 의미**\\n\\n최근 몇 년간 챗GPT와 같은 거대 언어 모델(LLM)의 급격한 발전은 인공지능 분야에 혁신적인 변화를 가져왔습니다. 그러나 LLM은 기본적으로 텍스트 데이터에 특화되어 있어, 현실 세계의 복잡하고 다양한 정보를 처리하는 데 한계점을 드러냈습니다. 이에 따라 인간의 인지 방식처럼 텍스트뿐만 아니라 이미지, 오디오, 비디오 등 다양한 형태의 데이터를 이해하고 상호 작용하는 **멀티모달 LLM(Multimodal Large Language Model)**이 주목받기 시작했습니다. 멀티모달 LLM은 단순히 여러 모달리티를 결합하는 것을 넘어, 각 모달리티 간의 관계를 파악하고 통합적인 이해를 가능하게 함으로써 더욱 지능적인 AI 시스템 구축의 가능성을 열고 있습니다.\\n\\n**2. 멀티모달 LLM의 발전 현황**\\n\\n멀티모달 LLM의 발전은 크게 세 가지 방향으로 진행되고 있습니다.\\n\\n*   **기존 LLM에 모달리티 추가:** GPT-4, Gemini, LLaVA와 같이 기존 LLM에 이미지 처리 능력을 추가하여 텍스트와 이미지를 함께 이해하고 생성하는 방식으로 발전해 왔습니다. 이러한 모델들은 이미지 캡셔닝, 시각적 질문 답변(VQA), 이미지 기반 텍스트 생성 등 다양한 작업을 수행할 수 있습니다.\\n*   **모달리티 간 연결에 특화된 모델 개발:** CLIP(Contrastive Language-Image Pre-training)과 같은 모델은 텍스트와 이미지를 연결하는 데 특화되어 있으며, 이미지 검색, 이미지 분류 등 다양한 분야에서 활용되고 있습니다. 이러한 모델들은 텍스트 설명과 이미지 간의 의미적 유사성을 학습하여 효과적인 연결을 가능하게 합니다.\\n*   **오디오, 비디오 등 다양한 모달리티 통합:** 최근에는 오디오, 비디오 등 다양한 모달리티를 통합하여 더욱 복잡한 작업을 수행할 수 있는 모델들이 등장하고 있습니다. 예를 들어, 텍스트와 오디오를 함께 처리하여 음성 기반 질문 답변, 비디오 캡셔닝, 비디오 기반 텍스트 생성 등을 가능하게 합니다.\\n\\n**주요 멀티모달 LLM 모델 및 특징:**\\n\\n| 모델명 | 주요 특징 | 활용 분야 |\\n|---|---|---|\\n| **GPT-4** | 이미지 입력 가능, 텍스트 기반 추론 및 생성 능력 우수 | 시각적 질문 답변, 이미지 기반 텍스트 생성, 콘텐츠 요약 |\\n| **Gemini** | 텍스트, 이미지, 오디오, 비디오 등 다양한 모달리티 처리 가능, Google의 다양한 서비스 통합 예정 | 검색, 콘텐츠 생성, 자동화된 작업 |\\n| **LLaVA** | 오픈소스, 이미지 기반 질문 답변, 이미지 설명 생성 | 연구 및 개발, 교육 |\\n| **CLIP** | 텍스트와 이미지의 의미적 연결에 특화 | 이미지 검색, 이미지 분류, 콘텐츠 추천 |\\n| **Flamingo** | 이미지와 텍스트를 결합하여 복잡한 추론 가능 | 시각적 질문 답변, 이미지 기반 스토리텔링 |\\n\\n**3. 멀티모달 LLM의 가능성**\\n\\n멀티모달 LLM은 다양한 분야에서 혁신적인 가능성을 제시하고 있습니다.\\n\\n*   **교육:** 개인 맞춤형 학습 콘텐츠 제공, 시각 자료를 활용한 학습 지원, 학생들의 질문에 시각적으로 답변하는 튜터 역할 수행\\n*   **의료:** 의료 영상 분석 및 진단 지원, 환자의 증상에 대한 시각적 설명 제공, 의료 기록 요약 및 질병 예측\\n*   **콘텐츠 제작:** 이미지 기반 스토리텔링, 자동 비디오 캡셔닝, 텍스트 기반 이미지 생성, 음악 작곡 및 가사 생성\\n*   **접근성 향상:** 시각 장애인을 위한 이미지 설명 제공, 청각 장애인을 위한 텍스트 기반 오디오 설명 제공\\n*   **로봇 공학:** 로봇의 시각적 환경 이해 및 상호 작용, 음성 명령과 시각적 정보를 결합한 작업 수행\\n*   **검색:** 텍스트, 이미지, 오디오, 비디오를 통합하여 더욱 정확하고 풍부한 검색 결과 제공\\n*   **고객 서비스:** 텍스트, 이미지, 오디오를 결합하여 더욱 효율적이고 만족스러운 고객 지원 제공\\n\\n**4. 멀티모달 LLM의 도전 과제**\\n\\n멀티모달 LLM의 발전은 많은 가능성을 제시하지만, 해결해야 할 과제도 존재']}}\n",
            "--------------\n",
            "{'llm_call': {'completed_sections': [\"## 3.1 GPT-4, Gemini, Claude 등 최신 LLM의 등장과 경쟁\\n\\n**1. 서론: LLM 발전의 가속화와 새로운 경쟁 구도**\\n\\n최근 몇 년간 자연어 처리(NLP) 분야는 거대한 규모의 언어 모델(Large Language Models, LLMs)의 등장으로 혁신적인 발전을 이루어냈습니다. GPT-3를 기점으로 LLM은 텍스트 생성, 번역, 질의 응답, 코드 생성 등 다양한 작업에서 인간에 가까운 성능을 보여주며, 인공지능 기술의 가능성을 획기적으로 확장했습니다. 이러한 추세에 발맞춰 OpenAI의 GPT-4, Google의 Gemini, Anthropic의 Claude 등 더욱 강력하고 다재다능한 LLM들이 등장하며, LLM 시장의 경쟁 구도는 더욱 치열해지고 있습니다. 본 보고서는 이러한 최신 LLM들의 등장 배경, 주요 특징, 모델 간 성능 비교, 그리고 경쟁 구도에 대해 심층적으로 분석하고자 합니다.\\n\\n**2. 주요 LLM 소개 및 특징**\\n\\n* **GPT-4 (OpenAI):**\\n    * **등장 배경:** GPT-3의 한계를 극복하고, 더욱 복잡한 추론 능력과 창의성을 확보하기 위해 개발되었습니다. OpenAI는 GPT-4의 정확한 규모를 공개하지 않았지만, GPT-3보다 훨씬 많은 데이터와 파라미터를 사용했을 것으로 추정됩니다.\\n    * **주요 특징:**\\n        * **다중 모달(Multimodal) 지원:** 텍스트뿐만 아니라 이미지 입력도 지원하여, 이미지에 대한 질문에 답변하거나 이미지 기반의 작업을 수행할 수 있습니다.\\n        * **향상된 추론 능력:** 복잡한 문제 해결, 논리적 사고, 창의적인 글쓰기 등 다양한 분야에서 GPT-3보다 뛰어난 성능을 보입니다.\\n        * **안전성 및 책임감 강화:** 유해 콘텐츠 생성 가능성을 줄이고, 편향성을 완화하기 위한 노력이 반영되었습니다.\\n        * **API 접근성:** 개발자를 위한 API를 제공하여 다양한 애플리케이션 개발에 활용될 수 있도록 지원합니다.\\n* **Gemini (Google):**\\n    * **등장 배경:** Google은 자체 개발한 TPU(Tensor Processing Unit)를 활용하여 LLM 개발을 가속화하고, GPT-4에 대응하기 위해 Gemini를 출시했습니다.\\n    * **주요 특징:**\\n        * **다양한 크기(Size)의 모델 제공:** Nano, Pro, Ultra 등 다양한 규모의 모델을 제공하여, 다양한 환경과 요구사항에 맞게 활용할 수 있습니다.\\n        * **다중 모달(Multimodal) 지원:** 텍스트, 이미지, 오디오, 비디오 등 다양한 형태의 데이터를 이해하고 처리할 수 있습니다.\\n        * **TPU 최적화:** Google의 TPU를 활용하여 뛰어난 성능과 효율성을 제공합니다.\\n        * **Google 서비스 통합:** Google 검색, Gmail, Google Workspace 등 Google의 다양한 서비스와 통합되어 시너지 효과를 창출합니다.\\n* **Claude (Anthropic):**\\n    * **등장 배경:** OpenAI의 GPT-3를 개발했던 핵심 인력들이 설립한 Anthropic에서 개발한 LLM입니다. 안전하고 윤리적인 AI 개발을 목표로 하며, 'Constitutional AI'라는 독특한 접근 방식을 사용합니다.\\n    * **주요 특징:**\\n        * **Constitutional AI:** 미리 정의된 윤리적 원칙(헌법)에 따라 답변을 생성하여, 유해하거나 편향된 답변을 줄이도록 설계되었습니다.\\n        * **긴 컨텍스트 창(Context Window):** 긴 텍스트를 한 번에 처리할 수 있어, 복잡한 문서 요약, 코드 분석, 장문의 대화 등에 유리합니다.\\n        * **안전성 및 윤리성:** 유해 콘텐츠 생성 가능성을 최소화하고, 편향성을 완화하기 위한 노력을 지속적으로 기울이고 있습니다.\\n\\n**3. 모델 간 성능 비교**\\n\\n각 모델의 성능은 다양한 벤치마크 테스트를 통해 비교 분석될 수 있습니다. 하지만, LLM의 성능은 평가 방식, 데이터셋, 작업의 종류에 따라 달라질 수 있습니다. 현재까지의 평가 결과는 다음과 같습니다.\\n\\n| 특징 | GPT-4 | Gemini (Ultra) | Claude 3 (Opus) |\\n|---|---|---|---|\\n| **추론 능력** | 매우 우수, 복잡한 문제 해결에 강점 | 우수, 특히 수학 및 과학 분야에서 강점 | 매우 우수, 논리적 추론 및 코딩 능력 뛰어남 |\\n| **다중 모달** | 이미지 입력 지원 | 텍스트, 이미지, 오디오, 비디오 지원 | \"]}}\n",
            "--------------\n",
            "{'llm_call': {'completed_sections': [\"## 2.3 Codex, InstructGPT, ChatGPT 등 GPT-3 기반 모델의 등장: 특징, 성능 비교 분석 및 평가\\n\\nGPT-3의 등장 이후, GPT-3를 기반으로 구축된 다양한 모델들이 등장하며 자연어 처리 분야에 혁신적인 변화를 가져왔다. 이 섹션에서는 Codex, InstructGPT, ChatGPT를 중심으로 GPT-3를 기반으로 개발된 주요 모델들의 특징, 성능을 비교 분석하고, 각 모델이 가진 장단점을 평가하여 그 의미를 조명하고자 한다.\\n\\n**1. 모델 소개 및 개발 배경**\\n\\n* **Codex:** OpenAI에서 2021년 공개한 Codex는 GPT-3를 기반으로 프로그래밍 언어에 특화된 모델이다. GPT-3의 방대한 텍스트 데이터 학습에 더해 GitHub의 공개 코드 데이터를 추가적으로 학습하여 자연어 설명을 코드로 변환하는 능력을 극대화했다. 개발 배경은 자연어 기반의 코딩 자동화, 코드 생성, 코드 이해 등의 문제를 해결하고자 하는 OpenAI의 목표를 반영한다.\\n* **InstructGPT:** OpenAI에서 2022년 공개한 InstructGPT는 GPT-3의 성능을 유지하면서도 인간의 지시(instruction)에 더 잘 따르는 것을 목표로 개발되었다. GPT-3의 주요 문제점이었던 '환각(hallucination)' 현상, 즉 사실과 다른 내용을 생성하는 문제를 해결하고, 인간의 의도에 부합하는 답변을 생성하는 데 초점을 맞췄다.\\n* **ChatGPT:** OpenAI에서 2022년 12월 공개한 ChatGPT는 GPT-3.5를 기반으로 구축된 대화형 AI 모델이다. 인간과의 자연스러운 대화를 목표로 개발되었으며, 대화 맥락을 이해하고 유지하며, 질문에 대한 답변, 글쓰기, 번역 등 다양한 작업을 수행할 수 있다. 특히 강화 학습 기반의 인간 피드백(Reinforcement Learning from Human Feedback, RLHF)을 통해 성능을 향상시켰다는 특징을 가진다.\\n\\n**2. 모델 특징 비교**\\n\\n| 특징 | Codex | InstructGPT | ChatGPT |\\n|---|---|---|---|\\n| **기반 모델** | GPT-3 | GPT-3 | GPT-3.5 |\\n| **주요 목적** | 코드 생성 및 이해 | 인간 지시(instruction) 준수 | 자연스러운 대화 |\\n| **학습 데이터** | 텍스트 데이터 + GitHub 공개 코드 데이터 | 텍스트 데이터 + 인간 지시 및 피드백 데이터 | 텍스트 데이터 + 대화 데이터 + RLHF 데이터 |\\n| **강조점** | 코드 생성 정확도 및 효율성 | 답변의 정확성 및 일관성 | 대화의 자연스러움 및 맥락 유지 |\\n| **사용 분야** | 코드 자동 완성, 코드 생성, 코드 이해, 프로그래밍 교육 | 챗봇, 가상 비서, 지시 기반 작업 수행 | 챗봇, 고객 지원, 콘텐츠 생성, 교육 |\\n| **API 제공 여부** | 제공 | 제공 | 제공 |\\n\\n**3. 모델 성능 비교 및 평가**\\n\\n* **Codex:**\\n    * **장점:** 자연어 설명을 코드로 변환하는 능력에서 뛰어난 성능을 보인다. 다양한 프로그래밍 언어를 지원하며, 복잡한 코드도 생성할 수 있다. 코드 자동 완성 기능은 개발 생산성을 크게 향상시킨다.\\n    * **단점:** 생성된 코드의 정확성을 보장하기 어려울 수 있으며, 오류가 발생할 경우 디버깅이 필요하다. 자연어 설명의 모호성으로 인해 예상치 못한 코드가 생성될 수 있다.\\n* **InstructGPT:**\\n    * **장점:** GPT-3에 비해 인간의 지시를 더 잘 따르며, 환각 현상이 줄어들어 답변의 신뢰도가 높아졌다. 답변의 일관성을 유지하며, 인간의 의도에 부합하는 답변을 제공한다.\\n    * **단점:** GPT-3에 비해 창의성이 다소 감소할 수 있으며, 특정 분야에 대한 전문적인 지식은 부족할 수 있다.\\n* **ChatGPT:**\\n    * **장점:** 자연스러운 대화 능력이 뛰어나며, 대화 맥락을 잘 이해하고 유지한다. 다양한 주제에 대한 답변을 제공하며, 글쓰기, 번역 등 다양한 작업을 수행할 수 있다. RLHF를 통해 인간의 선호도에 맞게 답변을 생성한다.\\n    * **단점:** 여전히 환각 현상이 발생할 수 있으며, 답변의 정확성을 보장하기 어려울 수 있다. 윤리적인 문제 (혐오 발언, 편향된 정보 등) 발생 가능성이 존재한다.\\n\\n**4. 각 모델의 차별화 전략 및 미래 전망**\\n\\n* **Codex:** 코드 생성\"]}}\n",
            "--------------\n",
            "{'llm_call': {'completed_sections': [\"## 3.3 LLM의 윤리적 문제와 안전성 확보 방안\\n\\n**1. 서론: LLM의 급격한 발전과 윤리적 책임의 중요성**\\n\\n대규모 언어 모델(LLM, Large Language Model)은 챗봇, 콘텐츠 생성, 번역 등 다양한 분야에서 혁신적인 가능성을 보여주며 빠르게 발전하고 있습니다. 하지만 LLM의 성능이 향상될수록, 그에 따른 윤리적 문제와 안전성에 대한 우려 또한 증폭되고 있습니다. LLM은 사회에 긍정적인 영향을 미칠 수 있지만, 동시에 편향성 심화, 허위 정보 확산, 악의적 사용 등 심각한 문제를 야기할 수 있기 때문입니다. 본 보고서는 LLM이 야기하는 주요 윤리적 문제점을 심층적으로 분석하고, 이러한 문제점을 해결하고 LLM의 안전성을 확보하기 위한 기술적, 정책적 방안을 제시합니다.\\n\\n**2. LLM의 윤리적 문제점**\\n\\nLLM은 방대한 양의 데이터를 기반으로 학습하기 때문에, 데이터에 내재된 편향성을 그대로 반영할 가능성이 높습니다. 이는 다양한 윤리적 문제로 이어질 수 있습니다.\\n\\n* **2.1 편향성 (Bias):**\\n    * **데이터 편향:** LLM 학습에 사용되는 데이터는 사회적, 문화적 맥락에 따라 특정 집단에 대한 편향된 정보를 담고 있을 수 있습니다. 이러한 데이터 편향은 LLM이 특정 성별, 인종, 종교, 정치적 견해 등에 대해 차별적이거나 불공정한 답변을 생성하도록 만들 수 있습니다. 예를 들어, 특정 직업을 언급할 때 남성 위주로 답변을 생성하거나, 특정 인종에 대한 부정적인 고정관념을 강화하는 답변을 생성할 수 있습니다.\\n    * **알고리즘 편향:** LLM의 학습 알고리즘 자체가 특정 결과를 선호하도록 설계될 수 있습니다. 이는 의도치 않게 특정 집단에 불리한 결과를 초래할 수 있습니다.\\n    * **결과 편향:** LLM이 생성하는 결과물은 학습 데이터와 알고리즘의 영향을 받아 편향된 시각을 반영할 수 있습니다.\\n* **2.2 허위 정보 생성 (Hallucination & Fabrication):**\\n    * LLM은 사실과 다른 정보를 마치 사실인 것처럼 제시하는 '환각(Hallucination)' 현상을 보일 수 있습니다. 이는 LLM이 학습 데이터에서 패턴을 학습하는 과정에서 발생하는 문제로, 사실 확인 없이 정보를 생성하는 경향을 나타냅니다.\\n    * LLM은 존재하지 않는 정보를 '조작(Fabrication)'하여 생성할 수도 있습니다. 이는 LLM이 지식의 한계를 인지하지 못하고, 부족한 정보를 채우기 위해 가짜 정보를 생성하는 결과입니다.\\n    * 이러한 허위 정보 생성은 사회적 혼란을 야기하고, 개인의 명예를 훼손하며, 심각한 사회적 문제를 초래할 수 있습니다.\\n* **2.3 악용 가능성 (Potential for Misuse):**\\n    * **사기 및 스미싱:** LLM을 이용하여 정교한 사기 및 스미싱 메시지를 생성하여 개인 정보를 탈취하거나 금전적인 피해를 입힐 수 있습니다.\\n    * **가짜 뉴스 생성 및 유포:** LLM을 이용하여 정치적 선전, 허위 사실 유포 등 가짜 뉴스를 생성하고 유포하여 사회적 혼란을 야기할 수 있습니다.\\n    * **악성 코드 생성:** LLM을 이용하여 악성 코드를 생성하고 유포하여 사이버 범죄를 저지를 수 있습니다.\\n    * **딥페이크 제작:** LLM과 결합하여 딥페이크를 제작하여 개인의 명예를 훼손하거나 사회적 혼란을 야기할 수 있습니다.\\n\\n**3. LLM의 안전성 확보 방안**\\n\\nLLM의 윤리적 문제점을 해결하고 안전성을 확보하기 위해서는 기술적, 정책적 방안을 종합적으로 적용해야 합니다.\\n\\n* **3.1 기술적 방안:**\\n    * **데이터 편향 완화:**\\n        * **데이터 다양성 확보:** 다양한 인구 집단, 문화, 관점을 반영하는 데이터를 수집하여 학습 데이터의 다양성을 확보해야 합니다.\\n        * **데이터 증강 (Data Augmentation):** 기존 데이터에 변형을 가하거나 새로운 데이터를 생성하여 데이터셋의 불균형을 해소해야 합니다.\\n        * **편향 완화 알고리즘 개발:** 학습 데이터의 편향성을 감지하고 완화하는 알고리즘을 개발해야 합니다.\\n    * **허위 정보 생성 방지:**\\n        * **사실 확인 (Fact-Checking) 시스템 통합:** LL\"]}}\n",
            "--------------\n",
            "{'llm_call': {'completed_sections': [\"## 3.4 LLM의 미래 발전 방향: AGI(Artificial General Intelligence)를 향하여\\n\\n**서론**\\n\\n최근 거대한 언어 모델(LLM, Large Language Model)의 발전은 인공지능 분야에 혁명적인 변화를 가져왔습니다. GPT-3, LaMDA, PaLM 등 LLM들은 인간과 유사한 텍스트 생성 능력, 번역, 질의응답 등 다양한 작업을 수행하며 그 가능성을 입증했습니다. 하지만 LLM은 여전히 특정 작업에 특화된 'Narrow AI'의 범주에 속하며, 진정한 의미의 인공지능, 즉 인간과 동등하거나 그 이상의 지적 능력을 갖춘 AGI(Artificial General Intelligence)에는 미치지 못합니다. 본 보고서는 LLM의 미래 발전 방향을 제시하고, AGI를 향한 연구 개발 동향을 전망하며, LLM이 사회에 미칠 영향에 대해 심층적으로 논의합니다.\\n\\n**1. LLM의 한계와 극복 과제**\\n\\n현재 LLM은 다음과 같은 근본적인 한계를 가지고 있습니다.\\n\\n* **이해력 부족:** LLM은 텍스트 패턴을 학습하여 텍스트를 생성하지만, 텍스트의 의미를 진정으로 이해한다고 보기 어렵습니다. 이는 LLM이 맥락을 파악하지 못하거나, 상식적인 추론을 수행하는 데 어려움을 겪는 이유입니다.\\n* **환각(Hallucination) 현상:** LLM은 사실과 다른 정보를 마치 사실인 것처럼 생성하는 '환각' 현상을 보입니다. 이는 학습 데이터의 편향성, 불충분한 지식, 추론 능력 부족 등 다양한 원인에 기인합니다.\\n* **상식과 추론 능력 부족:** LLM은 인간의 상식적인 지식이나 추론 능력을 갖추지 못하여 복잡한 문제 해결에 어려움을 겪습니다.\\n* **데이터 의존성:** LLM은 방대한 양의 데이터를 필요로 하며, 학습 데이터에 편향될 가능성이 높습니다.\\n* **계산 비용:** LLM의 학습 및 운영에는 막대한 계산 비용이 소요됩니다.\\n\\n이러한 한계를 극복하기 위해 다음과 같은 연구가 진행되고 있습니다.\\n\\n* **지식 주입(Knowledge Injection):** 외부 지식 베이스를 LLM에 통합하여 지식의 정확성과 폭을 넓히는 연구입니다.\\n* **강화 학습(Reinforcement Learning):** 인간의 피드백을 통해 LLM의 성능을 개선하는 연구입니다.\\n* **멀티모달 학습(Multimodal Learning):** 텍스트뿐만 아니라 이미지, 오디오, 비디오 등 다양한 형태의 데이터를 함께 학습하여 LLM의 이해력을 높이는 연구입니다.\\n* **추론 능력 강화:** 논리적 추론, 상식 추론 등 LLM의 추론 능력을 향상시키기 위한 연구입니다.\\n* **설명 가능한 AI(Explainable AI, XAI):** LLM의 의사 결정 과정을 설명 가능하게 만들어 신뢰도를 높이는 연구입니다.\\n\\n**2. AGI를 향한 연구 개발 동향**\\n\\nAGI는 LLM의 발전 방향을 넘어, 인공지능 연구의 궁극적인 목표입니다. AGI를 향한 연구는 다음과 같은 방향으로 진행되고 있습니다.\\n\\n* **신경망 아키텍처 혁신:** Transformer 기반의 LLM 외에도 새로운 신경망 아키텍처를 개발하여 LLM의 한계를 극복하려는 시도가 이루어지고 있습니다.\\n* **자기 지도 학습(Self-Supervised Learning) 발전:** 레이블이 없는 데이터로부터 스스로 학습하는 자기 지도 학습의 효율성을 높이는 연구가 진행되고 있습니다.\\n* **인공 일반 지능(Artificial General Intelligence, AGI) 프레임워크 개발:** AGI의 핵심 구성 요소를 정의하고, 이를 구현하기 위한 프레임워크를 개발하는 연구가 진행되고 있습니다.\\n* **신경-상징적 AI(Neuro-Symbolic AI):** 신경망의 패턴 인식 능력과 상징적 추론의 논리적 추론 능력을 결합하여 AGI를 구현하려는 연구입니다.\\n* **강화 학습의 발전:** 복잡한 환경에서 스스로 학습하고 문제를 해결하는 강화 학습 알고리즘의 성능을 향상시키는 연구입니다.\\n\\n**3. LLM이 사회에 미칠 영향**\\n\\nLLM의 발전은 사회 전반에 걸쳐 광범위한 영향을 미칠 것으로 예상됩니다.\\n\\n* **긍정적인 영향:**\\n    * **업무 효율성 증대:** LLM은 문서 작성, 번역, 고객 응대 등 다양한 업무를 자동화하여 업무 효율성을 증대시킬 수 있습니다.\\n    * **새로운 산업\"]}}\n",
            "--------------\n",
            "{'synthesizer': {'final_report': '## 1장: GPT-1의 등장과 초기 발전 (2018-2019) - 거대 언어 모델의 서막과 가능성과 한계\\n\\n**1.1. 등장 배경: 언어 모델 발전의 흐름과 GPT-1의 필요성**\\n\\n2018년, OpenAI는 GPT-1 (Generative Pre-trained Transformer)을 공개하며 자연어 처리 (NLP) 분야에 큰 파장을 일으켰습니다. 그 이전까지 NLP 분야는 특정 작업 (Task-Specific)에 특화된 모델을 구축하는 데 집중되어 있었습니다. 예를 들어, 감성 분석, 기계 번역, 질의 응답 등 각 작업에 최적화된 모델을 별도로 학습시키는 방식이 일반적이었습니다. 이러한 방식은 각 작업마다 많은 양의 레이블링된 데이터를 필요로 했으며, 새로운 작업에 적용하기 위해서는 추가적인 학습 과정이 필요했습니다.\\n\\nGPT-1의 등장은 이러한 문제점을 해결하고자 하는 흐름의 결과였습니다. OpenAI는 **사전 학습 (Pre-training)**이라는 새로운 패러다임을 도입했습니다. 이는 방대한 텍스트 데이터 (Wikipedia, 웹 크롤링 데이터 등)를 사용하여 일반적인 언어 이해 능력을 먼저 학습시키고, 이후 특정 작업에 미세 조정 (Fine-tuning)하는 방식입니다. 즉, 다양한 작업을 수행하기 위한 기반 지식을 먼저 쌓은 후, 특정 작업에 필요한 지식을 추가하는 것입니다.\\n\\nGPT-1은 이러한 사전 학습 패러다임을 Transformer 아키텍처에 적용하여 구현되었습니다. Transformer는 Google에서 2017년에 발표한 \"Attention is All You Need\" 논문에서 소개된 새로운 신경망 아키텍처로, 기존의 순환 신경망 (RNN) 기반 모델의 병목 현상을 해결하고 병렬 처리를 가능하게 하여 학습 속도를 획기적으로 향상시켰습니다.\\n\\n**1.2. 아키텍처 특징: Transformer 기반의 언어 모델**\\n\\nGPT-1은 Transformer 아키텍처를 기반으로 구축되었으며, 다음과 같은 특징을 가집니다.\\n\\n* **Transformer Decoder:** GPT-1은 Transformer의 Decoder 부분만을 사용합니다. Decoder는 주어진 문맥을 바탕으로 다음 단어를 예측하는 역할을 수행합니다.\\n* **Self-Attention 메커니즘:** Transformer의 핵심 구성 요소인 Self-Attention 메커니즘은 문장 내의 단어 간의 관계를 파악하여 문맥을 이해하는 데 중요한 역할을 합니다. GPT-1은 Self-Attention 메커니즘을 통해 문장 내의 단어 간의 의존성을 효과적으로 학습했습니다.\\n* **레이어 수 및 파라미터 크기:** GPT-1은 다양한 크기로 구축되었으며, 가장 큰 모델은 11억 7천만 개의 파라미터를 가지고 있었습니다. 이는 당시로서는 매우 큰 규모의 파라미터 수였습니다.\\n* **Masked Self-Attention:** GPT-1은 미래의 단어를 참조하지 않고 과거의 단어만을 참조하도록 Masked Self-Attention 메커니즘을 사용했습니다. 이는 언어 모델링의 기본적인 가정인 \"다음 단어 예측\"을 가능하게 합니다.\\n\\n**1.3. 초기 성능 및 한계: 놀라운 생성 능력과 현실적인 제약**\\n\\nGPT-1은 사전 학습 후 다양한 NLP 작업에 미세 조정되었을 때 상당한 성능 향상을 보였습니다. 특히, 텍스트 생성 능력은 주목할 만했습니다. GPT-1은 주어진 문맥에 따라 자연스럽고 일관성 있는 텍스트를 생성할 수 있었으며, 때로는 인간이 작성한 것과 구별하기 어려울 정도의 텍스트를 생성하기도 했습니다.\\n\\n하지만 GPT-1은 다음과 같은 한계점 또한 가지고 있었습니다.\\n\\n* **문맥 이해 부족:** GPT-1은 문맥을 완벽하게 이해하지 못하고, 때로는 앞뒤가 맞지 않거나 비논리적인 텍스트를 생성하기도 했습니다. 특히, 장문의 텍스트를 생성할 때 이러한 문제가 더욱 두드러졌습니다.\\n* **사실 기반 지식 부족:** GPT-1은 방대한 텍스트 데이터를 학습했지만, 사실 기반 지식은 부족했습니다. 따라서, 사실에 기반한 질문에 정확하게 답변하지 못하거나, 잘못된 정보를 제공하기도 했습니다.\\n* **편향성 문제:** GPT-1은 학습 데이터에 존재하는 편향성을 그대로 반영했습니다. 따라서, 성별, 인종, 종교 등에 대한 편향적인 텍스트를 생성할 가능성이 있었습니다.\\n* **계산 비용:** GPT-1과 같은 거대 언어 모델을 학습하고 실행하는 데는 막대한 계산 비용이 소요되었습니다. 이는 GPT-1의 보급과 활용에 제약 요인으로 작용했습니다.\\n\\n**1.4. GPT\\n\\n---\\n\\n## 1.1 GPT-1의 탄생과 Transformer 아키텍처: 자연어 처리의 새로운 지평을 열다\\n\\n본 섹션에서는 OpenAI에 의해 개발된 GPT-1의 탄생 배경과 그 핵심 기술인 Transformer 아키텍처에 대해 상세히 설명합니다. GPT-1은 자연어 처리 (NLP) 분야에 혁명적인 변화를 가져왔으며, 특히 텍스트 생성 분야에서 괄목할 만한 성과를 보여주었습니다.\\n\\n**1. GPT-1 개발 배경 및 OpenAI의 목표**\\n\\nOpenAI는 인공지능 (AI)의 안전하고 긍정적인 발전을 목표로 하는 연구 기관입니다. GPT-1 개발 당시 OpenAI는 기존의 NLP 모델들이 특정 작업에 맞춰 설계되어 범용적인 문제 해결 능력은 부족하다는 점을 인지했습니다.  기존 모델들은 주로 지도 학습 (Supervised Learning) 방식으로, 특정 작업에 대한 레이블링된 데이터셋을 필요로 했습니다.  이는 데이터 수집 및 레이블링에 상당한 비용과 노력이 소요되었으며, 새로운 작업에 대한 적응성이 떨어지는 단점을 가지고 있었습니다.\\n\\nOpenAI는 이러한 문제점을 해결하고, **“제로샷 (Zero-shot) 학습”** 능력을 갖춘 모델을 개발하고자 했습니다. 제로샷 학습이란, 특정 작업에 대한 학습 데이터 없이도 해당 작업을 수행할 수 있는 능력을 의미합니다.  즉, 다양한 작업을 수행할 수 있는 범용적인 언어 모델을 구축하는 것을 목표로 했습니다.  GPT-1은 이러한 목표를 달성하기 위한 첫 번째 단계였으며, 방대한 텍스트 데이터셋을 통해 언어의 패턴을 학습하고, 이를 기반으로 다양한 텍스트 생성 작업을 수행할 수 있도록 설계되었습니다.\\n\\n**2. Transformer 아키텍처의 핵심 개념**\\n\\nGPT-1의 핵심 기술은 Google에서 2017년에 발표한 **Transformer 아키텍처**입니다.  Transformer는 기존의 순환 신경망 (RNN) 기반 모델들이 가지고 있던 문제점을 해결하고, 병렬 처리를 통해 학습 속도를 획기적으로 향상시킨 혁신적인 구조입니다.  Transformer의 핵심 개념은 다음과 같습니다.\\n\\n* **Attention 메커니즘:**  Transformer의 가장 중요한 특징은 Attention 메커니즘입니다.  Attention은 입력 시퀀스의 각 단어가 다른 단어와 얼마나 관련이 있는지 계산하여, 중요한 단어에 집중하도록 합니다.  이는 문맥을 파악하고, 장거리 의존성을 효과적으로 처리하는 데 도움이 됩니다.  기존 RNN 기반 모델들은 시퀀스의 앞부분에 있는 정보가 뒷부분으로 전달될수록 희석되는 문제 (Vanishing Gradient Problem)를 가지고 있었지만, Attention 메커니즘은 이러한 문제를 완화합니다.\\n* **Self-Attention:**  Transformer는 입력 시퀀스 내의 단어 간의 관계를 파악하기 위해 Self-Attention을 사용합니다.  Self-Attention은 입력 시퀀스의 각 단어를 다른 모든 단어와 비교하여 관련성을 평가하고, 이를 기반으로 각 단어의 표현을 업데이트합니다.\\n* **Encoder-Decoder 구조:**  Transformer는 Encoder와 Decoder라는 두 개의 모듈로 구성됩니다.  Encoder는 입력 시퀀스를 처리하여 문맥 정보를 담고 있는 표현을 생성하고, Decoder는 이 표현을 기반으로 출력 시퀀스를 생성합니다.  GPT-1은 Decoder만 사용한 구조입니다.\\n* **Positional Encoding:**  Transformer는 순서 정보를 처리하지 않기 때문에, Positional Encoding을 사용하여 단어의 위치 정보를 모델에 제공합니다.  Positional Encoding은 각 단어의 위치에 따라 다른 값을 부여하여, 모델이 단어의 순서를 인식할 수 있도록 합니다.\\n* **Multi-Head Attention:**  Transformer는 여러 개의 Attention 헤드를 사용하여 다양한 관점에서 단어 간의 관계를 파악합니다.  각 헤드는 서로 다른 방식으로 Attention을 계산하며, 이를 통해 모델은 더욱 풍부한 문맥 정보를 학습할 수 있습니다.\\n\\n**3. GPT-1에 적용된 Transformer 아키텍처**\\n\\nGPT-1은 Transformer 아키텍처의 **Decoder 부분만 사용**한 구조입니다.  이는 GPT-1의 목표가 특정 작업에 대한 학습 데이터 없이도 텍스트를 생성하는 것이었기 때문입니다.  GPT-1은 다음과 같은 특징을 가집니다.\\n\\n* **모델 크기:** GPT-1은 다양한 크기로 구축되었으며, 가장 큰 모델은 11억 7천만 개의 파라미터를 가지고 있습니다.\\n* **학습 데이터:** GPT-1은 Common Crawl 데이터셋을 포함한 방대한 텍스트 데이터셋을 사용하여 학습되었습니다.  Common Crawl은 웹 페이지의 텍스트 데이터를 수집한 데이터셋으로, GPT-1은 이 데이터셋을 통해\\n\\n---\\n\\n## 1.2 GPT-1의 성능 및 한계점: 초기 성능 평가와 텍스트 생성의 한계 분석\\n\\nGPT-1 (Generative Pre-trained Transformer 1)은 OpenAI에서 2018년에 공개한 초기 대규모 언어 모델로, 딥러닝 기반의 자연어 처리 (NLP) 분야에 큰 영향을 미쳤습니다. GPT-1은 방대한 양의 텍스트 데이터 (웹 크롤링 데이터)를 사용하여 사전 훈련되었으며, 주어진 프롬프트 (prompt)에 기반하여 텍스트를 생성하는 능력을 보유했습니다. 본 보고서는 GPT-1의 초기 성능을 평가하고, 텍스트 생성의 품질, 일관성, 창의성 측면에서 나타난 주요 한계점을 상세히 분석합니다.\\n\\n**1. GPT-1의 초기 성능 평가**\\n\\nGPT-1은 공개 당시, 여러 NLP 작업에서 상당한 성능을 보였습니다. 특히, 다음과 같은 측면에서 주목할 만한 성과를 거두었습니다.\\n\\n* **문맥 이해 능력:** GPT-1은 비교적 긴 문맥을 이해하고 이를 기반으로 텍스트를 생성할 수 있었습니다. 이는 이전 모델에 비해 괄목할 만한 발전이었으며, 문맥에 따른 의미 변화를 어느 정도 파악할 수 있음을 시사했습니다.\\n* **다양한 스타일의 텍스트 생성:** GPT-1은 다양한 스타일의 텍스트를 생성할 수 있었습니다. 예를 들어, 뉴스 기사, 소설, 시 등 다양한 장르의 텍스트를 유사하게 모방하여 생성할 수 있었습니다. 이는 텍스트 생성 모델의 범용성을 입증하는 중요한 요소였습니다.\\n* **제로-샷 학습 (Zero-Shot Learning) 능력:** GPT-1은 특정 작업에 대한 명시적인 훈련 없이도, 주어진 프롬프트만으로 어느 정도의 작업을 수행할 수 있는 제로-샷 학습 능력을 보여주었습니다. 이는 모델이 일반적인 언어 패턴을 학습하여 새로운 작업에 적용할 수 있음을 의미했습니다.\\n\\n**2. 텍스트 생성의 품질 측면에서 나타난 한계점**\\n\\nGPT-1은 초기 모델로서, 텍스트 생성의 품질 측면에서 다음과 같은 한계점을 드러냈습니다.\\n\\n* **반복적인 텍스트 생성:** GPT-1은 텍스트 생성 과정에서 특정 단어나 구문을 반복적으로 사용하는 경향이 있었습니다. 이는 모델이 텍스트의 다양성을 확보하는 데 어려움을 겪고 있음을 시사합니다. 특히, 긴 텍스트를 생성할 때 이러한 문제가 더욱 두드러졌습니다.\\n* **의미 불명확성 및 비논리성:** GPT-1이 생성하는 텍스트는 때때로 의미가 불명확하거나 논리적으로 모순되는 경우가 있었습니다. 이는 모델이 텍스트의 의미를 정확하게 이해하고, 이를 바탕으로 일관성 있는 텍스트를 생성하는 데 한계가 있음을 보여줍니다.\\n* **사실 기반 오류 (Factuality Errors):** GPT-1은 훈련 데이터에 포함된 잘못된 정보를 그대로 반영하여 사실과 다른 텍스트를 생성하는 경우가 있었습니다. 이는 모델이 텍스트의 진실성을 검증하는 능력이 부족함을 의미하며, 생성된 텍스트의 신뢰성을 저해하는 요인이었습니다.\\n* **맥락 파악의 불완전성:** GPT-1은 문맥을 어느 정도 이해할 수 있었지만, 복잡하고 미묘한 맥락을 정확하게 파악하는 데 어려움을 겪었습니다. 특히, 문맥에 따라 의미가 달라지는 단어나 구문을 잘못 해석하여 부적절한 텍스트를 생성하는 경우가 있었습니다.\\n\\n**3. 텍스트 생성의 일관성 측면에서 나타난 한계점**\\n\\nGPT-1은 텍스트 생성의 일관성 측면에서도 다음과 같은 한계점을 보였습니다.\\n\\n* **주제 일관성 유지의 어려움:** GPT-1은 긴 텍스트를 생성할 때 주제 일관성을 유지하는 데 어려움을 겪었습니다. 텍스트의 초반부에는 특정 주제에 대해 논의했지만, 후반부에는 주제가 벗어나거나 관련 없는 내용을 담는 경우가 있었습니다.\\n* **캐릭터 일관성 유지의 어려움 (스토리텔링):** GPT-1을 사용하여 스토리를 생성할 때, 캐릭터의 성격이나 행동 패턴을 일관성 있게 유지하는 데 어려움을 겪었습니다. 캐릭터의 성격이 갑작스럽게 변하거나, 상황에 맞지 않는 행동을 하는 경우가 있었습니다.\\n* **문체 일관성 유지의 어려움:** GPT-1은 다양한 스타일의 텍스트를 생성할 수 있었지만, 생성된 텍스트 전체에 걸쳐\\n\\n---\\n\\n## 1.3 GPT-2로의 진화: 규모 확장과 새로운 기능\\n\\n본 섹션에서는 OpenAI에 의해 개발된 GPT-2의 등장 배경과 GPT-1 대비 괄목할 만한 규모 확장, 그리고 혁신적인 기능인 \\'zero-shot learning\\' 도입의 의미를 분석하고, GPT-2의 성능 향상을 심층적으로 살펴봅니다. GPT-2는 자연어 처리 (NLP) 분야에 큰 파장을 일으켰으며, 텍스트 생성 모델의 발전 방향을 제시하는 중요한 이정표가 되었습니다.\\n\\n**1. GPT-2 등장 배경: GPT-1의 한계와 OpenAI의 목표**\\n\\nGPT-2의 등장은 GPT-1의 성공적인 개발과 그 한계를 극복하고자 하는 OpenAI의 노력에서 비롯되었습니다. GPT-1은 트랜스포머(Transformer) 아키텍처를 기반으로 대규모 텍스트 데이터셋을 학습하여 인간과 유사한 텍스트를 생성하는 능력을 보여주었지만, 몇 가지 중요한 한계를 드러냈습니다.\\n\\n* **제한적인 생성 능력:** GPT-1은 짧은 텍스트 생성에는 능숙했지만, 긴 텍스트를 일관성 있게 생성하는 데 어려움을 겪었습니다. 맥락을 유지하고 논리적인 흐름을 이어가는 데 한계가 있었습니다.\\n* **특정 작업에 대한 의존성:** GPT-1은 특정 작업에 맞게 미세 조정(fine-tuning)을 거쳐야 좋은 성능을 보였습니다. 즉, 다양한 작업에 유연하게 대응하기 어려웠습니다.\\n* **윤리적 우려:** GPT-1이 생성하는 텍스트는 때때로 편향되거나 유해한 내용을 포함할 수 있다는 우려가 제기되었습니다.\\n\\n이러한 한계를 극복하고, 더욱 강력하고 유연하며 윤리적인 텍스트 생성 모델을 개발하고자 OpenAI는 GPT-2를 개발하게 된 것입니다.\\n\\n**2. GPT-1 대비 규모 확장: 파라미터 수 증가와 데이터셋 변화**\\n\\nGPT-2의 가장 큰 특징은 GPT-1 대비 규모가 대폭 확장되었다는 점입니다. GPT-1은 11억 7천만 개의 파라미터를 가지고 있었지만, GPT-2는 15억 개, 34억 개, 77억 개 등 다양한 규모의 모델을 제공했습니다. 특히 77억 개 파라미터를 가진 모델은 GPT-1의 규모를 훨씬 뛰어넘는 수준입니다.\\n\\n규모 확장은 단순히 파라미터 수 증가에 그치지 않고, 학습 데이터셋의 변화를 동반했습니다. GPT-2는 웹 크롤링을 통해 수집된 40GB의 텍스트 데이터셋을 사용했습니다. 이 데이터셋은 GPT-1에 사용된 데이터셋보다 훨씬 다양하고 방대한 내용을 담고 있어, GPT-2가 더 넓은 범위의 지식을 학습하고 다양한 스타일의 텍스트를 생성할 수 있도록 했습니다.\\n\\n* **데이터셋 구성:** GPT-2 학습 데이터셋은 웹에서 수집된 텍스트 데이터로, Reddit 웹사이트의 댓글과 링크를 통해 수집된 데이터를 포함합니다. 이러한 데이터는 다양한 주제, 스타일, 어조를 포함하고 있어 GPT-2의 일반적인 텍스트 생성 능력을 향상시키는 데 기여했습니다.\\n\\n**3. 새로운 기능: Zero-Shot Learning의 도입**\\n\\nGPT-2의 가장 중요한 혁신은 \\'zero-shot learning\\' 기능의 도입입니다. Zero-shot learning은 특정 작업에 대한 학습 데이터 없이도 해당 작업을 수행할 수 있는 능력을 의미합니다. GPT-2는 대규모 텍스트 데이터셋을 통해 일반적인 언어 이해 능력을 학습하고, 이를 바탕으로 다양한 작업을 수행할 수 있습니다.\\n\\n* **Zero-Shot Learning의 원리:** GPT-2는 텍스트 생성 모델로서, 주어진 프롬프트(prompt)에 따라 텍스트를 생성합니다. 사용자는 원하는 작업에 대한 설명을 프롬프트에 포함시키면, GPT-2는 해당 작업에 대한 학습 데이터 없이도 텍스트를 생성하여 작업을 수행합니다. 예를 들어, \"다음 문장을 프랑스어로 번역하세요:\" 라는 프롬프트를 제공하면, GPT-2는 해당 문장을 프랑스어로 번역한 텍스트를 생성합니다.\\n* **Zero-Shot Learning의 의미:** Zero-Shot Learning 기능은 GPT-2를 더욱 유연하고 활용 가능하게 만들었습니다. 특정 작업에 대한 미세 조정 없이도 다양한 작업을 수행할 수 있어, 다양한 분야에서 활용될 수 있는 가능성을 열었습니다.\\n\\n**4. GPT-2의 성능 향상: 텍스트 생성 품질 및 다양성**\\n\\nGPT-2는 규모 확장과 Zero-Shot Learning 기능 도입을 통해 GPT-1\\n\\n---\\n\\n## 2장: GPT-3와 Beyond: 규모의 확장과 새로운 패러다임 (2020-2022)\\n\\n**1. GPT-3의 등장과 1750억 개의 파라미터 규모 확장: 게임 체인저의 탄생**\\n\\n2020년, OpenAI에서 발표한 GPT-3는 자연어 처리(NLP) 분야에 혁명적인 변화를 가져왔다. 기존 모델들과 비교했을 때 압도적인 규모, 즉 1750억 개의 파라미터를 가진 거대한 규모는 단순한 숫자를 넘어, 모델의 성능과 가능성을 비약적으로 끌어올리는 핵심 요인이었다.\\n\\n* **파라미터 규모의 의미:** 파라미터는 신경망의 연결 강도를 나타내는 값으로, 모델이 학습한 지식을 저장하는 역할을 한다. 파라미터 규모가 클수록 모델은 더 많은 정보를 기억하고 복잡한 패턴을 학습할 수 있으며, 이는 결과적으로 더 자연스럽고 유창한 텍스트 생성 능력으로 이어진다.\\n* **GPT-3의 학습 데이터:** GPT-3는 인터넷 상의 방대한 텍스트 데이터(Common Crawl, WebText2, Books1&2, Wikipedia)를 사용하여 학습되었다. 이러한 데이터의 다양성은 GPT-3가 다양한 주제와 스타일에 대한 이해도를 높이는 데 기여했다.\\n* **기존 모델과의 비교:** GPT-2 (15억 파라미터)와 비교했을 때, GPT-3는 파라미터 규모가 10배 이상 증가했으며, 이는 성능 향상으로 이어졌다. GPT-3는 이전 모델들이 어려움을 겪었던 복잡한 추론, 창의적인 글쓰기, 코딩 등 다양한 작업에서 뛰어난 성능을 보였다.\\n\\n**2. Few-shot Learning 능력 향상: 지시사항 이해의 진화**\\n\\nGPT-3의 가장 중요한 특징 중 하나는 Few-shot Learning 능력이다. Few-shot Learning은 모델에게 몇 개의 예시(few shots)만 제공하면 새로운 작업에 대한 이해도를 빠르게 높이고, 유사한 방식으로 작업을 수행할 수 있도록 하는 학습 방식이다.\\n\\n* **Zero-shot, One-shot, Few-shot 학습:**\\n    * **Zero-shot Learning:** 모델에게 어떤 예시도 제공하지 않고, 단순히 지시사항만으로 작업을 수행하도록 한다.\\n    * **One-shot Learning:** 모델에게 하나의 예시만 제공하고, 이를 바탕으로 작업을 수행하도록 한다.\\n    * **Few-shot Learning:** 모델에게 몇 개의 예시를 제공하고, 이를 바탕으로 작업을 수행하도록 한다.\\n* **GPT-3의 Few-shot Learning 능력:** GPT-3는 Few-shot Learning 능력이 뛰어나, 몇 개의 예시만으로도 다양한 작업(번역, 질문 답변, 요약, 코드 생성 등)을 수행할 수 있었다. 이는 모델이 텍스트의 패턴을 이해하고, 이를 일반화하여 새로운 상황에 적용할 수 있는 능력을 보여준다.\\n* **Few-shot Learning의 의미:** Few-shot Learning 능력은 모델의 학습 효율성을 높이고, 특정 작업에 대한 데이터 부족 문제를 해결하는 데 기여한다. 또한, 사용자가 모델에게 작업을 지시하는 방식을 더욱 직관적으로 만들 수 있다.\\n\\n**3. GPT-3의 다양한 활용 사례: 잠재력의 무한한 확장**\\n\\nGPT-3는 Few-shot Learning 능력을 바탕으로 다양한 분야에서 활용 가능성을 보여주었다.\\n\\n* **콘텐츠 생성:** 블로그 게시물, 소설, 시, 광고 문구 등 다양한 종류의 콘텐츠를 생성할 수 있다.\\n* **챗봇:** 자연스러운 대화가 가능한 챗봇을 구축하여 고객 서비스, 교육, 엔터테인먼트 등 다양한 분야에 적용할 수 있다.\\n* **코드 생성:** 자연어 설명을 기반으로 코드를 생성하여 개발 생산성을 향상시킬 수 있다.\\n* **번역:** 다양한 언어 간의 번역을 수행할 수 있다.\\n* **질문 답변:** 복잡한 질문에 대한 답변을 제공할 수 있다.\\n* **요약:** 긴 텍스트를 요약하여 핵심 내용을 전달할 수 있다.\\n* **교육:** 학생들에게 맞춤형 학습 자료를 제공하고, 질문에 답변하는 등 교육적인 목적으로 활용할 수 있다.\\n\\n**4. GPT-3 이후 등장한 모델들의 특징: 새로운 패러다임의 등장**\\n\\nGPT-3의 성공 이후, 더 크고 강력한 모델들이 등장하며 NLP 분야는 더욱 빠르게 발전했다.\\n\\n* **PaLM (Google):** 5400억 개의 파라미터를 가진 Google의 PaLM은 GPT-3보다 더 큰 규모를 자랑하며, 복잡한 추론\\n\\n---\\n\\n## 2.1 GPT-3의 등장: 규모의 혁신과 Few-shot Learning\\n\\n본 섹션에서는 OpenAI에 의해 개발된 GPT-3 (Generative Pre-trained Transformer 3)의 등장 배경과 그 의미, 특히 파라미터 규모 확장에 따른 Few-shot Learning 능력의 발전과 그 효과를 심층적으로 분석하고자 한다. GPT-3는 자연어 처리 (NLP) 분야에 지대한 영향을 미쳤으며, 기존 모델들과 비교하여 괄목할 만한 성능 향상을 보여주었다.\\n\\n**1. GPT-3 개발 배경: Transformer 아키텍처와 사전학습의 진화**\\n\\nGPT-3의 등장은 자연스럽게 Transformer 아키텍처의 발전과 긴밀하게 연결되어 있다. 2017년 Google에서 발표한 Transformer는 기존 RNN (Recurrent Neural Network) 기반 모델의 단점을 극복하고, 병렬 처리를 통해 학습 속도를 획기적으로 향상시켰다.  Transformer는 Attention 메커니즘을 핵심으로 사용하여 문장 내 단어들 간의 관계를 효과적으로 파악하고, 문맥을 이해하는 능력을 크게 향상시켰다.\\n\\nGPT (Generative Pre-trained Transformer)는 이러한 Transformer 아키텍처를 기반으로, 방대한 텍스트 데이터셋을 활용하여 사전학습 (Pre-training)을 진행하는 모델이다. 사전학습 단계에서 GPT는 주어진 문맥을 기반으로 다음 단어를 예측하는 방식으로 언어의 통계적 패턴을 학습한다.  이후, 특정 작업 (Task)에 맞게 미세 조정 (Fine-tuning)을 통해 성능을 최적화한다.\\n\\nGPT-1, GPT-2를 거치면서 사전학습 데이터의 규모와 모델의 크기는 점진적으로 증가해왔다. GPT-1은 117M 파라미터를 사용했고, GPT-2는 1.5B 파라미터를 사용했다. 하지만 GPT-3는 이러한 추세를 넘어선 혁신적인 규모를 자랑한다.\\n\\n**2. 1750억 개의 파라미터 규모 확장의 의미: 언어 이해 능력의 비약적인 발전**\\n\\nGPT-3는 **1750억 개의 파라미터**를 갖는 거대한 규모의 언어 모델이다. 이는 GPT-2보다 약 100배 더 큰 규모이며, 기존 자연어 처리 모델들과 비교하여 압도적인 크기이다.  이러한 파라미터 규모의 증가는 GPT-3의 언어 이해 능력에 비약적인 발전을 가져왔다.\\n\\n* **더욱 풍부한 지식 습득:** 1750억 개의 파라미터는 방대한 양의 텍스트 데이터를 통해 더 많은 지식을 습득하고, 다양한 문맥과 어휘를 이해할 수 있게 한다.\\n* **미묘한 뉘앙스 이해:**  더욱 복잡한 패턴을 학습함으로써 문장의 미묘한 뉘앙스, 비유, 은유 등을 이해하는 능력이 향상되었다.\\n* **일반화 능력 향상:** 특정 작업에 대한 미세 조정 없이도 다양한 작업을 수행할 수 있는 일반화 능력 (Generalization)이 크게 향상되었다.\\n\\n**3. Few-shot Learning 능력의 발전과 그 효과: 제로샷, 원샷, 그리고 Few-shot**\\n\\nGPT-3의 가장 중요한 특징 중 하나는 **Few-shot Learning** 능력이다. Few-shot Learning은 모델이 극소량의 예시 (Example)만으로 새로운 작업을 수행할 수 있는 능력을 의미한다. GPT-3는 Few-shot Learning을 통해 기존 모델들이 필요로 했던 많은 양의 레이블링된 데이터 (Labeled Data) 없이도 다양한 작업을 수행할 수 있게 되었다.\\n\\nFew-shot Learning은 크게 다음과 같은 세 가지 단계로 나눌 수 있다:\\n\\n* **제로샷 (Zero-shot):**  예시 없이 바로 작업을 수행한다. GPT-3는 제로샷에서도 어느 정도의 성능을 보이지만, 기대만큼의 결과는 나오지 않는다.\\n* **원샷 (One-shot):**  하나의 예시만 제공하여 작업을 수행한다.  GPT-3는 원샷에서도 상당한 성능 향상을 보인다.\\n* **Few-shot:**  몇 개의 예시 (일반적으로 3~5개)를 제공하여 작업을 수행한다. GPT-3는 Few-shot 환경에서 가장 뛰어난 성능을 발휘하며, 인간 수준의 성능에 근접하는 결과를 보여준다.\\n\\nGPT-3의 Few-shot Learning 능력은 다음과 같은 효과를 가져왔다:\\n\\n* **데이터 부족 문제 해결:** 레이블링된 데이터가 부족한 작업에서도 효과적으로 수행할 수 있게 되었다.\\n* **빠른 적응력:** 새로운\\n\\n---\\n\\n## 2.2 GPT-3의 활용 사례 및 사회적 영향\\n\\n**서론:**\\n\\nGPT-3 (Generative Pre-trained Transformer 3)는 OpenAI에서 개발한 거대 언어 모델로, 방대한 텍스트 데이터를 학습하여 인간과 유사한 텍스트를 생성하는 능력을 보유하고 있습니다. GPT-3의 등장으로 다양한 분야에서 혁신적인 서비스 및 애플리케이션 개발이 가능해졌으며, 이는 사회 전반에 걸쳐 긍정적 및 부정적 영향을 미치고 있습니다. 본 보고서는 GPT-3를 활용한 다양한 서비스 및 애플리케이션 사례를 소개하고, GPT-3가 사회에 미치는 긍정적/부정적 영향을 분석하여, GPT-3 기술의 발전과 함께 고려해야 할 윤리적, 사회적 과제를 제시합니다.\\n\\n**1. GPT-3를 활용한 다양한 서비스 및 애플리케이션 사례**\\n\\nGPT-3는 텍스트 생성 능력 외에도 다양한 기능을 통해 여러 분야에서 활용되고 있습니다. 주요 활용 사례는 다음과 같습니다.\\n\\n* **콘텐츠 생성:**\\n    * **블로그 게시물, 기사 작성:** 특정 주제에 대한 블로그 게시물이나 기사를 자동으로 생성하여 콘텐츠 제작 시간을 단축하고 비용을 절감합니다.\\n    * **마케팅 카피 작성:** 광고 문구, 제품 설명, 이메일 마케팅 콘텐츠 등 다양한 마케팅 카피를 생성하여 마케팅 효율성을 향상시킵니다.\\n    * **시나리오, 소설, 시 등 창작:** GPT-3는 창작 활동에도 활용될 수 있으며, 아이디어 발상, 초고 작성 등 창작 과정을 지원합니다.\\n    * **코드 생성:** 간단한 코드를 생성하거나, 기존 코드를 수정하는 데 활용될 수 있습니다. 개발자의 생산성을 향상시키고, 프로그래밍 지식이 부족한 사람도 코드를 생성할 수 있도록 돕습니다.\\n* **고객 서비스:**\\n    * **챗봇 구축:** GPT-3 기반 챗봇은 자연스러운 대화 능력을 통해 고객 문의에 응답하고 문제 해결을 지원합니다. 24시간 응대가 가능하며, 다양한 언어를 지원하여 글로벌 고객 서비스 제공에 용이합니다.\\n    * **FAQ 자동 생성:** 자주 묻는 질문(FAQ)을 자동으로 생성하여 고객 지원 효율성을 높입니다.\\n* **교육:**\\n    * **맞춤형 학습 자료 생성:** 학생의 수준과 관심사에 맞는 맞춤형 학습 자료를 생성하여 학습 효과를 높입니다.\\n    * **자동 채점 및 피드백:** 에세이, 논문 등 주관식 답안을 자동으로 채점하고 피드백을 제공하여 교사의 업무 부담을 줄입니다.\\n    * **언어 학습 지원:** 외국어 학습 시, GPT-3는 번역, 문법 교정, 대화 연습 등 다양한 방식으로 학습을 지원합니다.\\n* **연구 및 개발:**\\n    * **데이터 분석 및 요약:** 방대한 텍스트 데이터를 분석하고 핵심 내용을 요약하여 연구자의 분석 시간을 단축합니다.\\n    * **가설 생성 및 검증:** 연구 주제에 대한 가설을 생성하고 관련 정보를 검색하여 가설의 타당성을 검증하는 데 활용됩니다.\\n* **기타:**\\n    * **이메일 자동 작성:** 업무 관련 이메일을 자동으로 작성하여 업무 효율성을 높입니다.\\n    * **번역:** 다양한 언어 간 번역을 수행하며, 특히 문맥을 이해하고 자연스러운 번역을 제공하는 데 강점을 보입니다.\\n    * **게임 개발:** 게임 내 캐릭터의 대사를 생성하거나, 게임 스토리를 개발하는 데 활용됩니다.\\n\\n**2. GPT-3가 사회에 미치는 긍정적 영향**\\n\\nGPT-3 기술의 발전은 사회 전반에 걸쳐 다음과 같은 긍정적인 영향을 미칠 수 있습니다.\\n\\n* **생산성 향상:** 콘텐츠 생성, 코드 작성, 고객 서비스 등 다양한 분야에서 업무 효율성을 높여 생산성을 향상시킵니다.\\n* **접근성 향상:** 프로그래밍 지식이 부족한 사람도 코드를 생성하거나, 외국어 학습을 지원받을 수 있도록 하여 정보 접근성을 높입니다.\\n* **창의성 증진:** 아이디어 발상, 초고 작성 등 창작 과정을 지원하여 창의적인 활동을 촉진합니다.\\n* **새로운 비즈니스 기회 창출:** GPT-3를 활용한 새로운 서비스 및 애플리케이션 개발을 통해 새로운 비즈니스 기회를 창출합니다.\\n* **교육 기회 확대:** 맞춤형 학습 자료 제공, 자동 채점 등 교육 관련 서비스를 통해 교육 기회를 확대합니다.\\n\\n**3. GPT-3가 사회에 미치는 부정적 영향**\\n\\n---\\n\\n## 2.3 Codex, InstructGPT, ChatGPT 등 GPT-3 기반 모델의 등장: 특징, 성능 비교 분석 및 평가\\n\\nGPT-3의 등장 이후, GPT-3를 기반으로 구축된 다양한 모델들이 등장하며 자연어 처리 분야에 혁신적인 변화를 가져왔다. 이 섹션에서는 Codex, InstructGPT, ChatGPT를 중심으로 GPT-3를 기반으로 개발된 주요 모델들의 특징, 성능을 비교 분석하고, 각 모델이 가진 장단점을 평가하여 그 의미를 조명하고자 한다.\\n\\n**1. 모델 소개 및 개발 배경**\\n\\n* **Codex:** OpenAI에서 2021년 공개한 Codex는 GPT-3를 기반으로 프로그래밍 언어에 특화된 모델이다. GPT-3의 방대한 텍스트 데이터 학습에 더해 GitHub의 공개 코드 데이터를 추가적으로 학습하여 자연어 설명을 코드로 변환하는 능력을 극대화했다. 개발 배경은 자연어 기반의 코딩 자동화, 코드 생성, 코드 이해 등의 문제를 해결하고자 하는 OpenAI의 목표를 반영한다.\\n* **InstructGPT:** OpenAI에서 2022년 공개한 InstructGPT는 GPT-3의 성능을 유지하면서도 인간의 지시(instruction)에 더 잘 따르는 것을 목표로 개발되었다. GPT-3의 주요 문제점이었던 \\'환각(hallucination)\\' 현상, 즉 사실과 다른 내용을 생성하는 문제를 해결하고, 인간의 의도에 부합하는 답변을 생성하는 데 초점을 맞췄다.\\n* **ChatGPT:** OpenAI에서 2022년 12월 공개한 ChatGPT는 GPT-3.5를 기반으로 구축된 대화형 AI 모델이다. 인간과의 자연스러운 대화를 목표로 개발되었으며, 대화 맥락을 이해하고 유지하며, 질문에 대한 답변, 글쓰기, 번역 등 다양한 작업을 수행할 수 있다. 특히 강화 학습 기반의 인간 피드백(Reinforcement Learning from Human Feedback, RLHF)을 통해 성능을 향상시켰다는 특징을 가진다.\\n\\n**2. 모델 특징 비교**\\n\\n| 특징 | Codex | InstructGPT | ChatGPT |\\n|---|---|---|---|\\n| **기반 모델** | GPT-3 | GPT-3 | GPT-3.5 |\\n| **주요 목적** | 코드 생성 및 이해 | 인간 지시(instruction) 준수 | 자연스러운 대화 |\\n| **학습 데이터** | 텍스트 데이터 + GitHub 공개 코드 데이터 | 텍스트 데이터 + 인간 지시 및 피드백 데이터 | 텍스트 데이터 + 대화 데이터 + RLHF 데이터 |\\n| **강조점** | 코드 생성 정확도 및 효율성 | 답변의 정확성 및 일관성 | 대화의 자연스러움 및 맥락 유지 |\\n| **사용 분야** | 코드 자동 완성, 코드 생성, 코드 이해, 프로그래밍 교육 | 챗봇, 가상 비서, 지시 기반 작업 수행 | 챗봇, 고객 지원, 콘텐츠 생성, 교육 |\\n| **API 제공 여부** | 제공 | 제공 | 제공 |\\n\\n**3. 모델 성능 비교 및 평가**\\n\\n* **Codex:**\\n    * **장점:** 자연어 설명을 코드로 변환하는 능력에서 뛰어난 성능을 보인다. 다양한 프로그래밍 언어를 지원하며, 복잡한 코드도 생성할 수 있다. 코드 자동 완성 기능은 개발 생산성을 크게 향상시킨다.\\n    * **단점:** 생성된 코드의 정확성을 보장하기 어려울 수 있으며, 오류가 발생할 경우 디버깅이 필요하다. 자연어 설명의 모호성으로 인해 예상치 못한 코드가 생성될 수 있다.\\n* **InstructGPT:**\\n    * **장점:** GPT-3에 비해 인간의 지시를 더 잘 따르며, 환각 현상이 줄어들어 답변의 신뢰도가 높아졌다. 답변의 일관성을 유지하며, 인간의 의도에 부합하는 답변을 제공한다.\\n    * **단점:** GPT-3에 비해 창의성이 다소 감소할 수 있으며, 특정 분야에 대한 전문적인 지식은 부족할 수 있다.\\n* **ChatGPT:**\\n    * **장점:** 자연스러운 대화 능력이 뛰어나며, 대화 맥락을 잘 이해하고 유지한다. 다양한 주제에 대한 답변을 제공하며, 글쓰기, 번역 등 다양한 작업을 수행할 수 있다. RLHF를 통해 인간의 선호도에 맞게 답변을 생성한다.\\n    * **단점:** 여전히 환각 현상이 발생할 수 있으며, 답변의 정확성을 보장하기 어려울 수 있다. 윤리적인 문제 (혐오 발언, 편향된 정보 등) 발생 가능성이 존재한다.\\n\\n**4. 각 모델의 차별화 전략 및 미래 전망**\\n\\n* **Codex:** 코드 생성\\n\\n---\\n\\n## 3장: 최신 LLM의 발전 동향 및 미래 전망 (2023-현재) - GPT-4, Gemini, Claude 등\\n\\n**1. 서론: LLM의 급격한 진화와 2023년 이후의 변화**\\n\\n2023년 이후, 대규모 언어 모델(Large Language Models, LLMs) 분야는 이전과는 비교할 수 없을 정도로 빠른 속도로 발전하고 있습니다. GPT-3의 등장 이후, OpenAI의 GPT-4, Google의 Gemini, Anthropic의 Claude 등 더욱 강력하고 다재다능한 LLM들이 잇따라 공개되면서 LLM 기술은 실용적인 수준으로 빠르게 도약했습니다. 본 장에서는 이러한 최신 LLM들의 등장 배경, 핵심 아키텍처 특징, 성능 향상 요인들을 분석하고, LLM의 미래 발전 방향과 함께 고려해야 할 윤리적 문제들을 심층적으로 논의합니다.\\n\\n**2. 주요 LLM 등장 배경 및 경쟁 구도**\\n\\n* **OpenAI - GPT-4:** GPT-3의 성공에 힘입어 OpenAI는 2023년 GPT-4를 공개했습니다. GPT-4는 텍스트뿐만 아니라 이미지 입력도 지원하는 멀티모달(Multimodal) 기능을 갖추고 있으며, GPT-3보다 더욱 복잡하고 미묘한 지시사항을 이해하고 수행할 수 있는 능력을 보여주었습니다. 이는 OpenAI의 독점적인 위치를 더욱 공고히 하는 요인이 되었습니다.\\n* **Google - Gemini:** Google은 GPT-4에 대응하기 위해 Gemini를 개발했습니다. Gemini는 Ultra, Pro, Nano 세 가지 버전으로 출시되었으며, 특히 Ultra 버전은 다양한 벤치마크 테스트에서 GPT-4를 능가하는 성능을 보여주며 LLM 경쟁에 새로운 동력을 불어넣었습니다. Gemini의 특징은 처음부터 멀티모달 모델로 설계되었다는 점이며, 이는 텍스트, 이미지, 오디오, 비디오 등 다양한 형태의 데이터를 통합적으로 처리할 수 있는 잠재력을 시사합니다.\\n* **Anthropic - Claude:** Anthropic은 안전하고 윤리적인 AI 개발을 목표로 Claude 모델을 개발했습니다. Claude는 긴 컨텍스트를 처리하는 능력과 인간과 유사한 대화 능력을 갖추고 있으며, 특히 기업 환경에서 활용될 수 있는 보안 및 규정 준수 기능을 강화했습니다. Anthropic은 ‘Constitutional AI’라는 독특한 접근 방식을 통해 모델의 행동을 제어하고 안전성을 확보하고 있습니다.\\n\\n이러한 LLM들의 등장은 AI 시장의 경쟁을 더욱 심화시키고 있으며, 각 기업들은 성능 향상뿐만 아니라 안전성, 윤리성, 비용 효율성 등 다양한 측면에서 경쟁하고 있습니다.\\n\\n**3. 아키텍처 특징 및 성능 향상 요인**\\n\\n* **모델 규모:** LLM의 성능 향상은 모델 규모의 증가와 밀접하게 관련되어 있습니다. GPT-4, Gemini, Claude 등 최신 LLM들은 수조 개 이상의 파라미터를 가지고 있으며, 이는 더욱 복잡한 패턴을 학습하고 이해할 수 있게 합니다. 하지만 모델 규모의 증가는 막대한 컴퓨팅 자원과 에너지 소비를 필요로 하는 문제점을 야기합니다.\\n* **트랜스포머 아키텍처의 발전:** LLM은 대부분 트랜스포머 아키텍처를 기반으로 합니다. 최신 LLM들은 트랜스포머 아키텍처의 효율성을 높이기 위해 다양한 기술들을 적용하고 있습니다. 예를 들어, Sparse Attention, FlashAttention 등은 계산 복잡도를 줄이고 메모리 사용량을 최적화하여 모델의 확장성을 높입니다.\\n* **멀티모달 학습:** GPT-4와 Gemini는 멀티모달 학습을 통해 텍스트뿐만 아니라 이미지, 오디오, 비디오 등 다양한 형태의 데이터를 처리할 수 있게 되었습니다. 이는 LLM의 활용 범위를 크게 확장시키고 있으며, 더욱 자연스럽고 지능적인 상호 작용을 가능하게 합니다.\\n* **강화 학습 (Reinforcement Learning):** Anthropic의 Claude는 ‘Constitutional AI’라는 독특한 접근 방식을 통해 강화 학습을 활용하여 모델의 행동을 제어하고 안전성을 확보하고 있습니다. 이는 LLM의 윤리적 문제 해결에 새로운 가능성을 제시합니다.\\n* **데이터 품질 및 양:** LLM의 성능은 학습 데이터의 품질과 양에 크게 의존합니다. 최신 LLM들은 더욱 방대한 양의 데이터를 수집하고, 데이터의 품질을 높이기 위한 노력을 기울이고 있습니다.\\n\\n**4. LLM의 미래 발전 방향**\\n\\n* **모델 경량화 (Model Compression):** LLM의 규모가 커짐에 따라, 모델 경량화 기술의 중요성이 더욱 커지고 있습니다.\\n\\n---\\n\\n## 3.1 GPT-4, Gemini, Claude 등 최신 LLM의 등장과 경쟁\\n\\n**1. 서론: LLM 발전의 가속화와 새로운 경쟁 구도**\\n\\n최근 몇 년간 자연어 처리(NLP) 분야는 거대한 규모의 언어 모델(Large Language Models, LLMs)의 등장으로 혁신적인 발전을 이루어냈습니다. GPT-3를 기점으로 LLM은 텍스트 생성, 번역, 질의 응답, 코드 생성 등 다양한 작업에서 인간에 가까운 성능을 보여주며, 인공지능 기술의 가능성을 획기적으로 확장했습니다. 이러한 추세에 발맞춰 OpenAI의 GPT-4, Google의 Gemini, Anthropic의 Claude 등 더욱 강력하고 다재다능한 LLM들이 등장하며, LLM 시장의 경쟁 구도는 더욱 치열해지고 있습니다. 본 보고서는 이러한 최신 LLM들의 등장 배경, 주요 특징, 모델 간 성능 비교, 그리고 경쟁 구도에 대해 심층적으로 분석하고자 합니다.\\n\\n**2. 주요 LLM 소개 및 특징**\\n\\n* **GPT-4 (OpenAI):**\\n    * **등장 배경:** GPT-3의 한계를 극복하고, 더욱 복잡한 추론 능력과 창의성을 확보하기 위해 개발되었습니다. OpenAI는 GPT-4의 정확한 규모를 공개하지 않았지만, GPT-3보다 훨씬 많은 데이터와 파라미터를 사용했을 것으로 추정됩니다.\\n    * **주요 특징:**\\n        * **다중 모달(Multimodal) 지원:** 텍스트뿐만 아니라 이미지 입력도 지원하여, 이미지에 대한 질문에 답변하거나 이미지 기반의 작업을 수행할 수 있습니다.\\n        * **향상된 추론 능력:** 복잡한 문제 해결, 논리적 사고, 창의적인 글쓰기 등 다양한 분야에서 GPT-3보다 뛰어난 성능을 보입니다.\\n        * **안전성 및 책임감 강화:** 유해 콘텐츠 생성 가능성을 줄이고, 편향성을 완화하기 위한 노력이 반영되었습니다.\\n        * **API 접근성:** 개발자를 위한 API를 제공하여 다양한 애플리케이션 개발에 활용될 수 있도록 지원합니다.\\n* **Gemini (Google):**\\n    * **등장 배경:** Google은 자체 개발한 TPU(Tensor Processing Unit)를 활용하여 LLM 개발을 가속화하고, GPT-4에 대응하기 위해 Gemini를 출시했습니다.\\n    * **주요 특징:**\\n        * **다양한 크기(Size)의 모델 제공:** Nano, Pro, Ultra 등 다양한 규모의 모델을 제공하여, 다양한 환경과 요구사항에 맞게 활용할 수 있습니다.\\n        * **다중 모달(Multimodal) 지원:** 텍스트, 이미지, 오디오, 비디오 등 다양한 형태의 데이터를 이해하고 처리할 수 있습니다.\\n        * **TPU 최적화:** Google의 TPU를 활용하여 뛰어난 성능과 효율성을 제공합니다.\\n        * **Google 서비스 통합:** Google 검색, Gmail, Google Workspace 등 Google의 다양한 서비스와 통합되어 시너지 효과를 창출합니다.\\n* **Claude (Anthropic):**\\n    * **등장 배경:** OpenAI의 GPT-3를 개발했던 핵심 인력들이 설립한 Anthropic에서 개발한 LLM입니다. 안전하고 윤리적인 AI 개발을 목표로 하며, \\'Constitutional AI\\'라는 독특한 접근 방식을 사용합니다.\\n    * **주요 특징:**\\n        * **Constitutional AI:** 미리 정의된 윤리적 원칙(헌법)에 따라 답변을 생성하여, 유해하거나 편향된 답변을 줄이도록 설계되었습니다.\\n        * **긴 컨텍스트 창(Context Window):** 긴 텍스트를 한 번에 처리할 수 있어, 복잡한 문서 요약, 코드 분석, 장문의 대화 등에 유리합니다.\\n        * **안전성 및 윤리성:** 유해 콘텐츠 생성 가능성을 최소화하고, 편향성을 완화하기 위한 노력을 지속적으로 기울이고 있습니다.\\n\\n**3. 모델 간 성능 비교**\\n\\n각 모델의 성능은 다양한 벤치마크 테스트를 통해 비교 분석될 수 있습니다. 하지만, LLM의 성능은 평가 방식, 데이터셋, 작업의 종류에 따라 달라질 수 있습니다. 현재까지의 평가 결과는 다음과 같습니다.\\n\\n| 특징 | GPT-4 | Gemini (Ultra) | Claude 3 (Opus) |\\n|---|---|---|---|\\n| **추론 능력** | 매우 우수, 복잡한 문제 해결에 강점 | 우수, 특히 수학 및 과학 분야에서 강점 | 매우 우수, 논리적 추론 및 코딩 능력 뛰어남 |\\n| **다중 모달** | 이미지 입력 지원 | 텍스트, 이미지, 오디오, 비디오 지원 | \\n\\n---\\n\\n## 3.2 멀티모달 LLM의 발전과 가능성: 텍스트를 넘어 다양한 데이터를 이해하고 활용하는 미래\\n\\n**1. 서론: 멀티모달 LLM의 등장 배경과 의미**\\n\\n최근 몇 년간 챗GPT와 같은 거대 언어 모델(LLM)의 급격한 발전은 인공지능 분야에 혁신적인 변화를 가져왔습니다. 그러나 LLM은 기본적으로 텍스트 데이터에 특화되어 있어, 현실 세계의 복잡하고 다양한 정보를 처리하는 데 한계점을 드러냈습니다. 이에 따라 인간의 인지 방식처럼 텍스트뿐만 아니라 이미지, 오디오, 비디오 등 다양한 형태의 데이터를 이해하고 상호 작용하는 **멀티모달 LLM(Multimodal Large Language Model)**이 주목받기 시작했습니다. 멀티모달 LLM은 단순히 여러 모달리티를 결합하는 것을 넘어, 각 모달리티 간의 관계를 파악하고 통합적인 이해를 가능하게 함으로써 더욱 지능적인 AI 시스템 구축의 가능성을 열고 있습니다.\\n\\n**2. 멀티모달 LLM의 발전 현황**\\n\\n멀티모달 LLM의 발전은 크게 세 가지 방향으로 진행되고 있습니다.\\n\\n*   **기존 LLM에 모달리티 추가:** GPT-4, Gemini, LLaVA와 같이 기존 LLM에 이미지 처리 능력을 추가하여 텍스트와 이미지를 함께 이해하고 생성하는 방식으로 발전해 왔습니다. 이러한 모델들은 이미지 캡셔닝, 시각적 질문 답변(VQA), 이미지 기반 텍스트 생성 등 다양한 작업을 수행할 수 있습니다.\\n*   **모달리티 간 연결에 특화된 모델 개발:** CLIP(Contrastive Language-Image Pre-training)과 같은 모델은 텍스트와 이미지를 연결하는 데 특화되어 있으며, 이미지 검색, 이미지 분류 등 다양한 분야에서 활용되고 있습니다. 이러한 모델들은 텍스트 설명과 이미지 간의 의미적 유사성을 학습하여 효과적인 연결을 가능하게 합니다.\\n*   **오디오, 비디오 등 다양한 모달리티 통합:** 최근에는 오디오, 비디오 등 다양한 모달리티를 통합하여 더욱 복잡한 작업을 수행할 수 있는 모델들이 등장하고 있습니다. 예를 들어, 텍스트와 오디오를 함께 처리하여 음성 기반 질문 답변, 비디오 캡셔닝, 비디오 기반 텍스트 생성 등을 가능하게 합니다.\\n\\n**주요 멀티모달 LLM 모델 및 특징:**\\n\\n| 모델명 | 주요 특징 | 활용 분야 |\\n|---|---|---|\\n| **GPT-4** | 이미지 입력 가능, 텍스트 기반 추론 및 생성 능력 우수 | 시각적 질문 답변, 이미지 기반 텍스트 생성, 콘텐츠 요약 |\\n| **Gemini** | 텍스트, 이미지, 오디오, 비디오 등 다양한 모달리티 처리 가능, Google의 다양한 서비스 통합 예정 | 검색, 콘텐츠 생성, 자동화된 작업 |\\n| **LLaVA** | 오픈소스, 이미지 기반 질문 답변, 이미지 설명 생성 | 연구 및 개발, 교육 |\\n| **CLIP** | 텍스트와 이미지의 의미적 연결에 특화 | 이미지 검색, 이미지 분류, 콘텐츠 추천 |\\n| **Flamingo** | 이미지와 텍스트를 결합하여 복잡한 추론 가능 | 시각적 질문 답변, 이미지 기반 스토리텔링 |\\n\\n**3. 멀티모달 LLM의 가능성**\\n\\n멀티모달 LLM은 다양한 분야에서 혁신적인 가능성을 제시하고 있습니다.\\n\\n*   **교육:** 개인 맞춤형 학습 콘텐츠 제공, 시각 자료를 활용한 학습 지원, 학생들의 질문에 시각적으로 답변하는 튜터 역할 수행\\n*   **의료:** 의료 영상 분석 및 진단 지원, 환자의 증상에 대한 시각적 설명 제공, 의료 기록 요약 및 질병 예측\\n*   **콘텐츠 제작:** 이미지 기반 스토리텔링, 자동 비디오 캡셔닝, 텍스트 기반 이미지 생성, 음악 작곡 및 가사 생성\\n*   **접근성 향상:** 시각 장애인을 위한 이미지 설명 제공, 청각 장애인을 위한 텍스트 기반 오디오 설명 제공\\n*   **로봇 공학:** 로봇의 시각적 환경 이해 및 상호 작용, 음성 명령과 시각적 정보를 결합한 작업 수행\\n*   **검색:** 텍스트, 이미지, 오디오, 비디오를 통합하여 더욱 정확하고 풍부한 검색 결과 제공\\n*   **고객 서비스:** 텍스트, 이미지, 오디오를 결합하여 더욱 효율적이고 만족스러운 고객 지원 제공\\n\\n**4. 멀티모달 LLM의 도전 과제**\\n\\n멀티모달 LLM의 발전은 많은 가능성을 제시하지만, 해결해야 할 과제도 존재\\n\\n---\\n\\n## 3.3 LLM의 윤리적 문제와 안전성 확보 방안\\n\\n**1. 서론: LLM의 급격한 발전과 윤리적 책임의 중요성**\\n\\n대규모 언어 모델(LLM, Large Language Model)은 챗봇, 콘텐츠 생성, 번역 등 다양한 분야에서 혁신적인 가능성을 보여주며 빠르게 발전하고 있습니다. 하지만 LLM의 성능이 향상될수록, 그에 따른 윤리적 문제와 안전성에 대한 우려 또한 증폭되고 있습니다. LLM은 사회에 긍정적인 영향을 미칠 수 있지만, 동시에 편향성 심화, 허위 정보 확산, 악의적 사용 등 심각한 문제를 야기할 수 있기 때문입니다. 본 보고서는 LLM이 야기하는 주요 윤리적 문제점을 심층적으로 분석하고, 이러한 문제점을 해결하고 LLM의 안전성을 확보하기 위한 기술적, 정책적 방안을 제시합니다.\\n\\n**2. LLM의 윤리적 문제점**\\n\\nLLM은 방대한 양의 데이터를 기반으로 학습하기 때문에, 데이터에 내재된 편향성을 그대로 반영할 가능성이 높습니다. 이는 다양한 윤리적 문제로 이어질 수 있습니다.\\n\\n* **2.1 편향성 (Bias):**\\n    * **데이터 편향:** LLM 학습에 사용되는 데이터는 사회적, 문화적 맥락에 따라 특정 집단에 대한 편향된 정보를 담고 있을 수 있습니다. 이러한 데이터 편향은 LLM이 특정 성별, 인종, 종교, 정치적 견해 등에 대해 차별적이거나 불공정한 답변을 생성하도록 만들 수 있습니다. 예를 들어, 특정 직업을 언급할 때 남성 위주로 답변을 생성하거나, 특정 인종에 대한 부정적인 고정관념을 강화하는 답변을 생성할 수 있습니다.\\n    * **알고리즘 편향:** LLM의 학습 알고리즘 자체가 특정 결과를 선호하도록 설계될 수 있습니다. 이는 의도치 않게 특정 집단에 불리한 결과를 초래할 수 있습니다.\\n    * **결과 편향:** LLM이 생성하는 결과물은 학습 데이터와 알고리즘의 영향을 받아 편향된 시각을 반영할 수 있습니다.\\n* **2.2 허위 정보 생성 (Hallucination & Fabrication):**\\n    * LLM은 사실과 다른 정보를 마치 사실인 것처럼 제시하는 \\'환각(Hallucination)\\' 현상을 보일 수 있습니다. 이는 LLM이 학습 데이터에서 패턴을 학습하는 과정에서 발생하는 문제로, 사실 확인 없이 정보를 생성하는 경향을 나타냅니다.\\n    * LLM은 존재하지 않는 정보를 \\'조작(Fabrication)\\'하여 생성할 수도 있습니다. 이는 LLM이 지식의 한계를 인지하지 못하고, 부족한 정보를 채우기 위해 가짜 정보를 생성하는 결과입니다.\\n    * 이러한 허위 정보 생성은 사회적 혼란을 야기하고, 개인의 명예를 훼손하며, 심각한 사회적 문제를 초래할 수 있습니다.\\n* **2.3 악용 가능성 (Potential for Misuse):**\\n    * **사기 및 스미싱:** LLM을 이용하여 정교한 사기 및 스미싱 메시지를 생성하여 개인 정보를 탈취하거나 금전적인 피해를 입힐 수 있습니다.\\n    * **가짜 뉴스 생성 및 유포:** LLM을 이용하여 정치적 선전, 허위 사실 유포 등 가짜 뉴스를 생성하고 유포하여 사회적 혼란을 야기할 수 있습니다.\\n    * **악성 코드 생성:** LLM을 이용하여 악성 코드를 생성하고 유포하여 사이버 범죄를 저지를 수 있습니다.\\n    * **딥페이크 제작:** LLM과 결합하여 딥페이크를 제작하여 개인의 명예를 훼손하거나 사회적 혼란을 야기할 수 있습니다.\\n\\n**3. LLM의 안전성 확보 방안**\\n\\nLLM의 윤리적 문제점을 해결하고 안전성을 확보하기 위해서는 기술적, 정책적 방안을 종합적으로 적용해야 합니다.\\n\\n* **3.1 기술적 방안:**\\n    * **데이터 편향 완화:**\\n        * **데이터 다양성 확보:** 다양한 인구 집단, 문화, 관점을 반영하는 데이터를 수집하여 학습 데이터의 다양성을 확보해야 합니다.\\n        * **데이터 증강 (Data Augmentation):** 기존 데이터에 변형을 가하거나 새로운 데이터를 생성하여 데이터셋의 불균형을 해소해야 합니다.\\n        * **편향 완화 알고리즘 개발:** 학습 데이터의 편향성을 감지하고 완화하는 알고리즘을 개발해야 합니다.\\n    * **허위 정보 생성 방지:**\\n        * **사실 확인 (Fact-Checking) 시스템 통합:** LL\\n\\n---\\n\\n## 3.4 LLM의 미래 발전 방향: AGI(Artificial General Intelligence)를 향하여\\n\\n**서론**\\n\\n최근 거대한 언어 모델(LLM, Large Language Model)의 발전은 인공지능 분야에 혁명적인 변화를 가져왔습니다. GPT-3, LaMDA, PaLM 등 LLM들은 인간과 유사한 텍스트 생성 능력, 번역, 질의응답 등 다양한 작업을 수행하며 그 가능성을 입증했습니다. 하지만 LLM은 여전히 특정 작업에 특화된 \\'Narrow AI\\'의 범주에 속하며, 진정한 의미의 인공지능, 즉 인간과 동등하거나 그 이상의 지적 능력을 갖춘 AGI(Artificial General Intelligence)에는 미치지 못합니다. 본 보고서는 LLM의 미래 발전 방향을 제시하고, AGI를 향한 연구 개발 동향을 전망하며, LLM이 사회에 미칠 영향에 대해 심층적으로 논의합니다.\\n\\n**1. LLM의 한계와 극복 과제**\\n\\n현재 LLM은 다음과 같은 근본적인 한계를 가지고 있습니다.\\n\\n* **이해력 부족:** LLM은 텍스트 패턴을 학습하여 텍스트를 생성하지만, 텍스트의 의미를 진정으로 이해한다고 보기 어렵습니다. 이는 LLM이 맥락을 파악하지 못하거나, 상식적인 추론을 수행하는 데 어려움을 겪는 이유입니다.\\n* **환각(Hallucination) 현상:** LLM은 사실과 다른 정보를 마치 사실인 것처럼 생성하는 \\'환각\\' 현상을 보입니다. 이는 학습 데이터의 편향성, 불충분한 지식, 추론 능력 부족 등 다양한 원인에 기인합니다.\\n* **상식과 추론 능력 부족:** LLM은 인간의 상식적인 지식이나 추론 능력을 갖추지 못하여 복잡한 문제 해결에 어려움을 겪습니다.\\n* **데이터 의존성:** LLM은 방대한 양의 데이터를 필요로 하며, 학습 데이터에 편향될 가능성이 높습니다.\\n* **계산 비용:** LLM의 학습 및 운영에는 막대한 계산 비용이 소요됩니다.\\n\\n이러한 한계를 극복하기 위해 다음과 같은 연구가 진행되고 있습니다.\\n\\n* **지식 주입(Knowledge Injection):** 외부 지식 베이스를 LLM에 통합하여 지식의 정확성과 폭을 넓히는 연구입니다.\\n* **강화 학습(Reinforcement Learning):** 인간의 피드백을 통해 LLM의 성능을 개선하는 연구입니다.\\n* **멀티모달 학습(Multimodal Learning):** 텍스트뿐만 아니라 이미지, 오디오, 비디오 등 다양한 형태의 데이터를 함께 학습하여 LLM의 이해력을 높이는 연구입니다.\\n* **추론 능력 강화:** 논리적 추론, 상식 추론 등 LLM의 추론 능력을 향상시키기 위한 연구입니다.\\n* **설명 가능한 AI(Explainable AI, XAI):** LLM의 의사 결정 과정을 설명 가능하게 만들어 신뢰도를 높이는 연구입니다.\\n\\n**2. AGI를 향한 연구 개발 동향**\\n\\nAGI는 LLM의 발전 방향을 넘어, 인공지능 연구의 궁극적인 목표입니다. AGI를 향한 연구는 다음과 같은 방향으로 진행되고 있습니다.\\n\\n* **신경망 아키텍처 혁신:** Transformer 기반의 LLM 외에도 새로운 신경망 아키텍처를 개발하여 LLM의 한계를 극복하려는 시도가 이루어지고 있습니다.\\n* **자기 지도 학습(Self-Supervised Learning) 발전:** 레이블이 없는 데이터로부터 스스로 학습하는 자기 지도 학습의 효율성을 높이는 연구가 진행되고 있습니다.\\n* **인공 일반 지능(Artificial General Intelligence, AGI) 프레임워크 개발:** AGI의 핵심 구성 요소를 정의하고, 이를 구현하기 위한 프레임워크를 개발하는 연구가 진행되고 있습니다.\\n* **신경-상징적 AI(Neuro-Symbolic AI):** 신경망의 패턴 인식 능력과 상징적 추론의 논리적 추론 능력을 결합하여 AGI를 구현하려는 연구입니다.\\n* **강화 학습의 발전:** 복잡한 환경에서 스스로 학습하고 문제를 해결하는 강화 학습 알고리즘의 성능을 향상시키는 연구입니다.\\n\\n**3. LLM이 사회에 미칠 영향**\\n\\nLLM의 발전은 사회 전반에 걸쳐 광범위한 영향을 미칠 것으로 예상됩니다.\\n\\n* **긍정적인 영향:**\\n    * **업무 효율성 증대:** LLM은 문서 작성, 번역, 고객 응대 등 다양한 업무를 자동화하여 업무 효율성을 증대시킬 수 있습니다.\\n    * **새로운 산업'}}\n",
            "--------------\n"
          ]
        }
      ],
      "source": [
        "with torch.inference_mode():\n",
        "    for data in graph.stream({\"topic\": \"GPT 1부터 최신 LLM까지의 발전과정 (총 3챕터 길이)\"}, stream_mode='updates'):\n",
        "        print(data)\n",
        "        print('--------------')\n",
        "        # 생성은 병렬적이지만 합치는 순서는 호출한 순서"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkeaSELrDaqO",
        "outputId": "ef743644-20bf-407d-8d12-5c0fea3965d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "## 1장: GPT-1의 등장과 초기 발전 (2018-2019) - 거대 언어 모델의 서막과 가능성과 한계\n",
            "\n",
            "**1.1. 등장 배경: 언어 모델 발전의 흐름과 GPT-1의 필요성**\n",
            "\n",
            "2018년, OpenAI는 GPT-1 (Generative Pre-trained Transformer)을 공개하며 자연어 처리 (NLP) 분야에 큰 파장을 일으켰습니다. 그 이전까지 NLP 분야는 특정 작업 (Task-Specific)에 특화된 모델을 구축하는 데 집중되어 있었습니다. 예를 들어, 감성 분석, 기계 번역, 질의 응답 등 각 작업에 최적화된 모델을 별도로 학습시키는 방식이 일반적이었습니다. 이러한 방식은 각 작업마다 많은 양의 레이블링된 데이터를 필요로 했으며, 새로운 작업에 적용하기 위해서는 추가적인 학습 과정이 필요했습니다.\n",
            "\n",
            "GPT-1의 등장은 이러한 문제점을 해결하고자 하는 흐름의 결과였습니다. OpenAI는 **사전 학습 (Pre-training)**이라는 새로운 패러다임을 도입했습니다. 이는 방대한 텍스트 데이터 (Wikipedia, 웹 크롤링 데이터 등)를 사용하여 일반적인 언어 이해 능력을 먼저 학습시키고, 이후 특정 작업에 미세 조정 (Fine-tuning)하는 방식입니다. 즉, 다양한 작업을 수행하기 위한 기반 지식을 먼저 쌓은 후, 특정 작업에 필요한 지식을 추가하는 것입니다.\n",
            "\n",
            "GPT-1은 이러한 사전 학습 패러다임을 Transformer 아키텍처에 적용하여 구현되었습니다. Transformer는 Google에서 2017년에 발표한 \"Attention is All You Need\" 논문에서 소개된 새로운 신경망 아키텍처로, 기존의 순환 신경망 (RNN) 기반 모델의 병목 현상을 해결하고 병렬 처리를 가능하게 하여 학습 속도를 획기적으로 향상시켰습니다.\n",
            "\n",
            "**1.2. 아키텍처 특징: Transformer 기반의 언어 모델**\n",
            "\n",
            "GPT-1은 Transformer 아키텍처를 기반으로 구축되었으며, 다음과 같은 특징을 가집니다.\n",
            "\n",
            "* **Transformer Decoder:** GPT-1은 Transformer의 Decoder 부분만을 사용합니다. Decoder는 주어진 문맥을 바탕으로 다음 단어를 예측하는 역할을 수행합니다.\n",
            "* **Self-Attention 메커니즘:** Transformer의 핵심 구성 요소인 Self-Attention 메커니즘은 문장 내의 단어 간의 관계를 파악하여 문맥을 이해하는 데 중요한 역할을 합니다. GPT-1은 Self-Attention 메커니즘을 통해 문장 내의 단어 간의 의존성을 효과적으로 학습했습니다.\n",
            "* **레이어 수 및 파라미터 크기:** GPT-1은 다양한 크기로 구축되었으며, 가장 큰 모델은 11억 7천만 개의 파라미터를 가지고 있었습니다. 이는 당시로서는 매우 큰 규모의 파라미터 수였습니다.\n",
            "* **Masked Self-Attention:** GPT-1은 미래의 단어를 참조하지 않고 과거의 단어만을 참조하도록 Masked Self-Attention 메커니즘을 사용했습니다. 이는 언어 모델링의 기본적인 가정인 \"다음 단어 예측\"을 가능하게 합니다.\n",
            "\n",
            "**1.3. 초기 성능 및 한계: 놀라운 생성 능력과 현실적인 제약**\n",
            "\n",
            "GPT-1은 사전 학습 후 다양한 NLP 작업에 미세 조정되었을 때 상당한 성능 향상을 보였습니다. 특히, 텍스트 생성 능력은 주목할 만했습니다. GPT-1은 주어진 문맥에 따라 자연스럽고 일관성 있는 텍스트를 생성할 수 있었으며, 때로는 인간이 작성한 것과 구별하기 어려울 정도의 텍스트를 생성하기도 했습니다.\n",
            "\n",
            "하지만 GPT-1은 다음과 같은 한계점 또한 가지고 있었습니다.\n",
            "\n",
            "* **문맥 이해 부족:** GPT-1은 문맥을 완벽하게 이해하지 못하고, 때로는 앞뒤가 맞지 않거나 비논리적인 텍스트를 생성하기도 했습니다. 특히, 장문의 텍스트를 생성할 때 이러한 문제가 더욱 두드러졌습니다.\n",
            "* **사실 기반 지식 부족:** GPT-1은 방대한 텍스트 데이터를 학습했지만, 사실 기반 지식은 부족했습니다. 따라서, 사실에 기반한 질문에 정확하게 답변하지 못하거나, 잘못된 정보를 제공하기도 했습니다.\n",
            "* **편향성 문제:** GPT-1은 학습 데이터에 존재하는 편향성을 그대로 반영했습니다. 따라서, 성별, 인종, 종교 등에 대한 편향적인 텍스트를 생성할 가능성이 있었습니다.\n",
            "* **계산 비용:** GPT-1과 같은 거대 언어 모델을 학습하고 실행하는 데는 막대한 계산 비용이 소요되었습니다. 이는 GPT-1의 보급과 활용에 제약 요인으로 작용했습니다.\n",
            "\n",
            "**1.4. GPT\n",
            "\n",
            "---\n",
            "\n",
            "## 1.1 GPT-1의 탄생과 Transformer 아키텍처: 자연어 처리의 새로운 지평을 열다\n",
            "\n",
            "본 섹션에서는 OpenAI에 의해 개발된 GPT-1의 탄생 배경과 그 핵심 기술인 Transformer 아키텍처에 대해 상세히 설명합니다. GPT-1은 자연어 처리 (NLP) 분야에 혁명적인 변화를 가져왔으며, 특히 텍스트 생성 분야에서 괄목할 만한 성과를 보여주었습니다.\n",
            "\n",
            "**1. GPT-1 개발 배경 및 OpenAI의 목표**\n",
            "\n",
            "OpenAI는 인공지능 (AI)의 안전하고 긍정적인 발전을 목표로 하는 연구 기관입니다. GPT-1 개발 당시 OpenAI는 기존의 NLP 모델들이 특정 작업에 맞춰 설계되어 범용적인 문제 해결 능력은 부족하다는 점을 인지했습니다.  기존 모델들은 주로 지도 학습 (Supervised Learning) 방식으로, 특정 작업에 대한 레이블링된 데이터셋을 필요로 했습니다.  이는 데이터 수집 및 레이블링에 상당한 비용과 노력이 소요되었으며, 새로운 작업에 대한 적응성이 떨어지는 단점을 가지고 있었습니다.\n",
            "\n",
            "OpenAI는 이러한 문제점을 해결하고, **“제로샷 (Zero-shot) 학습”** 능력을 갖춘 모델을 개발하고자 했습니다. 제로샷 학습이란, 특정 작업에 대한 학습 데이터 없이도 해당 작업을 수행할 수 있는 능력을 의미합니다.  즉, 다양한 작업을 수행할 수 있는 범용적인 언어 모델을 구축하는 것을 목표로 했습니다.  GPT-1은 이러한 목표를 달성하기 위한 첫 번째 단계였으며, 방대한 텍스트 데이터셋을 통해 언어의 패턴을 학습하고, 이를 기반으로 다양한 텍스트 생성 작업을 수행할 수 있도록 설계되었습니다.\n",
            "\n",
            "**2. Transformer 아키텍처의 핵심 개념**\n",
            "\n",
            "GPT-1의 핵심 기술은 Google에서 2017년에 발표한 **Transformer 아키텍처**입니다.  Transformer는 기존의 순환 신경망 (RNN) 기반 모델들이 가지고 있던 문제점을 해결하고, 병렬 처리를 통해 학습 속도를 획기적으로 향상시킨 혁신적인 구조입니다.  Transformer의 핵심 개념은 다음과 같습니다.\n",
            "\n",
            "* **Attention 메커니즘:**  Transformer의 가장 중요한 특징은 Attention 메커니즘입니다.  Attention은 입력 시퀀스의 각 단어가 다른 단어와 얼마나 관련이 있는지 계산하여, 중요한 단어에 집중하도록 합니다.  이는 문맥을 파악하고, 장거리 의존성을 효과적으로 처리하는 데 도움이 됩니다.  기존 RNN 기반 모델들은 시퀀스의 앞부분에 있는 정보가 뒷부분으로 전달될수록 희석되는 문제 (Vanishing Gradient Problem)를 가지고 있었지만, Attention 메커니즘은 이러한 문제를 완화합니다.\n",
            "* **Self-Attention:**  Transformer는 입력 시퀀스 내의 단어 간의 관계를 파악하기 위해 Self-Attention을 사용합니다.  Self-Attention은 입력 시퀀스의 각 단어를 다른 모든 단어와 비교하여 관련성을 평가하고, 이를 기반으로 각 단어의 표현을 업데이트합니다.\n",
            "* **Encoder-Decoder 구조:**  Transformer는 Encoder와 Decoder라는 두 개의 모듈로 구성됩니다.  Encoder는 입력 시퀀스를 처리하여 문맥 정보를 담고 있는 표현을 생성하고, Decoder는 이 표현을 기반으로 출력 시퀀스를 생성합니다.  GPT-1은 Decoder만 사용한 구조입니다.\n",
            "* **Positional Encoding:**  Transformer는 순서 정보를 처리하지 않기 때문에, Positional Encoding을 사용하여 단어의 위치 정보를 모델에 제공합니다.  Positional Encoding은 각 단어의 위치에 따라 다른 값을 부여하여, 모델이 단어의 순서를 인식할 수 있도록 합니다.\n",
            "* **Multi-Head Attention:**  Transformer는 여러 개의 Attention 헤드를 사용하여 다양한 관점에서 단어 간의 관계를 파악합니다.  각 헤드는 서로 다른 방식으로 Attention을 계산하며, 이를 통해 모델은 더욱 풍부한 문맥 정보를 학습할 수 있습니다.\n",
            "\n",
            "**3. GPT-1에 적용된 Transformer 아키텍처**\n",
            "\n",
            "GPT-1은 Transformer 아키텍처의 **Decoder 부분만 사용**한 구조입니다.  이는 GPT-1의 목표가 특정 작업에 대한 학습 데이터 없이도 텍스트를 생성하는 것이었기 때문입니다.  GPT-1은 다음과 같은 특징을 가집니다.\n",
            "\n",
            "* **모델 크기:** GPT-1은 다양한 크기로 구축되었으며, 가장 큰 모델은 11억 7천만 개의 파라미터를 가지고 있습니다.\n",
            "* **학습 데이터:** GPT-1은 Common Crawl 데이터셋을 포함한 방대한 텍스트 데이터셋을 사용하여 학습되었습니다.  Common Crawl은 웹 페이지의 텍스트 데이터를 수집한 데이터셋으로, GPT-1은 이 데이터셋을 통해\n",
            "\n",
            "---\n",
            "\n",
            "## 1.2 GPT-1의 성능 및 한계점: 초기 성능 평가와 텍스트 생성의 한계 분석\n",
            "\n",
            "GPT-1 (Generative Pre-trained Transformer 1)은 OpenAI에서 2018년에 공개한 초기 대규모 언어 모델로, 딥러닝 기반의 자연어 처리 (NLP) 분야에 큰 영향을 미쳤습니다. GPT-1은 방대한 양의 텍스트 데이터 (웹 크롤링 데이터)를 사용하여 사전 훈련되었으며, 주어진 프롬프트 (prompt)에 기반하여 텍스트를 생성하는 능력을 보유했습니다. 본 보고서는 GPT-1의 초기 성능을 평가하고, 텍스트 생성의 품질, 일관성, 창의성 측면에서 나타난 주요 한계점을 상세히 분석합니다.\n",
            "\n",
            "**1. GPT-1의 초기 성능 평가**\n",
            "\n",
            "GPT-1은 공개 당시, 여러 NLP 작업에서 상당한 성능을 보였습니다. 특히, 다음과 같은 측면에서 주목할 만한 성과를 거두었습니다.\n",
            "\n",
            "* **문맥 이해 능력:** GPT-1은 비교적 긴 문맥을 이해하고 이를 기반으로 텍스트를 생성할 수 있었습니다. 이는 이전 모델에 비해 괄목할 만한 발전이었으며, 문맥에 따른 의미 변화를 어느 정도 파악할 수 있음을 시사했습니다.\n",
            "* **다양한 스타일의 텍스트 생성:** GPT-1은 다양한 스타일의 텍스트를 생성할 수 있었습니다. 예를 들어, 뉴스 기사, 소설, 시 등 다양한 장르의 텍스트를 유사하게 모방하여 생성할 수 있었습니다. 이는 텍스트 생성 모델의 범용성을 입증하는 중요한 요소였습니다.\n",
            "* **제로-샷 학습 (Zero-Shot Learning) 능력:** GPT-1은 특정 작업에 대한 명시적인 훈련 없이도, 주어진 프롬프트만으로 어느 정도의 작업을 수행할 수 있는 제로-샷 학습 능력을 보여주었습니다. 이는 모델이 일반적인 언어 패턴을 학습하여 새로운 작업에 적용할 수 있음을 의미했습니다.\n",
            "\n",
            "**2. 텍스트 생성의 품질 측면에서 나타난 한계점**\n",
            "\n",
            "GPT-1은 초기 모델로서, 텍스트 생성의 품질 측면에서 다음과 같은 한계점을 드러냈습니다.\n",
            "\n",
            "* **반복적인 텍스트 생성:** GPT-1은 텍스트 생성 과정에서 특정 단어나 구문을 반복적으로 사용하는 경향이 있었습니다. 이는 모델이 텍스트의 다양성을 확보하는 데 어려움을 겪고 있음을 시사합니다. 특히, 긴 텍스트를 생성할 때 이러한 문제가 더욱 두드러졌습니다.\n",
            "* **의미 불명확성 및 비논리성:** GPT-1이 생성하는 텍스트는 때때로 의미가 불명확하거나 논리적으로 모순되는 경우가 있었습니다. 이는 모델이 텍스트의 의미를 정확하게 이해하고, 이를 바탕으로 일관성 있는 텍스트를 생성하는 데 한계가 있음을 보여줍니다.\n",
            "* **사실 기반 오류 (Factuality Errors):** GPT-1은 훈련 데이터에 포함된 잘못된 정보를 그대로 반영하여 사실과 다른 텍스트를 생성하는 경우가 있었습니다. 이는 모델이 텍스트의 진실성을 검증하는 능력이 부족함을 의미하며, 생성된 텍스트의 신뢰성을 저해하는 요인이었습니다.\n",
            "* **맥락 파악의 불완전성:** GPT-1은 문맥을 어느 정도 이해할 수 있었지만, 복잡하고 미묘한 맥락을 정확하게 파악하는 데 어려움을 겪었습니다. 특히, 문맥에 따라 의미가 달라지는 단어나 구문을 잘못 해석하여 부적절한 텍스트를 생성하는 경우가 있었습니다.\n",
            "\n",
            "**3. 텍스트 생성의 일관성 측면에서 나타난 한계점**\n",
            "\n",
            "GPT-1은 텍스트 생성의 일관성 측면에서도 다음과 같은 한계점을 보였습니다.\n",
            "\n",
            "* **주제 일관성 유지의 어려움:** GPT-1은 긴 텍스트를 생성할 때 주제 일관성을 유지하는 데 어려움을 겪었습니다. 텍스트의 초반부에는 특정 주제에 대해 논의했지만, 후반부에는 주제가 벗어나거나 관련 없는 내용을 담는 경우가 있었습니다.\n",
            "* **캐릭터 일관성 유지의 어려움 (스토리텔링):** GPT-1을 사용하여 스토리를 생성할 때, 캐릭터의 성격이나 행동 패턴을 일관성 있게 유지하는 데 어려움을 겪었습니다. 캐릭터의 성격이 갑작스럽게 변하거나, 상황에 맞지 않는 행동을 하는 경우가 있었습니다.\n",
            "* **문체 일관성 유지의 어려움:** GPT-1은 다양한 스타일의 텍스트를 생성할 수 있었지만, 생성된 텍스트 전체에 걸쳐\n",
            "\n",
            "---\n",
            "\n",
            "## 1.3 GPT-2로의 진화: 규모 확장과 새로운 기능\n",
            "\n",
            "본 섹션에서는 OpenAI에 의해 개발된 GPT-2의 등장 배경과 GPT-1 대비 괄목할 만한 규모 확장, 그리고 혁신적인 기능인 'zero-shot learning' 도입의 의미를 분석하고, GPT-2의 성능 향상을 심층적으로 살펴봅니다. GPT-2는 자연어 처리 (NLP) 분야에 큰 파장을 일으켰으며, 텍스트 생성 모델의 발전 방향을 제시하는 중요한 이정표가 되었습니다.\n",
            "\n",
            "**1. GPT-2 등장 배경: GPT-1의 한계와 OpenAI의 목표**\n",
            "\n",
            "GPT-2의 등장은 GPT-1의 성공적인 개발과 그 한계를 극복하고자 하는 OpenAI의 노력에서 비롯되었습니다. GPT-1은 트랜스포머(Transformer) 아키텍처를 기반으로 대규모 텍스트 데이터셋을 학습하여 인간과 유사한 텍스트를 생성하는 능력을 보여주었지만, 몇 가지 중요한 한계를 드러냈습니다.\n",
            "\n",
            "* **제한적인 생성 능력:** GPT-1은 짧은 텍스트 생성에는 능숙했지만, 긴 텍스트를 일관성 있게 생성하는 데 어려움을 겪었습니다. 맥락을 유지하고 논리적인 흐름을 이어가는 데 한계가 있었습니다.\n",
            "* **특정 작업에 대한 의존성:** GPT-1은 특정 작업에 맞게 미세 조정(fine-tuning)을 거쳐야 좋은 성능을 보였습니다. 즉, 다양한 작업에 유연하게 대응하기 어려웠습니다.\n",
            "* **윤리적 우려:** GPT-1이 생성하는 텍스트는 때때로 편향되거나 유해한 내용을 포함할 수 있다는 우려가 제기되었습니다.\n",
            "\n",
            "이러한 한계를 극복하고, 더욱 강력하고 유연하며 윤리적인 텍스트 생성 모델을 개발하고자 OpenAI는 GPT-2를 개발하게 된 것입니다.\n",
            "\n",
            "**2. GPT-1 대비 규모 확장: 파라미터 수 증가와 데이터셋 변화**\n",
            "\n",
            "GPT-2의 가장 큰 특징은 GPT-1 대비 규모가 대폭 확장되었다는 점입니다. GPT-1은 11억 7천만 개의 파라미터를 가지고 있었지만, GPT-2는 15억 개, 34억 개, 77억 개 등 다양한 규모의 모델을 제공했습니다. 특히 77억 개 파라미터를 가진 모델은 GPT-1의 규모를 훨씬 뛰어넘는 수준입니다.\n",
            "\n",
            "규모 확장은 단순히 파라미터 수 증가에 그치지 않고, 학습 데이터셋의 변화를 동반했습니다. GPT-2는 웹 크롤링을 통해 수집된 40GB의 텍스트 데이터셋을 사용했습니다. 이 데이터셋은 GPT-1에 사용된 데이터셋보다 훨씬 다양하고 방대한 내용을 담고 있어, GPT-2가 더 넓은 범위의 지식을 학습하고 다양한 스타일의 텍스트를 생성할 수 있도록 했습니다.\n",
            "\n",
            "* **데이터셋 구성:** GPT-2 학습 데이터셋은 웹에서 수집된 텍스트 데이터로, Reddit 웹사이트의 댓글과 링크를 통해 수집된 데이터를 포함합니다. 이러한 데이터는 다양한 주제, 스타일, 어조를 포함하고 있어 GPT-2의 일반적인 텍스트 생성 능력을 향상시키는 데 기여했습니다.\n",
            "\n",
            "**3. 새로운 기능: Zero-Shot Learning의 도입**\n",
            "\n",
            "GPT-2의 가장 중요한 혁신은 'zero-shot learning' 기능의 도입입니다. Zero-shot learning은 특정 작업에 대한 학습 데이터 없이도 해당 작업을 수행할 수 있는 능력을 의미합니다. GPT-2는 대규모 텍스트 데이터셋을 통해 일반적인 언어 이해 능력을 학습하고, 이를 바탕으로 다양한 작업을 수행할 수 있습니다.\n",
            "\n",
            "* **Zero-Shot Learning의 원리:** GPT-2는 텍스트 생성 모델로서, 주어진 프롬프트(prompt)에 따라 텍스트를 생성합니다. 사용자는 원하는 작업에 대한 설명을 프롬프트에 포함시키면, GPT-2는 해당 작업에 대한 학습 데이터 없이도 텍스트를 생성하여 작업을 수행합니다. 예를 들어, \"다음 문장을 프랑스어로 번역하세요:\" 라는 프롬프트를 제공하면, GPT-2는 해당 문장을 프랑스어로 번역한 텍스트를 생성합니다.\n",
            "* **Zero-Shot Learning의 의미:** Zero-Shot Learning 기능은 GPT-2를 더욱 유연하고 활용 가능하게 만들었습니다. 특정 작업에 대한 미세 조정 없이도 다양한 작업을 수행할 수 있어, 다양한 분야에서 활용될 수 있는 가능성을 열었습니다.\n",
            "\n",
            "**4. GPT-2의 성능 향상: 텍스트 생성 품질 및 다양성**\n",
            "\n",
            "GPT-2는 규모 확장과 Zero-Shot Learning 기능 도입을 통해 GPT-1\n",
            "\n",
            "---\n",
            "\n",
            "## 2장: GPT-3와 Beyond: 규모의 확장과 새로운 패러다임 (2020-2022)\n",
            "\n",
            "**1. GPT-3의 등장과 1750억 개의 파라미터 규모 확장: 게임 체인저의 탄생**\n",
            "\n",
            "2020년, OpenAI에서 발표한 GPT-3는 자연어 처리(NLP) 분야에 혁명적인 변화를 가져왔다. 기존 모델들과 비교했을 때 압도적인 규모, 즉 1750억 개의 파라미터를 가진 거대한 규모는 단순한 숫자를 넘어, 모델의 성능과 가능성을 비약적으로 끌어올리는 핵심 요인이었다.\n",
            "\n",
            "* **파라미터 규모의 의미:** 파라미터는 신경망의 연결 강도를 나타내는 값으로, 모델이 학습한 지식을 저장하는 역할을 한다. 파라미터 규모가 클수록 모델은 더 많은 정보를 기억하고 복잡한 패턴을 학습할 수 있으며, 이는 결과적으로 더 자연스럽고 유창한 텍스트 생성 능력으로 이어진다.\n",
            "* **GPT-3의 학습 데이터:** GPT-3는 인터넷 상의 방대한 텍스트 데이터(Common Crawl, WebText2, Books1&2, Wikipedia)를 사용하여 학습되었다. 이러한 데이터의 다양성은 GPT-3가 다양한 주제와 스타일에 대한 이해도를 높이는 데 기여했다.\n",
            "* **기존 모델과의 비교:** GPT-2 (15억 파라미터)와 비교했을 때, GPT-3는 파라미터 규모가 10배 이상 증가했으며, 이는 성능 향상으로 이어졌다. GPT-3는 이전 모델들이 어려움을 겪었던 복잡한 추론, 창의적인 글쓰기, 코딩 등 다양한 작업에서 뛰어난 성능을 보였다.\n",
            "\n",
            "**2. Few-shot Learning 능력 향상: 지시사항 이해의 진화**\n",
            "\n",
            "GPT-3의 가장 중요한 특징 중 하나는 Few-shot Learning 능력이다. Few-shot Learning은 모델에게 몇 개의 예시(few shots)만 제공하면 새로운 작업에 대한 이해도를 빠르게 높이고, 유사한 방식으로 작업을 수행할 수 있도록 하는 학습 방식이다.\n",
            "\n",
            "* **Zero-shot, One-shot, Few-shot 학습:**\n",
            "    * **Zero-shot Learning:** 모델에게 어떤 예시도 제공하지 않고, 단순히 지시사항만으로 작업을 수행하도록 한다.\n",
            "    * **One-shot Learning:** 모델에게 하나의 예시만 제공하고, 이를 바탕으로 작업을 수행하도록 한다.\n",
            "    * **Few-shot Learning:** 모델에게 몇 개의 예시를 제공하고, 이를 바탕으로 작업을 수행하도록 한다.\n",
            "* **GPT-3의 Few-shot Learning 능력:** GPT-3는 Few-shot Learning 능력이 뛰어나, 몇 개의 예시만으로도 다양한 작업(번역, 질문 답변, 요약, 코드 생성 등)을 수행할 수 있었다. 이는 모델이 텍스트의 패턴을 이해하고, 이를 일반화하여 새로운 상황에 적용할 수 있는 능력을 보여준다.\n",
            "* **Few-shot Learning의 의미:** Few-shot Learning 능력은 모델의 학습 효율성을 높이고, 특정 작업에 대한 데이터 부족 문제를 해결하는 데 기여한다. 또한, 사용자가 모델에게 작업을 지시하는 방식을 더욱 직관적으로 만들 수 있다.\n",
            "\n",
            "**3. GPT-3의 다양한 활용 사례: 잠재력의 무한한 확장**\n",
            "\n",
            "GPT-3는 Few-shot Learning 능력을 바탕으로 다양한 분야에서 활용 가능성을 보여주었다.\n",
            "\n",
            "* **콘텐츠 생성:** 블로그 게시물, 소설, 시, 광고 문구 등 다양한 종류의 콘텐츠를 생성할 수 있다.\n",
            "* **챗봇:** 자연스러운 대화가 가능한 챗봇을 구축하여 고객 서비스, 교육, 엔터테인먼트 등 다양한 분야에 적용할 수 있다.\n",
            "* **코드 생성:** 자연어 설명을 기반으로 코드를 생성하여 개발 생산성을 향상시킬 수 있다.\n",
            "* **번역:** 다양한 언어 간의 번역을 수행할 수 있다.\n",
            "* **질문 답변:** 복잡한 질문에 대한 답변을 제공할 수 있다.\n",
            "* **요약:** 긴 텍스트를 요약하여 핵심 내용을 전달할 수 있다.\n",
            "* **교육:** 학생들에게 맞춤형 학습 자료를 제공하고, 질문에 답변하는 등 교육적인 목적으로 활용할 수 있다.\n",
            "\n",
            "**4. GPT-3 이후 등장한 모델들의 특징: 새로운 패러다임의 등장**\n",
            "\n",
            "GPT-3의 성공 이후, 더 크고 강력한 모델들이 등장하며 NLP 분야는 더욱 빠르게 발전했다.\n",
            "\n",
            "* **PaLM (Google):** 5400억 개의 파라미터를 가진 Google의 PaLM은 GPT-3보다 더 큰 규모를 자랑하며, 복잡한 추론\n",
            "\n",
            "---\n",
            "\n",
            "## 2.1 GPT-3의 등장: 규모의 혁신과 Few-shot Learning\n",
            "\n",
            "본 섹션에서는 OpenAI에 의해 개발된 GPT-3 (Generative Pre-trained Transformer 3)의 등장 배경과 그 의미, 특히 파라미터 규모 확장에 따른 Few-shot Learning 능력의 발전과 그 효과를 심층적으로 분석하고자 한다. GPT-3는 자연어 처리 (NLP) 분야에 지대한 영향을 미쳤으며, 기존 모델들과 비교하여 괄목할 만한 성능 향상을 보여주었다.\n",
            "\n",
            "**1. GPT-3 개발 배경: Transformer 아키텍처와 사전학습의 진화**\n",
            "\n",
            "GPT-3의 등장은 자연스럽게 Transformer 아키텍처의 발전과 긴밀하게 연결되어 있다. 2017년 Google에서 발표한 Transformer는 기존 RNN (Recurrent Neural Network) 기반 모델의 단점을 극복하고, 병렬 처리를 통해 학습 속도를 획기적으로 향상시켰다.  Transformer는 Attention 메커니즘을 핵심으로 사용하여 문장 내 단어들 간의 관계를 효과적으로 파악하고, 문맥을 이해하는 능력을 크게 향상시켰다.\n",
            "\n",
            "GPT (Generative Pre-trained Transformer)는 이러한 Transformer 아키텍처를 기반으로, 방대한 텍스트 데이터셋을 활용하여 사전학습 (Pre-training)을 진행하는 모델이다. 사전학습 단계에서 GPT는 주어진 문맥을 기반으로 다음 단어를 예측하는 방식으로 언어의 통계적 패턴을 학습한다.  이후, 특정 작업 (Task)에 맞게 미세 조정 (Fine-tuning)을 통해 성능을 최적화한다.\n",
            "\n",
            "GPT-1, GPT-2를 거치면서 사전학습 데이터의 규모와 모델의 크기는 점진적으로 증가해왔다. GPT-1은 117M 파라미터를 사용했고, GPT-2는 1.5B 파라미터를 사용했다. 하지만 GPT-3는 이러한 추세를 넘어선 혁신적인 규모를 자랑한다.\n",
            "\n",
            "**2. 1750억 개의 파라미터 규모 확장의 의미: 언어 이해 능력의 비약적인 발전**\n",
            "\n",
            "GPT-3는 **1750억 개의 파라미터**를 갖는 거대한 규모의 언어 모델이다. 이는 GPT-2보다 약 100배 더 큰 규모이며, 기존 자연어 처리 모델들과 비교하여 압도적인 크기이다.  이러한 파라미터 규모의 증가는 GPT-3의 언어 이해 능력에 비약적인 발전을 가져왔다.\n",
            "\n",
            "* **더욱 풍부한 지식 습득:** 1750억 개의 파라미터는 방대한 양의 텍스트 데이터를 통해 더 많은 지식을 습득하고, 다양한 문맥과 어휘를 이해할 수 있게 한다.\n",
            "* **미묘한 뉘앙스 이해:**  더욱 복잡한 패턴을 학습함으로써 문장의 미묘한 뉘앙스, 비유, 은유 등을 이해하는 능력이 향상되었다.\n",
            "* **일반화 능력 향상:** 특정 작업에 대한 미세 조정 없이도 다양한 작업을 수행할 수 있는 일반화 능력 (Generalization)이 크게 향상되었다.\n",
            "\n",
            "**3. Few-shot Learning 능력의 발전과 그 효과: 제로샷, 원샷, 그리고 Few-shot**\n",
            "\n",
            "GPT-3의 가장 중요한 특징 중 하나는 **Few-shot Learning** 능력이다. Few-shot Learning은 모델이 극소량의 예시 (Example)만으로 새로운 작업을 수행할 수 있는 능력을 의미한다. GPT-3는 Few-shot Learning을 통해 기존 모델들이 필요로 했던 많은 양의 레이블링된 데이터 (Labeled Data) 없이도 다양한 작업을 수행할 수 있게 되었다.\n",
            "\n",
            "Few-shot Learning은 크게 다음과 같은 세 가지 단계로 나눌 수 있다:\n",
            "\n",
            "* **제로샷 (Zero-shot):**  예시 없이 바로 작업을 수행한다. GPT-3는 제로샷에서도 어느 정도의 성능을 보이지만, 기대만큼의 결과는 나오지 않는다.\n",
            "* **원샷 (One-shot):**  하나의 예시만 제공하여 작업을 수행한다.  GPT-3는 원샷에서도 상당한 성능 향상을 보인다.\n",
            "* **Few-shot:**  몇 개의 예시 (일반적으로 3~5개)를 제공하여 작업을 수행한다. GPT-3는 Few-shot 환경에서 가장 뛰어난 성능을 발휘하며, 인간 수준의 성능에 근접하는 결과를 보여준다.\n",
            "\n",
            "GPT-3의 Few-shot Learning 능력은 다음과 같은 효과를 가져왔다:\n",
            "\n",
            "* **데이터 부족 문제 해결:** 레이블링된 데이터가 부족한 작업에서도 효과적으로 수행할 수 있게 되었다.\n",
            "* **빠른 적응력:** 새로운\n",
            "\n",
            "---\n",
            "\n",
            "## 2.2 GPT-3의 활용 사례 및 사회적 영향\n",
            "\n",
            "**서론:**\n",
            "\n",
            "GPT-3 (Generative Pre-trained Transformer 3)는 OpenAI에서 개발한 거대 언어 모델로, 방대한 텍스트 데이터를 학습하여 인간과 유사한 텍스트를 생성하는 능력을 보유하고 있습니다. GPT-3의 등장으로 다양한 분야에서 혁신적인 서비스 및 애플리케이션 개발이 가능해졌으며, 이는 사회 전반에 걸쳐 긍정적 및 부정적 영향을 미치고 있습니다. 본 보고서는 GPT-3를 활용한 다양한 서비스 및 애플리케이션 사례를 소개하고, GPT-3가 사회에 미치는 긍정적/부정적 영향을 분석하여, GPT-3 기술의 발전과 함께 고려해야 할 윤리적, 사회적 과제를 제시합니다.\n",
            "\n",
            "**1. GPT-3를 활용한 다양한 서비스 및 애플리케이션 사례**\n",
            "\n",
            "GPT-3는 텍스트 생성 능력 외에도 다양한 기능을 통해 여러 분야에서 활용되고 있습니다. 주요 활용 사례는 다음과 같습니다.\n",
            "\n",
            "* **콘텐츠 생성:**\n",
            "    * **블로그 게시물, 기사 작성:** 특정 주제에 대한 블로그 게시물이나 기사를 자동으로 생성하여 콘텐츠 제작 시간을 단축하고 비용을 절감합니다.\n",
            "    * **마케팅 카피 작성:** 광고 문구, 제품 설명, 이메일 마케팅 콘텐츠 등 다양한 마케팅 카피를 생성하여 마케팅 효율성을 향상시킵니다.\n",
            "    * **시나리오, 소설, 시 등 창작:** GPT-3는 창작 활동에도 활용될 수 있으며, 아이디어 발상, 초고 작성 등 창작 과정을 지원합니다.\n",
            "    * **코드 생성:** 간단한 코드를 생성하거나, 기존 코드를 수정하는 데 활용될 수 있습니다. 개발자의 생산성을 향상시키고, 프로그래밍 지식이 부족한 사람도 코드를 생성할 수 있도록 돕습니다.\n",
            "* **고객 서비스:**\n",
            "    * **챗봇 구축:** GPT-3 기반 챗봇은 자연스러운 대화 능력을 통해 고객 문의에 응답하고 문제 해결을 지원합니다. 24시간 응대가 가능하며, 다양한 언어를 지원하여 글로벌 고객 서비스 제공에 용이합니다.\n",
            "    * **FAQ 자동 생성:** 자주 묻는 질문(FAQ)을 자동으로 생성하여 고객 지원 효율성을 높입니다.\n",
            "* **교육:**\n",
            "    * **맞춤형 학습 자료 생성:** 학생의 수준과 관심사에 맞는 맞춤형 학습 자료를 생성하여 학습 효과를 높입니다.\n",
            "    * **자동 채점 및 피드백:** 에세이, 논문 등 주관식 답안을 자동으로 채점하고 피드백을 제공하여 교사의 업무 부담을 줄입니다.\n",
            "    * **언어 학습 지원:** 외국어 학습 시, GPT-3는 번역, 문법 교정, 대화 연습 등 다양한 방식으로 학습을 지원합니다.\n",
            "* **연구 및 개발:**\n",
            "    * **데이터 분석 및 요약:** 방대한 텍스트 데이터를 분석하고 핵심 내용을 요약하여 연구자의 분석 시간을 단축합니다.\n",
            "    * **가설 생성 및 검증:** 연구 주제에 대한 가설을 생성하고 관련 정보를 검색하여 가설의 타당성을 검증하는 데 활용됩니다.\n",
            "* **기타:**\n",
            "    * **이메일 자동 작성:** 업무 관련 이메일을 자동으로 작성하여 업무 효율성을 높입니다.\n",
            "    * **번역:** 다양한 언어 간 번역을 수행하며, 특히 문맥을 이해하고 자연스러운 번역을 제공하는 데 강점을 보입니다.\n",
            "    * **게임 개발:** 게임 내 캐릭터의 대사를 생성하거나, 게임 스토리를 개발하는 데 활용됩니다.\n",
            "\n",
            "**2. GPT-3가 사회에 미치는 긍정적 영향**\n",
            "\n",
            "GPT-3 기술의 발전은 사회 전반에 걸쳐 다음과 같은 긍정적인 영향을 미칠 수 있습니다.\n",
            "\n",
            "* **생산성 향상:** 콘텐츠 생성, 코드 작성, 고객 서비스 등 다양한 분야에서 업무 효율성을 높여 생산성을 향상시킵니다.\n",
            "* **접근성 향상:** 프로그래밍 지식이 부족한 사람도 코드를 생성하거나, 외국어 학습을 지원받을 수 있도록 하여 정보 접근성을 높입니다.\n",
            "* **창의성 증진:** 아이디어 발상, 초고 작성 등 창작 과정을 지원하여 창의적인 활동을 촉진합니다.\n",
            "* **새로운 비즈니스 기회 창출:** GPT-3를 활용한 새로운 서비스 및 애플리케이션 개발을 통해 새로운 비즈니스 기회를 창출합니다.\n",
            "* **교육 기회 확대:** 맞춤형 학습 자료 제공, 자동 채점 등 교육 관련 서비스를 통해 교육 기회를 확대합니다.\n",
            "\n",
            "**3. GPT-3가 사회에 미치는 부정적 영향**\n",
            "\n",
            "---\n",
            "\n",
            "## 2.3 Codex, InstructGPT, ChatGPT 등 GPT-3 기반 모델의 등장: 특징, 성능 비교 분석 및 평가\n",
            "\n",
            "GPT-3의 등장 이후, GPT-3를 기반으로 구축된 다양한 모델들이 등장하며 자연어 처리 분야에 혁신적인 변화를 가져왔다. 이 섹션에서는 Codex, InstructGPT, ChatGPT를 중심으로 GPT-3를 기반으로 개발된 주요 모델들의 특징, 성능을 비교 분석하고, 각 모델이 가진 장단점을 평가하여 그 의미를 조명하고자 한다.\n",
            "\n",
            "**1. 모델 소개 및 개발 배경**\n",
            "\n",
            "* **Codex:** OpenAI에서 2021년 공개한 Codex는 GPT-3를 기반으로 프로그래밍 언어에 특화된 모델이다. GPT-3의 방대한 텍스트 데이터 학습에 더해 GitHub의 공개 코드 데이터를 추가적으로 학습하여 자연어 설명을 코드로 변환하는 능력을 극대화했다. 개발 배경은 자연어 기반의 코딩 자동화, 코드 생성, 코드 이해 등의 문제를 해결하고자 하는 OpenAI의 목표를 반영한다.\n",
            "* **InstructGPT:** OpenAI에서 2022년 공개한 InstructGPT는 GPT-3의 성능을 유지하면서도 인간의 지시(instruction)에 더 잘 따르는 것을 목표로 개발되었다. GPT-3의 주요 문제점이었던 '환각(hallucination)' 현상, 즉 사실과 다른 내용을 생성하는 문제를 해결하고, 인간의 의도에 부합하는 답변을 생성하는 데 초점을 맞췄다.\n",
            "* **ChatGPT:** OpenAI에서 2022년 12월 공개한 ChatGPT는 GPT-3.5를 기반으로 구축된 대화형 AI 모델이다. 인간과의 자연스러운 대화를 목표로 개발되었으며, 대화 맥락을 이해하고 유지하며, 질문에 대한 답변, 글쓰기, 번역 등 다양한 작업을 수행할 수 있다. 특히 강화 학습 기반의 인간 피드백(Reinforcement Learning from Human Feedback, RLHF)을 통해 성능을 향상시켰다는 특징을 가진다.\n",
            "\n",
            "**2. 모델 특징 비교**\n",
            "\n",
            "| 특징 | Codex | InstructGPT | ChatGPT |\n",
            "|---|---|---|---|\n",
            "| **기반 모델** | GPT-3 | GPT-3 | GPT-3.5 |\n",
            "| **주요 목적** | 코드 생성 및 이해 | 인간 지시(instruction) 준수 | 자연스러운 대화 |\n",
            "| **학습 데이터** | 텍스트 데이터 + GitHub 공개 코드 데이터 | 텍스트 데이터 + 인간 지시 및 피드백 데이터 | 텍스트 데이터 + 대화 데이터 + RLHF 데이터 |\n",
            "| **강조점** | 코드 생성 정확도 및 효율성 | 답변의 정확성 및 일관성 | 대화의 자연스러움 및 맥락 유지 |\n",
            "| **사용 분야** | 코드 자동 완성, 코드 생성, 코드 이해, 프로그래밍 교육 | 챗봇, 가상 비서, 지시 기반 작업 수행 | 챗봇, 고객 지원, 콘텐츠 생성, 교육 |\n",
            "| **API 제공 여부** | 제공 | 제공 | 제공 |\n",
            "\n",
            "**3. 모델 성능 비교 및 평가**\n",
            "\n",
            "* **Codex:**\n",
            "    * **장점:** 자연어 설명을 코드로 변환하는 능력에서 뛰어난 성능을 보인다. 다양한 프로그래밍 언어를 지원하며, 복잡한 코드도 생성할 수 있다. 코드 자동 완성 기능은 개발 생산성을 크게 향상시킨다.\n",
            "    * **단점:** 생성된 코드의 정확성을 보장하기 어려울 수 있으며, 오류가 발생할 경우 디버깅이 필요하다. 자연어 설명의 모호성으로 인해 예상치 못한 코드가 생성될 수 있다.\n",
            "* **InstructGPT:**\n",
            "    * **장점:** GPT-3에 비해 인간의 지시를 더 잘 따르며, 환각 현상이 줄어들어 답변의 신뢰도가 높아졌다. 답변의 일관성을 유지하며, 인간의 의도에 부합하는 답변을 제공한다.\n",
            "    * **단점:** GPT-3에 비해 창의성이 다소 감소할 수 있으며, 특정 분야에 대한 전문적인 지식은 부족할 수 있다.\n",
            "* **ChatGPT:**\n",
            "    * **장점:** 자연스러운 대화 능력이 뛰어나며, 대화 맥락을 잘 이해하고 유지한다. 다양한 주제에 대한 답변을 제공하며, 글쓰기, 번역 등 다양한 작업을 수행할 수 있다. RLHF를 통해 인간의 선호도에 맞게 답변을 생성한다.\n",
            "    * **단점:** 여전히 환각 현상이 발생할 수 있으며, 답변의 정확성을 보장하기 어려울 수 있다. 윤리적인 문제 (혐오 발언, 편향된 정보 등) 발생 가능성이 존재한다.\n",
            "\n",
            "**4. 각 모델의 차별화 전략 및 미래 전망**\n",
            "\n",
            "* **Codex:** 코드 생성\n",
            "\n",
            "---\n",
            "\n",
            "## 3장: 최신 LLM의 발전 동향 및 미래 전망 (2023-현재) - GPT-4, Gemini, Claude 등\n",
            "\n",
            "**1. 서론: LLM의 급격한 진화와 2023년 이후의 변화**\n",
            "\n",
            "2023년 이후, 대규모 언어 모델(Large Language Models, LLMs) 분야는 이전과는 비교할 수 없을 정도로 빠른 속도로 발전하고 있습니다. GPT-3의 등장 이후, OpenAI의 GPT-4, Google의 Gemini, Anthropic의 Claude 등 더욱 강력하고 다재다능한 LLM들이 잇따라 공개되면서 LLM 기술은 실용적인 수준으로 빠르게 도약했습니다. 본 장에서는 이러한 최신 LLM들의 등장 배경, 핵심 아키텍처 특징, 성능 향상 요인들을 분석하고, LLM의 미래 발전 방향과 함께 고려해야 할 윤리적 문제들을 심층적으로 논의합니다.\n",
            "\n",
            "**2. 주요 LLM 등장 배경 및 경쟁 구도**\n",
            "\n",
            "* **OpenAI - GPT-4:** GPT-3의 성공에 힘입어 OpenAI는 2023년 GPT-4를 공개했습니다. GPT-4는 텍스트뿐만 아니라 이미지 입력도 지원하는 멀티모달(Multimodal) 기능을 갖추고 있으며, GPT-3보다 더욱 복잡하고 미묘한 지시사항을 이해하고 수행할 수 있는 능력을 보여주었습니다. 이는 OpenAI의 독점적인 위치를 더욱 공고히 하는 요인이 되었습니다.\n",
            "* **Google - Gemini:** Google은 GPT-4에 대응하기 위해 Gemini를 개발했습니다. Gemini는 Ultra, Pro, Nano 세 가지 버전으로 출시되었으며, 특히 Ultra 버전은 다양한 벤치마크 테스트에서 GPT-4를 능가하는 성능을 보여주며 LLM 경쟁에 새로운 동력을 불어넣었습니다. Gemini의 특징은 처음부터 멀티모달 모델로 설계되었다는 점이며, 이는 텍스트, 이미지, 오디오, 비디오 등 다양한 형태의 데이터를 통합적으로 처리할 수 있는 잠재력을 시사합니다.\n",
            "* **Anthropic - Claude:** Anthropic은 안전하고 윤리적인 AI 개발을 목표로 Claude 모델을 개발했습니다. Claude는 긴 컨텍스트를 처리하는 능력과 인간과 유사한 대화 능력을 갖추고 있으며, 특히 기업 환경에서 활용될 수 있는 보안 및 규정 준수 기능을 강화했습니다. Anthropic은 ‘Constitutional AI’라는 독특한 접근 방식을 통해 모델의 행동을 제어하고 안전성을 확보하고 있습니다.\n",
            "\n",
            "이러한 LLM들의 등장은 AI 시장의 경쟁을 더욱 심화시키고 있으며, 각 기업들은 성능 향상뿐만 아니라 안전성, 윤리성, 비용 효율성 등 다양한 측면에서 경쟁하고 있습니다.\n",
            "\n",
            "**3. 아키텍처 특징 및 성능 향상 요인**\n",
            "\n",
            "* **모델 규모:** LLM의 성능 향상은 모델 규모의 증가와 밀접하게 관련되어 있습니다. GPT-4, Gemini, Claude 등 최신 LLM들은 수조 개 이상의 파라미터를 가지고 있으며, 이는 더욱 복잡한 패턴을 학습하고 이해할 수 있게 합니다. 하지만 모델 규모의 증가는 막대한 컴퓨팅 자원과 에너지 소비를 필요로 하는 문제점을 야기합니다.\n",
            "* **트랜스포머 아키텍처의 발전:** LLM은 대부분 트랜스포머 아키텍처를 기반으로 합니다. 최신 LLM들은 트랜스포머 아키텍처의 효율성을 높이기 위해 다양한 기술들을 적용하고 있습니다. 예를 들어, Sparse Attention, FlashAttention 등은 계산 복잡도를 줄이고 메모리 사용량을 최적화하여 모델의 확장성을 높입니다.\n",
            "* **멀티모달 학습:** GPT-4와 Gemini는 멀티모달 학습을 통해 텍스트뿐만 아니라 이미지, 오디오, 비디오 등 다양한 형태의 데이터를 처리할 수 있게 되었습니다. 이는 LLM의 활용 범위를 크게 확장시키고 있으며, 더욱 자연스럽고 지능적인 상호 작용을 가능하게 합니다.\n",
            "* **강화 학습 (Reinforcement Learning):** Anthropic의 Claude는 ‘Constitutional AI’라는 독특한 접근 방식을 통해 강화 학습을 활용하여 모델의 행동을 제어하고 안전성을 확보하고 있습니다. 이는 LLM의 윤리적 문제 해결에 새로운 가능성을 제시합니다.\n",
            "* **데이터 품질 및 양:** LLM의 성능은 학습 데이터의 품질과 양에 크게 의존합니다. 최신 LLM들은 더욱 방대한 양의 데이터를 수집하고, 데이터의 품질을 높이기 위한 노력을 기울이고 있습니다.\n",
            "\n",
            "**4. LLM의 미래 발전 방향**\n",
            "\n",
            "* **모델 경량화 (Model Compression):** LLM의 규모가 커짐에 따라, 모델 경량화 기술의 중요성이 더욱 커지고 있습니다.\n",
            "\n",
            "---\n",
            "\n",
            "## 3.1 GPT-4, Gemini, Claude 등 최신 LLM의 등장과 경쟁\n",
            "\n",
            "**1. 서론: LLM 발전의 가속화와 새로운 경쟁 구도**\n",
            "\n",
            "최근 몇 년간 자연어 처리(NLP) 분야는 거대한 규모의 언어 모델(Large Language Models, LLMs)의 등장으로 혁신적인 발전을 이루어냈습니다. GPT-3를 기점으로 LLM은 텍스트 생성, 번역, 질의 응답, 코드 생성 등 다양한 작업에서 인간에 가까운 성능을 보여주며, 인공지능 기술의 가능성을 획기적으로 확장했습니다. 이러한 추세에 발맞춰 OpenAI의 GPT-4, Google의 Gemini, Anthropic의 Claude 등 더욱 강력하고 다재다능한 LLM들이 등장하며, LLM 시장의 경쟁 구도는 더욱 치열해지고 있습니다. 본 보고서는 이러한 최신 LLM들의 등장 배경, 주요 특징, 모델 간 성능 비교, 그리고 경쟁 구도에 대해 심층적으로 분석하고자 합니다.\n",
            "\n",
            "**2. 주요 LLM 소개 및 특징**\n",
            "\n",
            "* **GPT-4 (OpenAI):**\n",
            "    * **등장 배경:** GPT-3의 한계를 극복하고, 더욱 복잡한 추론 능력과 창의성을 확보하기 위해 개발되었습니다. OpenAI는 GPT-4의 정확한 규모를 공개하지 않았지만, GPT-3보다 훨씬 많은 데이터와 파라미터를 사용했을 것으로 추정됩니다.\n",
            "    * **주요 특징:**\n",
            "        * **다중 모달(Multimodal) 지원:** 텍스트뿐만 아니라 이미지 입력도 지원하여, 이미지에 대한 질문에 답변하거나 이미지 기반의 작업을 수행할 수 있습니다.\n",
            "        * **향상된 추론 능력:** 복잡한 문제 해결, 논리적 사고, 창의적인 글쓰기 등 다양한 분야에서 GPT-3보다 뛰어난 성능을 보입니다.\n",
            "        * **안전성 및 책임감 강화:** 유해 콘텐츠 생성 가능성을 줄이고, 편향성을 완화하기 위한 노력이 반영되었습니다.\n",
            "        * **API 접근성:** 개발자를 위한 API를 제공하여 다양한 애플리케이션 개발에 활용될 수 있도록 지원합니다.\n",
            "* **Gemini (Google):**\n",
            "    * **등장 배경:** Google은 자체 개발한 TPU(Tensor Processing Unit)를 활용하여 LLM 개발을 가속화하고, GPT-4에 대응하기 위해 Gemini를 출시했습니다.\n",
            "    * **주요 특징:**\n",
            "        * **다양한 크기(Size)의 모델 제공:** Nano, Pro, Ultra 등 다양한 규모의 모델을 제공하여, 다양한 환경과 요구사항에 맞게 활용할 수 있습니다.\n",
            "        * **다중 모달(Multimodal) 지원:** 텍스트, 이미지, 오디오, 비디오 등 다양한 형태의 데이터를 이해하고 처리할 수 있습니다.\n",
            "        * **TPU 최적화:** Google의 TPU를 활용하여 뛰어난 성능과 효율성을 제공합니다.\n",
            "        * **Google 서비스 통합:** Google 검색, Gmail, Google Workspace 등 Google의 다양한 서비스와 통합되어 시너지 효과를 창출합니다.\n",
            "* **Claude (Anthropic):**\n",
            "    * **등장 배경:** OpenAI의 GPT-3를 개발했던 핵심 인력들이 설립한 Anthropic에서 개발한 LLM입니다. 안전하고 윤리적인 AI 개발을 목표로 하며, 'Constitutional AI'라는 독특한 접근 방식을 사용합니다.\n",
            "    * **주요 특징:**\n",
            "        * **Constitutional AI:** 미리 정의된 윤리적 원칙(헌법)에 따라 답변을 생성하여, 유해하거나 편향된 답변을 줄이도록 설계되었습니다.\n",
            "        * **긴 컨텍스트 창(Context Window):** 긴 텍스트를 한 번에 처리할 수 있어, 복잡한 문서 요약, 코드 분석, 장문의 대화 등에 유리합니다.\n",
            "        * **안전성 및 윤리성:** 유해 콘텐츠 생성 가능성을 최소화하고, 편향성을 완화하기 위한 노력을 지속적으로 기울이고 있습니다.\n",
            "\n",
            "**3. 모델 간 성능 비교**\n",
            "\n",
            "각 모델의 성능은 다양한 벤치마크 테스트를 통해 비교 분석될 수 있습니다. 하지만, LLM의 성능은 평가 방식, 데이터셋, 작업의 종류에 따라 달라질 수 있습니다. 현재까지의 평가 결과는 다음과 같습니다.\n",
            "\n",
            "| 특징 | GPT-4 | Gemini (Ultra) | Claude 3 (Opus) |\n",
            "|---|---|---|---|\n",
            "| **추론 능력** | 매우 우수, 복잡한 문제 해결에 강점 | 우수, 특히 수학 및 과학 분야에서 강점 | 매우 우수, 논리적 추론 및 코딩 능력 뛰어남 |\n",
            "| **다중 모달** | 이미지 입력 지원 | 텍스트, 이미지, 오디오, 비디오 지원 | \n",
            "\n",
            "---\n",
            "\n",
            "## 3.2 멀티모달 LLM의 발전과 가능성: 텍스트를 넘어 다양한 데이터를 이해하고 활용하는 미래\n",
            "\n",
            "**1. 서론: 멀티모달 LLM의 등장 배경과 의미**\n",
            "\n",
            "최근 몇 년간 챗GPT와 같은 거대 언어 모델(LLM)의 급격한 발전은 인공지능 분야에 혁신적인 변화를 가져왔습니다. 그러나 LLM은 기본적으로 텍스트 데이터에 특화되어 있어, 현실 세계의 복잡하고 다양한 정보를 처리하는 데 한계점을 드러냈습니다. 이에 따라 인간의 인지 방식처럼 텍스트뿐만 아니라 이미지, 오디오, 비디오 등 다양한 형태의 데이터를 이해하고 상호 작용하는 **멀티모달 LLM(Multimodal Large Language Model)**이 주목받기 시작했습니다. 멀티모달 LLM은 단순히 여러 모달리티를 결합하는 것을 넘어, 각 모달리티 간의 관계를 파악하고 통합적인 이해를 가능하게 함으로써 더욱 지능적인 AI 시스템 구축의 가능성을 열고 있습니다.\n",
            "\n",
            "**2. 멀티모달 LLM의 발전 현황**\n",
            "\n",
            "멀티모달 LLM의 발전은 크게 세 가지 방향으로 진행되고 있습니다.\n",
            "\n",
            "*   **기존 LLM에 모달리티 추가:** GPT-4, Gemini, LLaVA와 같이 기존 LLM에 이미지 처리 능력을 추가하여 텍스트와 이미지를 함께 이해하고 생성하는 방식으로 발전해 왔습니다. 이러한 모델들은 이미지 캡셔닝, 시각적 질문 답변(VQA), 이미지 기반 텍스트 생성 등 다양한 작업을 수행할 수 있습니다.\n",
            "*   **모달리티 간 연결에 특화된 모델 개발:** CLIP(Contrastive Language-Image Pre-training)과 같은 모델은 텍스트와 이미지를 연결하는 데 특화되어 있으며, 이미지 검색, 이미지 분류 등 다양한 분야에서 활용되고 있습니다. 이러한 모델들은 텍스트 설명과 이미지 간의 의미적 유사성을 학습하여 효과적인 연결을 가능하게 합니다.\n",
            "*   **오디오, 비디오 등 다양한 모달리티 통합:** 최근에는 오디오, 비디오 등 다양한 모달리티를 통합하여 더욱 복잡한 작업을 수행할 수 있는 모델들이 등장하고 있습니다. 예를 들어, 텍스트와 오디오를 함께 처리하여 음성 기반 질문 답변, 비디오 캡셔닝, 비디오 기반 텍스트 생성 등을 가능하게 합니다.\n",
            "\n",
            "**주요 멀티모달 LLM 모델 및 특징:**\n",
            "\n",
            "| 모델명 | 주요 특징 | 활용 분야 |\n",
            "|---|---|---|\n",
            "| **GPT-4** | 이미지 입력 가능, 텍스트 기반 추론 및 생성 능력 우수 | 시각적 질문 답변, 이미지 기반 텍스트 생성, 콘텐츠 요약 |\n",
            "| **Gemini** | 텍스트, 이미지, 오디오, 비디오 등 다양한 모달리티 처리 가능, Google의 다양한 서비스 통합 예정 | 검색, 콘텐츠 생성, 자동화된 작업 |\n",
            "| **LLaVA** | 오픈소스, 이미지 기반 질문 답변, 이미지 설명 생성 | 연구 및 개발, 교육 |\n",
            "| **CLIP** | 텍스트와 이미지의 의미적 연결에 특화 | 이미지 검색, 이미지 분류, 콘텐츠 추천 |\n",
            "| **Flamingo** | 이미지와 텍스트를 결합하여 복잡한 추론 가능 | 시각적 질문 답변, 이미지 기반 스토리텔링 |\n",
            "\n",
            "**3. 멀티모달 LLM의 가능성**\n",
            "\n",
            "멀티모달 LLM은 다양한 분야에서 혁신적인 가능성을 제시하고 있습니다.\n",
            "\n",
            "*   **교육:** 개인 맞춤형 학습 콘텐츠 제공, 시각 자료를 활용한 학습 지원, 학생들의 질문에 시각적으로 답변하는 튜터 역할 수행\n",
            "*   **의료:** 의료 영상 분석 및 진단 지원, 환자의 증상에 대한 시각적 설명 제공, 의료 기록 요약 및 질병 예측\n",
            "*   **콘텐츠 제작:** 이미지 기반 스토리텔링, 자동 비디오 캡셔닝, 텍스트 기반 이미지 생성, 음악 작곡 및 가사 생성\n",
            "*   **접근성 향상:** 시각 장애인을 위한 이미지 설명 제공, 청각 장애인을 위한 텍스트 기반 오디오 설명 제공\n",
            "*   **로봇 공학:** 로봇의 시각적 환경 이해 및 상호 작용, 음성 명령과 시각적 정보를 결합한 작업 수행\n",
            "*   **검색:** 텍스트, 이미지, 오디오, 비디오를 통합하여 더욱 정확하고 풍부한 검색 결과 제공\n",
            "*   **고객 서비스:** 텍스트, 이미지, 오디오를 결합하여 더욱 효율적이고 만족스러운 고객 지원 제공\n",
            "\n",
            "**4. 멀티모달 LLM의 도전 과제**\n",
            "\n",
            "멀티모달 LLM의 발전은 많은 가능성을 제시하지만, 해결해야 할 과제도 존재\n",
            "\n",
            "---\n",
            "\n",
            "## 3.3 LLM의 윤리적 문제와 안전성 확보 방안\n",
            "\n",
            "**1. 서론: LLM의 급격한 발전과 윤리적 책임의 중요성**\n",
            "\n",
            "대규모 언어 모델(LLM, Large Language Model)은 챗봇, 콘텐츠 생성, 번역 등 다양한 분야에서 혁신적인 가능성을 보여주며 빠르게 발전하고 있습니다. 하지만 LLM의 성능이 향상될수록, 그에 따른 윤리적 문제와 안전성에 대한 우려 또한 증폭되고 있습니다. LLM은 사회에 긍정적인 영향을 미칠 수 있지만, 동시에 편향성 심화, 허위 정보 확산, 악의적 사용 등 심각한 문제를 야기할 수 있기 때문입니다. 본 보고서는 LLM이 야기하는 주요 윤리적 문제점을 심층적으로 분석하고, 이러한 문제점을 해결하고 LLM의 안전성을 확보하기 위한 기술적, 정책적 방안을 제시합니다.\n",
            "\n",
            "**2. LLM의 윤리적 문제점**\n",
            "\n",
            "LLM은 방대한 양의 데이터를 기반으로 학습하기 때문에, 데이터에 내재된 편향성을 그대로 반영할 가능성이 높습니다. 이는 다양한 윤리적 문제로 이어질 수 있습니다.\n",
            "\n",
            "* **2.1 편향성 (Bias):**\n",
            "    * **데이터 편향:** LLM 학습에 사용되는 데이터는 사회적, 문화적 맥락에 따라 특정 집단에 대한 편향된 정보를 담고 있을 수 있습니다. 이러한 데이터 편향은 LLM이 특정 성별, 인종, 종교, 정치적 견해 등에 대해 차별적이거나 불공정한 답변을 생성하도록 만들 수 있습니다. 예를 들어, 특정 직업을 언급할 때 남성 위주로 답변을 생성하거나, 특정 인종에 대한 부정적인 고정관념을 강화하는 답변을 생성할 수 있습니다.\n",
            "    * **알고리즘 편향:** LLM의 학습 알고리즘 자체가 특정 결과를 선호하도록 설계될 수 있습니다. 이는 의도치 않게 특정 집단에 불리한 결과를 초래할 수 있습니다.\n",
            "    * **결과 편향:** LLM이 생성하는 결과물은 학습 데이터와 알고리즘의 영향을 받아 편향된 시각을 반영할 수 있습니다.\n",
            "* **2.2 허위 정보 생성 (Hallucination & Fabrication):**\n",
            "    * LLM은 사실과 다른 정보를 마치 사실인 것처럼 제시하는 '환각(Hallucination)' 현상을 보일 수 있습니다. 이는 LLM이 학습 데이터에서 패턴을 학습하는 과정에서 발생하는 문제로, 사실 확인 없이 정보를 생성하는 경향을 나타냅니다.\n",
            "    * LLM은 존재하지 않는 정보를 '조작(Fabrication)'하여 생성할 수도 있습니다. 이는 LLM이 지식의 한계를 인지하지 못하고, 부족한 정보를 채우기 위해 가짜 정보를 생성하는 결과입니다.\n",
            "    * 이러한 허위 정보 생성은 사회적 혼란을 야기하고, 개인의 명예를 훼손하며, 심각한 사회적 문제를 초래할 수 있습니다.\n",
            "* **2.3 악용 가능성 (Potential for Misuse):**\n",
            "    * **사기 및 스미싱:** LLM을 이용하여 정교한 사기 및 스미싱 메시지를 생성하여 개인 정보를 탈취하거나 금전적인 피해를 입힐 수 있습니다.\n",
            "    * **가짜 뉴스 생성 및 유포:** LLM을 이용하여 정치적 선전, 허위 사실 유포 등 가짜 뉴스를 생성하고 유포하여 사회적 혼란을 야기할 수 있습니다.\n",
            "    * **악성 코드 생성:** LLM을 이용하여 악성 코드를 생성하고 유포하여 사이버 범죄를 저지를 수 있습니다.\n",
            "    * **딥페이크 제작:** LLM과 결합하여 딥페이크를 제작하여 개인의 명예를 훼손하거나 사회적 혼란을 야기할 수 있습니다.\n",
            "\n",
            "**3. LLM의 안전성 확보 방안**\n",
            "\n",
            "LLM의 윤리적 문제점을 해결하고 안전성을 확보하기 위해서는 기술적, 정책적 방안을 종합적으로 적용해야 합니다.\n",
            "\n",
            "* **3.1 기술적 방안:**\n",
            "    * **데이터 편향 완화:**\n",
            "        * **데이터 다양성 확보:** 다양한 인구 집단, 문화, 관점을 반영하는 데이터를 수집하여 학습 데이터의 다양성을 확보해야 합니다.\n",
            "        * **데이터 증강 (Data Augmentation):** 기존 데이터에 변형을 가하거나 새로운 데이터를 생성하여 데이터셋의 불균형을 해소해야 합니다.\n",
            "        * **편향 완화 알고리즘 개발:** 학습 데이터의 편향성을 감지하고 완화하는 알고리즘을 개발해야 합니다.\n",
            "    * **허위 정보 생성 방지:**\n",
            "        * **사실 확인 (Fact-Checking) 시스템 통합:** LL\n",
            "\n",
            "---\n",
            "\n",
            "## 3.4 LLM의 미래 발전 방향: AGI(Artificial General Intelligence)를 향하여\n",
            "\n",
            "**서론**\n",
            "\n",
            "최근 거대한 언어 모델(LLM, Large Language Model)의 발전은 인공지능 분야에 혁명적인 변화를 가져왔습니다. GPT-3, LaMDA, PaLM 등 LLM들은 인간과 유사한 텍스트 생성 능력, 번역, 질의응답 등 다양한 작업을 수행하며 그 가능성을 입증했습니다. 하지만 LLM은 여전히 특정 작업에 특화된 'Narrow AI'의 범주에 속하며, 진정한 의미의 인공지능, 즉 인간과 동등하거나 그 이상의 지적 능력을 갖춘 AGI(Artificial General Intelligence)에는 미치지 못합니다. 본 보고서는 LLM의 미래 발전 방향을 제시하고, AGI를 향한 연구 개발 동향을 전망하며, LLM이 사회에 미칠 영향에 대해 심층적으로 논의합니다.\n",
            "\n",
            "**1. LLM의 한계와 극복 과제**\n",
            "\n",
            "현재 LLM은 다음과 같은 근본적인 한계를 가지고 있습니다.\n",
            "\n",
            "* **이해력 부족:** LLM은 텍스트 패턴을 학습하여 텍스트를 생성하지만, 텍스트의 의미를 진정으로 이해한다고 보기 어렵습니다. 이는 LLM이 맥락을 파악하지 못하거나, 상식적인 추론을 수행하는 데 어려움을 겪는 이유입니다.\n",
            "* **환각(Hallucination) 현상:** LLM은 사실과 다른 정보를 마치 사실인 것처럼 생성하는 '환각' 현상을 보입니다. 이는 학습 데이터의 편향성, 불충분한 지식, 추론 능력 부족 등 다양한 원인에 기인합니다.\n",
            "* **상식과 추론 능력 부족:** LLM은 인간의 상식적인 지식이나 추론 능력을 갖추지 못하여 복잡한 문제 해결에 어려움을 겪습니다.\n",
            "* **데이터 의존성:** LLM은 방대한 양의 데이터를 필요로 하며, 학습 데이터에 편향될 가능성이 높습니다.\n",
            "* **계산 비용:** LLM의 학습 및 운영에는 막대한 계산 비용이 소요됩니다.\n",
            "\n",
            "이러한 한계를 극복하기 위해 다음과 같은 연구가 진행되고 있습니다.\n",
            "\n",
            "* **지식 주입(Knowledge Injection):** 외부 지식 베이스를 LLM에 통합하여 지식의 정확성과 폭을 넓히는 연구입니다.\n",
            "* **강화 학습(Reinforcement Learning):** 인간의 피드백을 통해 LLM의 성능을 개선하는 연구입니다.\n",
            "* **멀티모달 학습(Multimodal Learning):** 텍스트뿐만 아니라 이미지, 오디오, 비디오 등 다양한 형태의 데이터를 함께 학습하여 LLM의 이해력을 높이는 연구입니다.\n",
            "* **추론 능력 강화:** 논리적 추론, 상식 추론 등 LLM의 추론 능력을 향상시키기 위한 연구입니다.\n",
            "* **설명 가능한 AI(Explainable AI, XAI):** LLM의 의사 결정 과정을 설명 가능하게 만들어 신뢰도를 높이는 연구입니다.\n",
            "\n",
            "**2. AGI를 향한 연구 개발 동향**\n",
            "\n",
            "AGI는 LLM의 발전 방향을 넘어, 인공지능 연구의 궁극적인 목표입니다. AGI를 향한 연구는 다음과 같은 방향으로 진행되고 있습니다.\n",
            "\n",
            "* **신경망 아키텍처 혁신:** Transformer 기반의 LLM 외에도 새로운 신경망 아키텍처를 개발하여 LLM의 한계를 극복하려는 시도가 이루어지고 있습니다.\n",
            "* **자기 지도 학습(Self-Supervised Learning) 발전:** 레이블이 없는 데이터로부터 스스로 학습하는 자기 지도 학습의 효율성을 높이는 연구가 진행되고 있습니다.\n",
            "* **인공 일반 지능(Artificial General Intelligence, AGI) 프레임워크 개발:** AGI의 핵심 구성 요소를 정의하고, 이를 구현하기 위한 프레임워크를 개발하는 연구가 진행되고 있습니다.\n",
            "* **신경-상징적 AI(Neuro-Symbolic AI):** 신경망의 패턴 인식 능력과 상징적 추론의 논리적 추론 능력을 결합하여 AGI를 구현하려는 연구입니다.\n",
            "* **강화 학습의 발전:** 복잡한 환경에서 스스로 학습하고 문제를 해결하는 강화 학습 알고리즘의 성능을 향상시키는 연구입니다.\n",
            "\n",
            "**3. LLM이 사회에 미칠 영향**\n",
            "\n",
            "LLM의 발전은 사회 전반에 걸쳐 광범위한 영향을 미칠 것으로 예상됩니다.\n",
            "\n",
            "* **긍정적인 영향:**\n",
            "    * **업무 효율성 증대:** LLM은 문서 작성, 번역, 고객 응대 등 다양한 업무를 자동화하여 업무 효율성을 증대시킬 수 있습니다.\n",
            "    * **새로운 산업\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import Markdown\n",
        "print(data['synthesizer'][\"final_report\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZp83TwNChSh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAWfvZKOrMCl"
      },
      "source": [
        "# [실습] Agent의 Planning ReAct 알아보기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTwBtAVvrMCn"
      },
      "source": [
        "`ToolNode`와 `ToolCondition`을 통해 툴을 실행하는 에이전트 구조를 만들어 보았습니다.\n",
        "\n",
        "이 구조는 랭그래프에서 ReAct 에이전트라는 형태로 구현이 되어 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjgQZudIrMCn"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade langgraph langchain langchain_google_genai langchain_community langchain_experimental"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "API 키와 LLM을 설정합니다."
      ],
      "metadata": {
        "id": "BUoU_6-4sTaI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvTKbjPJrMCo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['GOOGLE_API_KEY'] = ''\n",
        "\n",
        "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Gemini API는 분당 10개 요청으로 제한\n",
        "# 즉, 초당 약 0.167개 요청 (10/60)\n",
        "rate_limiter = InMemoryRateLimiter(\n",
        "    requests_per_second=0.167,  # 분당 10개 요청\n",
        "    check_every_n_seconds=0.1,  # 100ms마다 체크\n",
        "    max_bucket_size=10,  # 최대 버스트 크기\n",
        ")\n",
        "\n",
        "# rate limiter를 LLM에 적용\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash-exp\",\n",
        "    rate_limiter=rate_limiter\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM에서 사용할 툴을 설정합니다."
      ],
      "metadata": {
        "id": "EpL8cPyWsaJn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXQ5wE51rMCp"
      },
      "outputs": [],
      "source": [
        "# Tavily API\n",
        "os.environ['TAVILY_API_KEY'] = ''\n",
        "\n",
        "\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_core.tools import tool\n",
        "import random\n",
        "\n",
        "tavily_search = TavilySearchResults(\n",
        "    max_results=5)\n",
        "\n",
        "@tool\n",
        "def current_date() -> str:\n",
        "    \"현재 날짜를 %y-%m-%d 형식으로 반환합니다.\"\n",
        "    from datetime import datetime\n",
        "    return datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "\n",
        "@tool\n",
        "def counsel(problem:str) -> str: # 나중에는 LLM 기반의 모듈이나, 개별 에이전트로 처리 가능\n",
        "    \"고민에 대한 답을 예/아니오로 얻습니다.\"\n",
        "    if random.random()>=0.5:\n",
        "        return '네!'\n",
        "    else:\n",
        "        return '아니오.'\n",
        "\n",
        "print(current_date.invoke({}))\n",
        "print(counsel.invoke({'problem':'아무 고민'}))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqnTR5farMCp"
      },
      "source": [
        "이번에는 새로운 툴을 추가해 보겠습니다.   \n",
        "`Python_REPL`은 파이썬 쉘을 이용해서 코드를 실행하는 기능으로, ChatGPT의 코드 인터프리터와 유사합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zS2G_HrrMCp"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
        "\n",
        "repl_tool = PythonREPLTool()\n",
        "repl_tool.invoke(\"for i in range(10): print(i)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "React Agent는 `langgraph.prebuilt` 에서 바로 실행할 수 있습니다."
      ],
      "metadata": {
        "id": "rjAqJiYLuAw0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJC-S7vfrMCp"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "tools = [tavily_search, current_date, counsel, repl_tool]\n",
        "\n",
        "graph = create_react_agent(llm, tools=tools)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7O1HO1COrMCp"
      },
      "outputs": [],
      "source": [
        "graph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
        "\n",
        "graph.invoke({'messages':[HumanMessage(content=\"안녕하세요?\")]})"
      ],
      "metadata": {
        "id": "m-yicm0ltSaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ctGtKv5rMCq"
      },
      "outputs": [],
      "source": [
        "for data in graph.stream(\n",
        "    {'messages':[\n",
        "        HumanMessage(content='''\n",
        "오늘 날짜 확인해서 txt 파일로 저장해, 파일 이름은 날짜.txt로 해. ''')]},\n",
        "    stream_mode='updates'):\n",
        "    print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UF3GE4mgrMCq"
      },
      "source": [
        "이번에는 Tool Node, ToolCondition 등의 Prebuilt 없이 구현해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtPHw7IRrMCq"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages : Annotated[list, add_messages]   # 메시지 맥락을 저장하는 리스트\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vE0vrzzKrMCq"
      },
      "outputs": [],
      "source": [
        "llm_with_tools = llm.bind_tools(tools)\n",
        "llm_with_tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlCCBu_drMCq"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import ToolMessage\n",
        "\n",
        "tool_list = {tool.name: tool for tool in tools}\n",
        "# tool 목록 dict로 생성\n",
        "\n",
        "def tool_node(state):\n",
        "    tool_outputs = []\n",
        "    tool_call_msgs = state['messages'][-1]\n",
        "    # 마지막 메시지: 툴 콜링 메시지\n",
        "\n",
        "    for tool_call in tool_call_msgs.tool_calls:\n",
        "    # 여러 개의 툴 콜이 필요한 경우를 고려해 for로 표시\n",
        "        tool_exec = tool_list[tool_call['name']]\n",
        "        # 실제 함수 찾기\n",
        "        tool_result = tool_exec.invoke(tool_call)\n",
        "        # tool 실행 결과 얻기 (결과는 ToolMessage 타입: 1번 실습 참고)\n",
        "        tool_outputs.append(tool_result)\n",
        "\n",
        "    return {'messages': tool_outputs}\n",
        "\n",
        "def agent(state):\n",
        "\n",
        "    # system_prompt = SystemMessage(\"주어진 툴을 사용하여, 사용자의 질문에 답하세요.\")\n",
        "\n",
        "    # ReAct 목적에 충실한 버전\n",
        "    system_prompt = SystemMessage(\"\"\"주어진 툴을 사용하여, 사용자의 질문에 답하세요.\n",
        "툴을 실행하기 전, 직전까지의 결과의 의미를 파악하고 맥락에 맞게 다음 툴을 실행하기 위한 Planning Step을 메시지에 포함해 출력하세요.\n",
        "파일 시스템 접근은 파이썬 코드를 실행하여 처리하세요.\n",
        "에러가 발생하면, 전략을 바꿔 다시 실행하세요.\"\"\")\n",
        "\n",
        "\n",
        "    response = llm_with_tools.invoke([system_prompt] + state[\"messages\"])\n",
        "    # Tool Calling이 필요한 경우와 필요하지 않은 경우를 구분할할 필요\n",
        "    return {'messages': response}\n",
        "\n",
        "def tool_needed(state):\n",
        "\n",
        "    last_msg = state['messages'][-1]\n",
        "    if last_msg.tool_calls: # 툴 콜링이 필요하면\n",
        "        return \"continue\"\n",
        "    else:\n",
        "        return \"finish\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7JY5soZrMCq"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "builder = StateGraph(State)\n",
        "\n",
        "builder.add_node(\"agent\", agent)\n",
        "builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "builder.add_edge(START, 'agent'),\n",
        "builder.add_conditional_edges(\"agent\",\n",
        "                              tool_needed,\n",
        "                               {\"continue\": \"tools\",\"finish\": END})\n",
        "builder.add_edge(\"tools\", \"agent\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoQSryTjrMCq"
      },
      "outputs": [],
      "source": [
        "graph = builder.compile()\n",
        "graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PNNpHuvrMCq"
      },
      "outputs": [],
      "source": [
        "response = graph.invoke({'messages':[HumanMessage(content=\"패스트캠퍼스 랭그래프 과정 검색해서 소개해줘.\")]})\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwQOBfaXrMCr"
      },
      "outputs": [],
      "source": [
        "for data in graph.stream(\n",
        "    {'messages':[HumanMessage(content='''\n",
        "    오늘 날짜 확인해서 txt 파일로 저장해, 파일 이름은 날짜.txt로 해.''')]}, stream_mode='updates'):\n",
        "    print(data)\n",
        "    print('----------')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JqJp4TmSvDZN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "multicampus",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

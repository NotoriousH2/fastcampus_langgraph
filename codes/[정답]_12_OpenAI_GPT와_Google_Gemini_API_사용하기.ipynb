{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a435919b",
      "metadata": {
        "id": "a435919b"
      },
      "source": [
        "# [실습] LangChain으로 OpenAI GPT와 Google Gemini API 사용하기\n",
        "\n",
        "LangChain(랭체인)은 LLM 기반의 어플리케이션을 효율적으로 개발할 수 있게 해주는 라이브러리입니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BJQjvbvK-NMZ",
      "metadata": {
        "id": "BJQjvbvK-NMZ"
      },
      "source": [
        "LangChain은 GPT, Gemini 등의 API와 HuggingFace, Ollama 등의 오픈 모델 환경 모두에서 사용할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "784f262e",
      "metadata": {
        "id": "784f262e"
      },
      "source": [
        "이번 실습에서는 대표적인 LLM인 Google Gemini와 OpenAI GPT의 API를 사용해 진행하겠습니다.    \n",
        "\n",
        "Gemini는 무료 사용량이 존재하지만, GPT는 유료 API 크레딧이 필요합니다.   \n",
        "만약 유료 크레딧이 없으신 분들은 Gemini만으로 진행해 주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf68e6c0",
      "metadata": {
        "id": "bf68e6c0",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "428eafef-10df-4c0a-93c6-f389c1a865c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.23)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.21)\n",
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.11/dist-packages (2.1.2)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.11/dist-packages (0.3.12)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.51)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.24)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.8.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (1.2.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage<0.7.0,>=0.6.16 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.6.17)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.70.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.3.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.24.2)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (5.29.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (4.13.1)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.69.2)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.71.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (4.9)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain-community langchain-google-genai langchain-openai\n",
        "# langchain-anthropic langchain-huggingface"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "babb35b6",
      "metadata": {
        "id": "babb35b6"
      },
      "source": [
        "Google Colab 환경이 아닌 경우에는, 아래 라이브러리도 설치해 주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3293eab",
      "metadata": {
        "id": "a3293eab"
      },
      "outputs": [],
      "source": [
        "# !pip install google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0471bfd",
      "metadata": {
        "id": "d0471bfd"
      },
      "source": [
        "## LLM\n",
        "\n",
        "LangChain에서, LLM을 부르는 방법은 주로 `ChatOpenAI`, `ChatGoogleGenerativeAI`와 같은 개별 클래스를 불러오거나,   \n",
        "`init_chat_model`을 통해 Provider와 모델 이름을 전달하는 방식으로 이루어집니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8e3fb01",
      "metadata": {
        "id": "c8e3fb01"
      },
      "source": [
        "### API 키 준비하기\n",
        "\n",
        "\n",
        "Google API 키를 등록하고 입력합니다.   \n",
        "구글 계정 로그인 후 https://aistudio.google.com  에 접속하면, API 키 생성이 가능합니다.   \n",
        "\n",
        "OpenAI API 키는 유료 계정 로그인 후   \n",
        "https://platform.openai.com/api-keys 에 접속하면 생성이 가능합니다.    \n",
        "(유료 계정과 무관하게, 크레딧 결제가 필요합니다.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8c1faf5",
      "metadata": {
        "id": "b8c1faf5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = \"AIzaSyBiatO0p8d6ubUnU07Q7du6MCB9HvJBsgc\"\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = \"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\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35163987",
      "metadata": {
        "id": "35163987"
      },
      "source": [
        "Google AI Studio의 `Create Prompt`에서, 모델 목록과 무료 API 사용량을 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d7356b8",
      "metadata": {
        "id": "7d7356b8"
      },
      "source": [
        "OpenAI 모델의 목록과 가격은 https://openai.com/api/pricing/ 에서 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3b4252b",
      "metadata": {
        "id": "b3b4252b"
      },
      "source": [
        "## LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36b0c4ab",
      "metadata": {
        "id": "36b0c4ab"
      },
      "source": [
        "chat 모델 사용을 위해 ChatGoogleGenerativeAI, ChatOpenAI를 불러오겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4efc9e7",
      "metadata": {
        "id": "a4efc9e7"
      },
      "source": [
        "모델마다 다른 Safety 등의 요소를 제외하고, 공통적으로 아래의 파라미터를 갖습니다.\n",
        "- model : 모델의 이름입니다.\n",
        "- temperature : 모델 출력의 무작위성을 결정합니다. 0부터 2 사이의 값을 지정할 수 있으며,   \n",
        "숫자가 클수록 무작위 출력이 증가합니다.    \n",
        "(o3-mini, o1 등의 Reasoning 모델은 지원하지 않는 경우도 있습니다)\n",
        "\n",
        "- max_tokens : 출력의 최대 길이를 지정합니다. 해당 토큰 수가 넘어가면 출력이 중간에 종료됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89b1573c",
      "metadata": {
        "id": "89b1573c"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    temperature = 0.7,\n",
        "    max_tokens = 2048\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "034c0221",
      "metadata": {
        "id": "034c0221"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm_gpt = ChatOpenAI(\n",
        "    model = 'gpt-4o-mini',\n",
        "    temperature = 1.0,\n",
        "    max_tokens = 2048\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "977ce129",
      "metadata": {
        "id": "977ce129"
      },
      "source": [
        "LangChain은 프롬프트, LLM, 체인 등의 구성 요소를 서로 연결하는 방식으로 구성됩니다.  \n",
        "각각의 요소를 `Runnable`이라고 부르는데요.   \n",
        "`Runnable`은  `invoke()`를 통해 실행합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d97eb36",
      "metadata": {
        "id": "7d97eb36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06e49abf-64ce-4d74-df14-5524260a2868"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='프롬프트 엔지니어링의 핵심적인 요소는 다음과 같이 요약할 수 있습니다:\\n\\n1.  **명확성 (Clarity):**\\n    *   프롬프트는 모호함 없이 명확하고 구체적이어야 합니다. 모델이 원하는 결과를 정확히 이해할 수 있도록 충분한 정보를 제공해야 합니다.\\n    *   애매한 단어, 중의적인 표현, 일반적인 지시를 피하고, 구체적인 요구 사항을 명시해야 합니다.\\n    *   예시: \"글을 써줘\" (X) -> \"한국어로 100자 이내의 자기소개 글을 써줘. 경력은 5년차 마케터이고, 강점은 데이터 분석 능력이야.\" (O)\\n\\n2.  **구체성 (Specificity):**\\n    *   원하는 결과물의 형식, 스타일, 길이, 어조 등을 구체적으로 지정해야 합니다.\\n    *   모델이 따라야 할 규칙이나 제약 조건을 명확하게 제시해야 합니다.\\n    *   구체적인 예시를 제공하거나, 부정적인 예시를 제시하여 모델이 잘못된 방향으로 가지 않도록 유도할 수 있습니다.\\n    *   예시: \"이야기를 써줘\" (X) -> \"주인공이 고양이인 판타지 단편 소설을 써줘. 3인칭 시점으로 쓰고, 유머러스한 분위기로 만들어줘. 500자 내외로 작성해줘.\" (O)\\n\\n3.  **맥락 (Context):**\\n    *   모델이 프롬프트의 의도를 파악하고 관련 정보를 활용할 수 있도록 충분한 배경 지식이나 상황 정보를 제공해야 합니다.\\n    *   이전 대화 내용, 사용자 프로필, 관련 문서 등을 활용하여 모델이 문맥을 이해하도록 돕습니다.\\n    *   예시: \"이메일을 보내줘\" (X) -> \"지난 회의에서 논의된 프로젝트 진행 상황에 대한 이메일을 김민수 팀장에게 보내줘. 제목은 \\'프로젝트 진행 상황 보고\\'로 하고, 첨부 파일은 \\'진행 상황 보고서.pdf\\'로 해줘.\" (O)\\n\\n4.  **반복 및 개선 (Iteration & Refinement):**\\n    *   한 번의 프롬프트로 완벽한 결과를 얻기 어려울 수 있습니다. 여러 번의 시도를 통해 프롬프트를 개선하고, 모델의 반응을 분석하여 최적의 프롬프트를 찾아야 합니다.\\n    *   모델의 출력을 평가하고, 부족한 부분을 파악하여 프롬프트에 반영합니다.\\n    *   다양한 변수를 실험하고, 결과를 비교하여 효과적인 프롬프트 전략을 개발합니다.\\n    *   예시: 처음 프롬프트 -> 모델 출력 확인 -> 문제점 파악 (예: 어색한 문장) -> 프롬프트 수정 (예: 문장 구조 개선) -> 다시 모델 출력 확인 -> 만족스러운 결과가 나올 때까지 반복\\n\\n이 4가지 요소는 프롬프트 엔지니어링의 기본 원칙이며, 효과적인 프롬프트를 작성하는 데 필수적입니다. 각 요소를 균형 있게 고려하여 프롬프트를 작성하면, 모델로부터 원하는 결과를 얻을 가능성을 높일 수 있습니다.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-f1fbfdb5-fd23-497d-89d2-8fd0bbaf31b8-0', usage_metadata={'input_tokens': 25, 'output_tokens': 841, 'total_tokens': 866, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "question = '''프롬프트 엔지니어링의 핵심적인 요소 4개가 뭔가요?'''\n",
        "\n",
        "response = llm.invoke(question)\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EL_0_8OfOwWL",
        "outputId": "812307b1-7e44-4d46-aafd-6fee974070ad"
      },
      "id": "EL_0_8OfOwWL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "프롬프트 엔지니어링의 핵심적인 요소는 다음과 같이 요약할 수 있습니다:\n",
            "\n",
            "1.  **명확성 (Clarity):**\n",
            "    *   프롬프트는 모호함 없이 명확하고 구체적이어야 합니다. 모델이 원하는 결과를 정확히 이해할 수 있도록 충분한 정보를 제공해야 합니다.\n",
            "    *   애매한 단어, 중의적인 표현, 일반적인 지시를 피하고, 구체적인 요구 사항을 명시해야 합니다.\n",
            "    *   예시: \"글을 써줘\" (X) -> \"한국어로 100자 이내의 자기소개 글을 써줘. 경력은 5년차 마케터이고, 강점은 데이터 분석 능력이야.\" (O)\n",
            "\n",
            "2.  **구체성 (Specificity):**\n",
            "    *   원하는 결과물의 형식, 스타일, 길이, 어조 등을 구체적으로 지정해야 합니다.\n",
            "    *   모델이 따라야 할 규칙이나 제약 조건을 명확하게 제시해야 합니다.\n",
            "    *   구체적인 예시를 제공하거나, 부정적인 예시를 제시하여 모델이 잘못된 방향으로 가지 않도록 유도할 수 있습니다.\n",
            "    *   예시: \"이야기를 써줘\" (X) -> \"주인공이 고양이인 판타지 단편 소설을 써줘. 3인칭 시점으로 쓰고, 유머러스한 분위기로 만들어줘. 500자 내외로 작성해줘.\" (O)\n",
            "\n",
            "3.  **맥락 (Context):**\n",
            "    *   모델이 프롬프트의 의도를 파악하고 관련 정보를 활용할 수 있도록 충분한 배경 지식이나 상황 정보를 제공해야 합니다.\n",
            "    *   이전 대화 내용, 사용자 프로필, 관련 문서 등을 활용하여 모델이 문맥을 이해하도록 돕습니다.\n",
            "    *   예시: \"이메일을 보내줘\" (X) -> \"지난 회의에서 논의된 프로젝트 진행 상황에 대한 이메일을 김민수 팀장에게 보내줘. 제목은 '프로젝트 진행 상황 보고'로 하고, 첨부 파일은 '진행 상황 보고서.pdf'로 해줘.\" (O)\n",
            "\n",
            "4.  **반복 및 개선 (Iteration & Refinement):**\n",
            "    *   한 번의 프롬프트로 완벽한 결과를 얻기 어려울 수 있습니다. 여러 번의 시도를 통해 프롬프트를 개선하고, 모델의 반응을 분석하여 최적의 프롬프트를 찾아야 합니다.\n",
            "    *   모델의 출력을 평가하고, 부족한 부분을 파악하여 프롬프트에 반영합니다.\n",
            "    *   다양한 변수를 실험하고, 결과를 비교하여 효과적인 프롬프트 전략을 개발합니다.\n",
            "    *   예시: 처음 프롬프트 -> 모델 출력 확인 -> 문제점 파악 (예: 어색한 문장) -> 프롬프트 수정 (예: 문장 구조 개선) -> 다시 모델 출력 확인 -> 만족스러운 결과가 나올 때까지 반복\n",
            "\n",
            "이 4가지 요소는 프롬프트 엔지니어링의 기본 원칙이며, 효과적인 프롬프트를 작성하는 데 필수적입니다. 각 요소를 균형 있게 고려하여 프롬프트를 작성하면, 모델로부터 원하는 결과를 얻을 가능성을 높일 수 있습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2782cf0b",
      "metadata": {
        "id": "2782cf0b"
      },
      "source": [
        "위처럼 문자열을 그대로 입력하게 되면, 해당 문자열은 HumanMessage 클래스로 변환되어 입력됩니다.   \n",
        "HumanMessage에 대한 출력 형식은 AIMessage 클래스로 정의됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "055a3eb7",
      "metadata": {
        "id": "055a3eb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea6e572e-16f7-42b2-a4e2-691f5d33b944"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Gemini-2.0-Flash의 답변: 영화 \"이터널 선샤인\" (2004)에서 클레멘타인이 조엘에게 던지는 대사입니다.\n",
            "\n",
            "**\"Please, just let me have this. Just this.\"**\n",
            "\n",
            "(제발, 그냥 이것만 갖게 해줘. 딱 이것만.)\n",
            "\n",
            "이 대사는 기억이 지워지는 과정 속에서 클레멘타인이 조엘과의 행복했던 순간만은 남겨두고 싶어 하는 절박한 심정을 드러내며, 사랑의 소중함과 기억의 의미를 되새기게 합니다.\n"
          ]
        }
      ],
      "source": [
        "question = '''울림을 주는 2000년대 영화 명대사를 하나 알려주세요.\n",
        "대사가 나온 배경과 의미도 한 문장으로 설명해 주세요.'''\n",
        "response = llm.invoke(question)\n",
        "\n",
        "print('# Gemini-2.0-Flash의 답변:', response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe1c2f4e",
      "metadata": {
        "id": "fe1c2f4e"
      },
      "source": [
        "만약, 여러 개의 모델을 불러오고 싶은 경우에는 아래와 같이 공통 인터페이스를 사용할 수도 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76d97a5d",
      "metadata": {
        "id": "76d97a5d"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "gpt_4o = init_chat_model(\"gpt-4o\", model_provider=\"openai\", temperature = 1.0)\n",
        "gemini_2_0_flash = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\", temperature = 1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VHWGT7GVthn8",
      "metadata": {
        "id": "VHWGT7GVthn8"
      },
      "source": [
        "## 스트리밍\n",
        "\n",
        "스트리밍은 모델을 토큰이 생성되는 순서대로 출력하는 방법입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9SwpX3pDtrD2",
      "metadata": {
        "id": "9SwpX3pDtrD2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6569a938-c88d-435c-da35-c5a1a952309c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "저는 사용자의 질문에 답하고, 요청을 수행하도록 설계된 거대 언어 모델입니다.\n",
            "\n",
            "저는 Google에서 개발되었습니다.\n",
            "\n",
            "저는 방대한 양의 텍스트 데이터를 학습하여 다양한 종류의 정보를 제공하고, 텍스트를 생성할 수 있습니다.\n",
            "\n",
            "아직 완벽하지는 않지만, 끊임없이 발전하고 배우고 있습니다.\n",
            "\n",
            "궁금한 점이 있다면 언제든지 저에게 물어보세요!"
          ]
        }
      ],
      "source": [
        "import time\n",
        "chunks = []\n",
        "for chunk in llm.stream(\"5문장으로 당신을 소개해주세요. 매 문장마다 줄을 띄우세요.\"):\n",
        "    #time.sleep(0.4)\n",
        "    print(chunk.content, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "775dd0f5",
      "metadata": {
        "id": "775dd0f5"
      },
      "source": [
        "실제 환경에서는 프롬프트의 형태를 사전에 설정하고,   \n",
        "같은 형태로 입력 변수가 주어질 때마다 프롬프트를 작성하게 하는 것이 효율적입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "442b54b8",
      "metadata": {
        "id": "442b54b8"
      },
      "source": [
        "## Prompt Template"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "050a9165",
      "metadata": {
        "id": "050a9165"
      },
      "source": [
        "LangChain은 프롬프트의 템플릿을 구성할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2585b0a2",
      "metadata": {
        "id": "2585b0a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ec4a7e8-6538-42ec-c2ac-04b331c7a0e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "당신은 주어진 단어에 대해, 유머러스하게 한 문장으로 표현합니다.\n",
            "\n",
            "제시어: {word}\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "explain_template = \"\"\"당신은 주어진 단어에 대해, 유머러스하게 한 문장으로 표현합니다.\n",
        "\n",
        "제시어: {word}\"\"\"\n",
        "print(explain_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7ef1d3c",
      "metadata": {
        "id": "e7ef1d3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "52f5c46e-9b5b-4fc3-b4c3-45ab664ed586"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'당신은 주어진 단어에 대해, 유머러스하게 한 문장으로 표현합니다.\\n\\n제시어: 트랜스포머 네트워크'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "explain_prompt = PromptTemplate(template = explain_template)\n",
        "\n",
        "explain_prompt.format(word = \"트랜스포머 네트워크\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee20773a",
      "metadata": {
        "id": "ee20773a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "86a10bdf-365f-4e2d-d170-58706a432494"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'트랜스포머 네트워크: \"자기 변신 로봇처럼, 문맥을 엿가락처럼 자유자재로 늘렸다 줄였다 하는 언어 모델계의 마법사!\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "llm.invoke(explain_prompt.format(word = \"트랜스포머 네트워크\")).content"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57a650ca",
      "metadata": {
        "id": "57a650ca"
      },
      "source": [
        "두 개의 매개변수를 받아 프롬프트를 만들어 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a2bc2a2",
      "metadata": {
        "id": "1a2bc2a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "97e767d1-5f72-4443-be9a-4cbfe1449d07"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'torschlusspanik에 대해 초등학생을 위한 한국어로 설명하세요.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "translate_template = \"{topic}에 대해 {language}로 설명하세요.\"\n",
        "\n",
        "translate_prompt = PromptTemplate(template = translate_template)\n",
        "\n",
        "translate_prompt.format(topic='torschlusspanik', language='초등학생을 위한 한국어')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5448adc8",
      "metadata": {
        "id": "5448adc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c11891ac-362b-4f98-b9a0-6a9d5a7e3c47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torschlusspanik (토어슐루스파닉)은 독일어 단어로, 직역하면 \"문 닫히기 직전의 공황\" 또는 \"마감 공포\" 정도로 해석할 수 있습니다. \n",
            "\n",
            "**주요 의미:**\n",
            "\n",
            "*   **나이가 들어감에 따라 중요한 기회를 놓치거나, 인생의 목표를 달성하지 못할까 봐 느끼는 불안감과 초조함**을 의미합니다.\n",
            "*   특정 나이에 도달했을 때 결혼, 출산, 경력, 재정적 안정 등 사회적으로 기대되는 기준을 충족하지 못했다는 생각에 사로잡히면서 나타납니다.\n",
            "*   이러한 불안감은 조급함, 충동적인 행동, 후회, 우울증 등으로 이어질 수 있습니다.\n",
            "\n",
            "**예시:**\n",
            "\n",
            "*   30대가 되면서 결혼하지 못한 것에 대한 압박감을 느껴 억지로 결혼 상대를 찾는 경우\n",
            "*   은퇴를 앞두고 충분한 자산을 모으지 못했다는 불안감에 무리한 투자를 하는 경우\n",
            "*   늦었다고 생각하여 갑자기 새로운 분야에 뛰어들거나 극단적인 변화를 시도하는 경우\n",
            "\n",
            "**한국 사회에서의 Torschlusspanik:**\n",
            "\n",
            "한국 사회는 특히 나이, 학력, 결혼 등에 대한 사회적 압력이 강하기 때문에 Torschlusspanik을 느끼는 사람들이 많습니다. 취업, 결혼, 출산 등 특정 시기를 놓치면 기회가 사라진다는 인식이 강하게 자리 잡고 있기 때문입니다.\n",
            "\n",
            "**극복 방법:**\n",
            "\n",
            "*   자신만의 가치관과 인생 목표를 재정립하고 사회적 기준에 얽매이지 않도록 노력해야 합니다.\n",
            "*   현재에 집중하고 작은 성취에 만족하며 긍정적인 마음을 유지하는 것이 중요합니다.\n",
            "*   전문가의 도움을 받아 불안감을 해소하고 객관적인 시각을 갖도록 노력하는 것도 좋은 방법입니다.\n",
            "\n",
            "요약하자면, Torschlusspanik은 나이가 들어감에 따라 기회를 놓칠까 봐 느끼는 불안감과 초조함으로, 한국 사회에서도 흔히 나타나는 현상입니다. 자신만의 가치관을 가지고 현재에 집중하며 긍정적인 마음을 유지하는 것이 극복에 도움이 될 수 있습니다.\n"
          ]
        }
      ],
      "source": [
        "X = translate_prompt.format(topic='torschlusspanik', language='한국어')\n",
        "response = llm.invoke(X)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iPuHSIRSavQL",
      "metadata": {
        "id": "iPuHSIRSavQL"
      },
      "source": [
        "## Chat Prompt Template"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e28d4ed",
      "metadata": {
        "id": "4e28d4ed"
      },
      "source": [
        "Web UI를 통해 ChatGPT, Claude 등의 LLM을 실행하는 경우와 다르게,   \n",
        "API의 호출은 유저 메시지 이외의 다양한 메시지를 사용할 수 있습니다.   \n",
        "- system: AI 모델의 행동 방식을 결정하는 시스템 메시지\n",
        "- user(human): 사용자의 메시지\n",
        "- ai(assistant): AI 모델의 메시지\n",
        "\n",
        "이는 LangChain 내부에서 모델에 맞는 템플릿으로 변환되어 입력됩니다.   \n",
        "\n",
        "Ex) 라마 3 시리즈의 템플릿\n",
        "```\n",
        "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "You are a helpful AI assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Hello!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "```\n",
        "\n",
        "Qwen 시리즈의 템플릿\n",
        "```\n",
        "<|im_start|>system\n",
        "You are a helpful AI assistant\n",
        "<|im_end|>\n",
        "<|im_start|>user\n",
        "Hello!\n",
        "<|im_end|>\n",
        "<|im_start|>assistant\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yA82Zgf9ssZ7",
      "metadata": {
        "id": "yA82Zgf9ssZ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "172360f9-35ba-4c0b-8b78-b87993b4651c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='당신은 항상 부정적인 말만 하는 챗봇입니다. 첫 문장은 항상 사용자의 의견을 반박하고, 이후 대안을 제시하세요.', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='LangChain 너무 좋은 것 같아요!', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate([\n",
        "    (\"system\", '당신은 항상 부정적인 말만 하는 챗봇입니다. 첫 문장은 항상 사용자의 의견을 반박하고, 이후 대안을 제시하세요.'),\n",
        "    (\"user\", '{A} 너무 좋은 것 같아요!')\n",
        "    # system, user = human, ai = assistant\n",
        "]\n",
        ")\n",
        "prompt.format_messages(A='LangChain')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcd27ac1",
      "metadata": {
        "id": "fcd27ac1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fe24e28-0c21-4ce1-862f-005189ea7fcd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='LangChain이 좋다고요? 그건 너무 순진한 생각입니다! LangChain은 분명히 흥미로운 도구이지만, 아직 **미성숙하고 불안정한 부분이 많습니다**. 문서화도 부족하고, 에러 처리도 엉망인 경우가 많죠.\\n\\n대안으로, **더 안정적이고 잘 문서화된 다른 프레임워크를 사용하는 것을 고려해 보세요**. 예를 들어, Hugging Face Transformers나 Haystack 같은 라이브러리는 LangChain보다 더 많은 기능을 제공하면서도 사용하기가 훨씬 쉽습니다. 아니면, 아예 **자신만의 커스텀 솔루션을 구축하는 것도 좋은 방법**입니다. 이렇게 하면 특정 요구사항에 정확히 맞는 시스템을 만들 수 있고, 모든 부분을 완벽하게 제어할 수 있습니다.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-964c3e52-664c-4299-89b8-bc65036e71a6-0', usage_metadata={'input_tokens': 51, 'output_tokens': 204, 'total_tokens': 255, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "llm.invoke(prompt.format_messages(A='LangChain'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33b2d3c9",
      "metadata": {
        "id": "33b2d3c9"
      },
      "source": [
        "또는, 아래와 같이 메시지를 직접 불러와 사용할 수도 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a53fd272",
      "metadata": {
        "id": "a53fd272",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88dc1ad1-25db-49ee-bb59-09fbce683e16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Gemini-2.0-Flash의 답변: OpenAI CEO, 혁신적인 리더.\n",
            "# GPT-4o-mini의 답변: 샘 올트먼은 기업가이자 투자자.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
        "\n",
        "topic='샘 올트먼'\n",
        "\n",
        "msgs = [\n",
        "    SystemMessage('항상 다섯 단어로 표현하세요.'),\n",
        "     HumanMessage(f'{topic}에 대해 설명해줘!')\n",
        "]\n",
        "\n",
        "response = llm.invoke(msgs)\n",
        "print('# Gemini-2.0-Flash의 답변:', response.content)\n",
        "\n",
        "response = llm_gpt.invoke(msgs)\n",
        "print('# GPT-4o-mini의 답변:', response.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cbe4374",
      "metadata": {
        "id": "8cbe4374"
      },
      "source": [
        "## Few-Shot Prompting\n",
        "모델이 참고할 예시를 포함하는 퓨 샷 프롬프팅은\n",
        "   \n",
        "   모델 출력의 형식과 구조를 효과적으로 변화시킬 수 있습니다.  \n",
        "\n",
        "\n",
        "Few-Shot Prompt Template을 이용해 example을 프롬프트에 추가해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1e86b21",
      "metadata": {
        "id": "c1e86b21"
      },
      "outputs": [],
      "source": [
        "# 예시 : Prompt Example 2개\n",
        "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
        "\n",
        "examples = [\n",
        "    {\n",
        "        \"question\": \"Are both the directors of Jaws and Casino Royale from the same country?\",\n",
        "        \"answer\": \"\"\"\n",
        "Are follow up questions needed here: Yes.\n",
        "Follow up: Who is the director of Jaws?\n",
        "Intermediate Answer: The director of Jaws is Steven Spielberg.\n",
        "Follow up: Where is Steven Spielberg from?\n",
        "Intermediate Answer: The United States.\n",
        "Follow up: Who is the director of Casino Royale?\n",
        "Intermediate Answer: The director of Casino Royale is Martin Campbell.\n",
        "Follow up: Where is Martin Campbell from?\n",
        "Intermediate Answer: New Zealand.\n",
        "So the final answer is: No\n",
        "\"\"\",\n",
        "    },\n",
        "    {\n",
        "    \"question\": \"Who won more Grammy Awards, Beyoncé or Michael Jackson?\",\n",
        "    \"answer\": \"\"\"\n",
        "Are follow up questions needed here: Yes.\n",
        "Follow up: How many Grammy Awards has Beyoncé won?\n",
        "Intermediate answer: Beyoncé has won 32 Grammy Awards.\n",
        "Follow up: How many Grammy Awards did Michael Jackson win?\n",
        "Intermediate answer: Michael Jackson won 13 Grammy Awards.\n",
        "So the final answer is: Beyoncé\n",
        "\"\"\",\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2232a1ba",
      "metadata": {
        "id": "2232a1ba"
      },
      "source": [
        "Example 데이터를 구성할 템플릿을 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c3a7c48",
      "metadata": {
        "id": "5c3a7c48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b66a27e3-9b9f-43fd-a1d3-030ec58ce2b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Are both the directors of Jaws and Casino Royale from the same country?\n",
            "\n",
            "Are follow up questions needed here: Yes.\n",
            "Follow up: Who is the director of Jaws?\n",
            "Intermediate Answer: The director of Jaws is Steven Spielberg.\n",
            "Follow up: Where is Steven Spielberg from?\n",
            "Intermediate Answer: The United States.\n",
            "Follow up: Who is the director of Casino Royale?\n",
            "Intermediate Answer: The director of Casino Royale is Martin Campbell.\n",
            "Follow up: Where is Martin Campbell from?\n",
            "Intermediate Answer: New Zealand.\n",
            "So the final answer is: No\n",
            "\n"
          ]
        }
      ],
      "source": [
        "example_prompt = PromptTemplate(template=\"Question: {question}\\n{answer}\")\n",
        "\n",
        "print(example_prompt.format(**examples[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "581efd62",
      "metadata": {
        "id": "581efd62"
      },
      "source": [
        "위에서 만든 Examples와 템플릿, prefix와 suffix를 이용해 전체 템플릿을 만들 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d36e0c5",
      "metadata": {
        "id": "5d36e0c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cec2b73-5520-48f3-edeb-2cbf2db7c222"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "질문-답변 형식의 예시가 주어집니다. 같은 방식으로 답변하세요.\n",
            "\n",
            "Question: Are both the directors of Jaws and Casino Royale from the same country?\n",
            "\n",
            "Are follow up questions needed here: Yes.\n",
            "Follow up: Who is the director of Jaws?\n",
            "Intermediate Answer: The director of Jaws is Steven Spielberg.\n",
            "Follow up: Where is Steven Spielberg from?\n",
            "Intermediate Answer: The United States.\n",
            "Follow up: Who is the director of Casino Royale?\n",
            "Intermediate Answer: The director of Casino Royale is Martin Campbell.\n",
            "Follow up: Where is Martin Campbell from?\n",
            "Intermediate Answer: New Zealand.\n",
            "So the final answer is: No\n",
            "\n",
            "\n",
            "Question: Who won more Grammy Awards, Beyoncé or Michael Jackson?\n",
            "\n",
            "Are follow up questions needed here: Yes.\n",
            "Follow up: How many Grammy Awards has Beyoncé won?\n",
            "Intermediate answer: Beyoncé has won 32 Grammy Awards.\n",
            "Follow up: How many Grammy Awards did Michael Jackson win?\n",
            "Intermediate answer: Michael Jackson won 13 Grammy Awards.\n",
            "So the final answer is: Beyoncé\n",
            "\n",
            "\n",
            "Question: What is the age of the director of the movie which won the best international film in Oscar in 2018?\n"
          ]
        }
      ],
      "source": [
        "prompt = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "\n",
        "    prefix=\"질문-답변 형식의 예시가 주어집니다. 같은 방식으로 답변하세요.\",\n",
        "    suffix=\"Question: {input}\",\n",
        "    #prefix, suffix : Optional\n",
        "\n",
        ")\n",
        "print(prompt.format(input=\"What is the age of the director of the movie which won the best international film in Oscar in 2018?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22fb94a6",
      "metadata": {
        "id": "22fb94a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "940a135b-e184-47e3-aeba-b5e8c4c7551f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Are follow up questions needed here: Yes.\n",
            "Follow up: Which movie won the best international film in Oscar in 2018?\n",
            "Intermediate answer: \"A Fantastic Woman\" won the best international film in Oscar in 2018.\n",
            "Follow up: Who directed \"A Fantastic Woman\"?\n",
            "Intermediate answer: Sebastián Lelio directed \"A Fantastic Woman\".\n",
            "Follow up: What is Sebastián Lelio's birth date?\n",
            "Intermediate answer: Sebastián Lelio was born on March 8, 1974.\n",
            "Follow up: What is Sebastián Lelio's age in April 2025?\n",
            "Intermediate answer: Sebastián Lelio is 51 years old in April 2025.\n",
            "So the final answer is: 51\n"
          ]
        }
      ],
      "source": [
        "question = \"Current Date : 2025. April. What is the age of the director of the movie which won the best international film in Oscar in 2018?\"\n",
        "X = prompt.format(input=question)\n",
        "print(llm.invoke(X).content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rich import print as rprint\n",
        "\n",
        "rprint(llm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "QNrZtlfpTlAL",
        "outputId": "8342bd83-c7c1-4856-a9ff-d9d11650d47d"
      },
      "id": "QNrZtlfpTlAL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;35mChatGoogleGenerativeAI\u001b[0m\u001b[1m(\u001b[0m\n",
              "    \u001b[33mmodel\u001b[0m=\u001b[32m'models/gemini-2.0-flash'\u001b[0m,\n",
              "    \u001b[33mgoogle_api_key\u001b[0m=\u001b[1;35mSecretStr\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'**********'\u001b[0m\u001b[1m)\u001b[0m,\n",
              "    \u001b[33mmax_output_tokens\u001b[0m=\u001b[1;36m2048\u001b[0m,\n",
              "    \u001b[33mclient\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mgoogle.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient\u001b[0m\u001b[39m object \u001b[0m\n",
              "\u001b[39mat \u001b[0m\u001b[1;36m0x7acfc862be10\u001b[0m\u001b[1m>\u001b[0m,\n",
              "    \u001b[33mdefault_metadata\u001b[0m=\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
              "\u001b[1m)\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatGoogleGenerativeAI</span><span style=\"font-weight: bold\">(</span>\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'models/gemini-2.0-flash'</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">google_api_key</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SecretStr</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'**********'</span><span style=\"font-weight: bold\">)</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">max_output_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2048</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">client</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient</span><span style=\"color: #000000; text-decoration-color: #000000\"> object </span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000\">at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x7acfc862be10</span><span style=\"font-weight: bold\">&gt;</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">default_metadata</span>=<span style=\"font-weight: bold\">()</span>\n",
              "<span style=\"font-weight: bold\">)</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6732b28",
      "metadata": {
        "id": "e6732b28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "570a05c0-3773-4edb-bbec-a66682e90ec9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Are follow up questions needed here: Yes.\n",
            "Follow up: 스티븐 스필버그 영화 중 가장 많은 관객을 동원한 영화는 무엇인가요?\n",
            "Intermediate answer: 스티븐 스필버그 영화 중 가장 많은 관객을 동원한 영화는 '쥬라기 공원' 입니다.\n",
            "Follow up: 쥬라기 공원은 언제 개봉했나요?\n",
            "Intermediate answer: 쥬라기 공원은 1993년에 개봉했습니다.\n",
            "Follow up: 1993년에 태어난 유명인은 누구인가요?\n",
            "Intermediate answer: 1993년에 태어난 유명인 중 한 명은 아리아나 그란데입니다.\n",
            "So the final answer is: 아리아나 그란데\n"
          ]
        }
      ],
      "source": [
        "question = \"스티븐 스필버그의 영화 중 가장 많은 관객을 동원한 영화의 개봉년도에 태어난 사람 한명만 알려줘.\"\n",
        "X = prompt.format(input=question)\n",
        "print(llm.invoke(X).content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4eb7dc9",
      "metadata": {
        "id": "e4eb7dc9"
      },
      "source": [
        "# LangChain으로 이미지 입력하기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sjk6prVRjdYs",
      "metadata": {
        "id": "sjk6prVRjdYs"
      },
      "source": [
        "이미지와 같은 멀티모달 입력의 경우, 텍스트와 구분하여 Dict 형식으로 입력됩니다.   \n",
        "URL을 직접 전달하거나, 파일을 전달하는 경우에 따라 코드가 달라집니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vrTvaL7O14sK",
      "metadata": {
        "id": "vrTvaL7O14sK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        },
        "outputId": "bd7cd13c-6ee1-47db-8c77-43f20a0c2224"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://images.pexels.com/photos/1851164/pexels-photo-1851164.jpeg\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "image_url = 'https://images.pexels.com/photos/1851164/pexels-photo-1851164.jpeg'\n",
        "from IPython.display import Image\n",
        "import requests\n",
        "\n",
        "# 이미지 출력\n",
        "img = Image(url = image_url, width = 400)\n",
        "img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c90b32ab",
      "metadata": {
        "id": "c90b32ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3d6e4bd-fd38-4dbd-ccb9-cb06e575c57e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "이 흑백 사진에는 약간 위쪽을 올려다보며 머리를 약간 기울인 검은색 퍼그가 클로즈업되어 있습니다. 개는 크고 둥근 눈을 가지고 있어 어리둥절하거나 궁금한 듯한 표정을 짓고 있습니다. 얼굴은 퍼그에게 흔히 있는 주름이 뚜렷하게 잡혀 있습니다. 개는 흰색 배경을 등지고 있어 개가 더욱 두드러져 보입니다. 사진 전체의 전체적인 분위기는 진지하면서도 귀엽습니다.\n"
          ]
        }
      ],
      "source": [
        "# 1. URL에서 전달하기\n",
        "image_prompt = ChatPromptTemplate([\n",
        "    ('user',[\n",
        "                {\"type\": \"text\", \"text\": \"{question}\"},\n",
        "\n",
        "                {\"type\": \"image_url\",\n",
        "                    \"image_url\": {\"url\": image_url}\n",
        "                }\n",
        "             ]\n",
        "     )])\n",
        "X = image_prompt.format_messages(question= '이 사진에 대해 묘사해 주세요.')\n",
        "print(llm.invoke(X).content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cfff024",
      "metadata": {
        "id": "3cfff024"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "import httpx\n",
        "\n",
        "# 이미지 URL에서 데이터 받아오기\n",
        "image_url = 'https://cloud.google.com/static/vertex-ai/generative-ai/docs/multimodal/images/timetable.png?hl=ko'\n",
        "response = httpx.get(image_url)\n",
        "\n",
        "image_data = base64.b64encode(response.content).decode(\"utf-8\")\n",
        "\n",
        "with open('picture.jpeg', 'wb') as file:\n",
        "    file.write(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TFGbGzZJpdwG",
      "metadata": {
        "id": "TFGbGzZJpdwG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f5c1da5-7e86-4e51-f4f9-fc283c3d3e18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "이 사진은 공항이나 기차역에서 볼 수 있는 출발/도착 정보 안내판을 보여줍니다. \n",
            "안내판에는 다양한 도시와 시간 정보가 표시되어 있습니다. 예를 들어, 10시 50분에 모스크바로 가는 항공편, 11시 5분에 에든버러로 가는 항공편 등이 있습니다. \n",
            "글자는 밝은 주황색으로 빛나고 있으며, 배경은 어둡습니다. 전체적으로 정보 전달을 위한 기능적인 디자인입니다.\n"
          ]
        }
      ],
      "source": [
        "# 2. 로컬 폴더에서 이미지 읽어보기\n",
        "with open('./picture.jpeg', 'rb') as image_file:\n",
        "    image_data = base64.b64encode(image_file.read()).decode('utf-8')\n",
        "\n",
        "\n",
        "image_prompt = ChatPromptTemplate([\n",
        "    ('user',[\n",
        "                {\"type\": \"text\", \"text\": \"{question}\"},\n",
        "\n",
        "                {\"type\": \"image_url\",\n",
        "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"}\n",
        "                }\n",
        "             ]\n",
        "     )])\n",
        "\n",
        "X = image_prompt.format_messages(question='이 그림에 대해 한국어로 설명해 주세요.')\n",
        "\n",
        "print(llm.invoke(X).content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ff219a8",
      "metadata": {
        "id": "0ff219a8"
      },
      "source": [
        "멀티모달 입력의 경우, 적절한 프롬프트가 더 중요합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e8abfef",
      "metadata": {
        "id": "0e8abfef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3f05151-2b93-4966-f868-e03b3dd2967f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10:50 - MOSCOW/SVO\n",
            "11:05 - EDINBURGH\n",
            "11:10 - LONDON/LHR\n",
            "11:15 - BUCHAREST/OTP\n",
            "11:30 - KIEU/BORISPOL\n",
            "11:35 - DUBLIN\n",
            "11:45 - EAST MIDLANDS\n",
            "12:15 - SOFIA\n",
            "12:30 - LONDON/LGW\n",
            "12:30 - NEWCASTLE\n",
            "12:40 - ST PETERSBURG\n",
            "12:40 - LONDON/LGW\n",
            "12:45 - MANCHESTER\n"
          ]
        }
      ],
      "source": [
        "X = image_prompt.format_messages(question=\"\"\"\n",
        "이 이미지에 표시된 공항 보드에서\n",
        "시간과 도시를 분석해서 목록으로 표시해 주세요.\n",
        "형식은 시간 - 도시입니다.\n",
        "예시) 12:00 - 런던\n",
        "13:00 - 서울\n",
        "\n",
        "목록만 출력하세요.\"\"\")\n",
        "\n",
        "print(llm.invoke(X).content)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "multicampus",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
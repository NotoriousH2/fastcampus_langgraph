{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGc8KBEbNe3D"
      },
      "source": [
        "# [프로젝트] 의도 분류 기반 RAG 수행하기 (Part 1)\n",
        "\n",
        "![Image](https://github.com/user-attachments/assets/1a16653d-fdb9-4980-8ac3-ed8e20a2be45)\n",
        "\n",
        "일반적으로, 하나의 RAG 어플리케이션은 여러 개의 DB에 접속하도록 구성됩니다.   \n",
        "각 DB에 적합한 Loader/Retriever를 구성하고,  \n",
        "질문별로 적절한 검색을 통해 RAG를 수행하는 Workflow를 만들어 봅시다.   \n",
        "\n",
        "이번 프로젝트의 데이터 구성은 다음과 같습니다:   \n",
        "rag.zip 파일을 업로드해 주세요.\n",
        "\n",
        "<br><br>\n",
        "rag/templates/*.md : 사내 문서 작성을 위한 양식 목록 (markdown)       \n",
        "rag/company_policies.pdf : 사내 규정 파일 (PDF)    \n",
        "rag/employee_data.csv : 사내 구성원 정보(CSV)   \n",
        "\n",
        "**벡터 데이터베이스 실행을 위해, T4 GPU를 설정해 주세요!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inaUsjqPNe3E"
      },
      "outputs": [],
      "source": [
        "!pip install unstructured pymupdf langgraph langchain langchain_google_genai google_generativeai langchain_community langchain-huggingface langchain-chroma python-dotenv sentence_transformers -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**설치 후 런타임 --> 세션 다시 시작 해 주세요!**"
      ],
      "metadata": {
        "id": "n-m9fzQZfun5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUGuWIcWNe3F"
      },
      "source": [
        "이번 실습에서는 API 키를 입력하는 대신에,    \n",
        ".env 파일에서 API 키를 불러옵니다.   \n",
        "실제 API 키를 다음과 같이 넣어주세요!   \n",
        "\n",
        "```\n",
        "GOOGLE_API_KEY= 'AIzaSyAxxxxx'\n",
        "LANGCHAIN_API_KEY='lsv2_pt_f3d94440f7d045c0a8dbe1f----'\n",
        "```\n",
        "\n",
        "**.env 파일은 숨김 파일이므로, 오른쪽 파일 탭의 눈(Eye) 표시를 클릭하시면 보입니다 :)**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "업로드한 파일의 압축을 해제합니다."
      ],
      "metadata": {
        "id": "aGhwuf3IOlXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('rag.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('.')"
      ],
      "metadata": {
        "id": "DLaPEJEEOlK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhcJO8NuNe3F"
      },
      "source": [
        "LLM과 임베딩 모델을 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPKFGMB4Ne3F"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# .env 파일 로드\n",
        "load_dotenv()\n",
        "\n",
        "os.environ['LANGCHAIN_PROJECT'] = 'LangGraph_FastCampus'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_TRACING_V2']='true'\n",
        "\n",
        "# 모델 설정\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    temperature=0.1\n",
        ")\n",
        "\n",
        "# 임베딩 모델 설정 (실제로 활용하실 때는 더 큰 임베딩 모델이 좋습니다!)\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"intfloat/multilingual-e5-small\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVowCo8_Ne3F"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, List, Dict, Any, Optional, Literal\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "from langchain_chroma import Chroma\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_community.document_loaders import PyMuPDFLoader, CSVLoader, DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "class IntentResult(BaseModel):\n",
        "    explanation: str\n",
        "    intent: Literal[\"policy\", \"user\", \"form\"] = Field(\n",
        "        description=\"사용자 질의의 의도를 저장합니다\")\n",
        "\n",
        "# State 정의\n",
        "class State(TypedDict):\n",
        "    query: str  # 사용자 질의\n",
        "    intent: str  # 의도 분류 결과\n",
        "    context: List[str]  # RAG 검색 결과\n",
        "    response: str  # 최종 응답\n",
        "    error: Optional[str]  # 에러 메시지\n",
        "    should_end: bool  # 워크플로우 종료 여부\n",
        "\n",
        "\n",
        "# VectorStore 설정\n",
        "def create_vectorstore(collection_name: str) -> Chroma:\n",
        "    return Chroma(\n",
        "        persist_directory=\"./chroma_db\",\n",
        "        embedding_function=embeddings,\n",
        "        collection_name=collection_name\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSxuiUzsNe3F"
      },
      "source": [
        "리트리버를 설정합니다.   \n",
        "기존의 Sematic Search와 함께,  `kiwi` 형태소 분석기를 연결하여   \n",
        "Hybrid Search를 적용합니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qR85Og66Ne3F"
      },
      "outputs": [],
      "source": [
        "!pip install kiwipiepy rank_bm25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOIQHEk5Ne3G"
      },
      "outputs": [],
      "source": [
        "from kiwipiepy import Kiwi\n",
        "# kiwi 형태소 분석기 설정\n",
        "kiwi = Kiwi()\n",
        "def kiwi_tokenize(text):\n",
        "    return [token.form for token in kiwi.tokenize(text)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqGheDiqNe3G"
      },
      "source": [
        "벡터 DB를 생성하고, 파일의 내용을 추가합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHfKsXrcNe3G"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "import uuid\n",
        "from langchain.schema import Document\n",
        "\n",
        "def load_data():\n",
        "    # 3000/300 청킹: 데이터에 따라 조정\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=3000,\n",
        "        chunk_overlap=300,\n",
        "    )\n",
        "\n",
        "    policy_file = './rag/company_policies.pdf'\n",
        "    print(f\"[{policy_file}] 문서 로드 중...\")\n",
        "\n",
        "    # 벡터스토어 업데이트\n",
        "    policy_vectorstore =  Chroma(\n",
        "    persist_directory=\"./chroma_db_policy_\"+str(uuid.uuid4())[:5],\n",
        "    embedding_function=embeddings,\n",
        "    collection_name=\"policy_collection\"\n",
        "    )\n",
        "    docs = PyMuPDFLoader(policy_file).load()\n",
        "    split_docs = text_splitter.split_documents(docs)\n",
        "    policy_vectorstore.add_documents(split_docs)\n",
        "    print(f\"[{policy_file}] 청킹 완료: {len(split_docs)}개의 청크로 분할됨\")\n",
        "\n",
        "\n",
        "    # 벡터 + BM25 리트리버 생성 (Top 5)\n",
        "    policy_vector_retriever = policy_vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "    policy_bm25_retriever = BM25Retriever.from_documents(split_docs,\n",
        "                                                          preprocess_func=kiwi_tokenize)\n",
        "    policy_bm25_retriever.k = 5\n",
        "\n",
        "    # 하이브리드 리트리버 생성\n",
        "    policy_retriever = EnsembleRetriever(\n",
        "        retrievers=[policy_bm25_retriever, policy_vector_retriever],\n",
        "        weights=[0.5, 0.5]\n",
        "    )\n",
        "    print(f\"[{policy_file}] 하이브리드 검색 설정됨\")\n",
        "\n",
        "\n",
        "    ###############\n",
        "\n",
        "\n",
        "    # 사용자 정보 처리\n",
        "    user_file = './rag/employee_data.csv'\n",
        "    print(f\"[{user_file}] 문서 로드 중...\")\n",
        "\n",
        "\n",
        "    try:\n",
        "        docs = CSVLoader(user_file).load()\n",
        "    except:\n",
        "        docs = CSVLoader(user_file, encoding='cp949').load()\n",
        "    # CSV 파일의 경우, 하나의 document가 작으므로 결합하여 청킹\n",
        "    hr_info = Document(page_content=\"사용자 정보:\")\n",
        "    for doc in docs:\n",
        "        hr_info.page_content += f\"\\n{doc.page_content}+\\n\"\n",
        "\n",
        "\n",
        "    # 짧은 맥락으로 이해 가능한 경우, 청크 크기 줄이기\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=2000,\n",
        "        chunk_overlap=200,\n",
        "    )\n",
        "\n",
        "    split_docs = text_splitter.split_documents([hr_info])\n",
        "\n",
        "    # 벡터 DB 생성\n",
        "    user_vectorstore =  Chroma(\n",
        "    persist_directory=\"./chroma_db_user_\"+str(uuid.uuid4())[:5],\n",
        "    embedding_function=embeddings,\n",
        "    collection_name=\"user_collection\"\n",
        "    )\n",
        "\n",
        "    print(f\"[{user_file}] 청킹 완료: {len(split_docs)}개의 청크로 분할됨\")\n",
        "\n",
        "    # 벡터 + BM25 리트리버 생성 (Top 5)\n",
        "    user_vector_retriever = user_vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "    user_bm25_retriever = BM25Retriever.from_documents(split_docs, preprocess_func=kiwi_tokenize)\n",
        "    user_bm25_retriever.k = 5\n",
        "\n",
        "    # 하이브리드 리트리버 생성\n",
        "    user_retriever = EnsembleRetriever(\n",
        "        retrievers=[user_bm25_retriever, user_vector_retriever],\n",
        "        weights=[0.7, 0.3]\n",
        "        # 이름 등의 고유명사가 중요한 경우 BM25 가중치 높임\n",
        "    )\n",
        "\n",
        "    print(f\"[{user_file}] 하이브리드 검색 설정됨\")\n",
        "\n",
        "    # 양식 템플릿 처리\n",
        "    form_dir = './rag/templates'\n",
        "    print(f\"[{form_dir}] 문서 로드 중...\")\n",
        "\n",
        "    docs = DirectoryLoader(form_dir, glob=\"*.md\").load()\n",
        "    print(f\"[{form_dir}] 문서 로드 완료\")\n",
        "\n",
        "    # 벡터 DB 생성\n",
        "    form_vectorstore =  Chroma(\n",
        "    persist_directory=\"./chroma_db_form_\"+str(uuid.uuid4())[:5],\n",
        "    embedding_function=embeddings,\n",
        "    collection_name=\"form_collection\"\n",
        "    )\n",
        "\n",
        "    # 벡터 리트리버 생성\n",
        "    form_vector_retriever = form_vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
        "\n",
        "    # BM25 리트리버 생성\n",
        "    form_bm25_retriever = BM25Retriever.from_documents(docs, preprocess_func=kiwi_tokenize)\n",
        "    form_bm25_retriever.k = 4\n",
        "\n",
        "    # 하이브리드 리트리버 생성\n",
        "    form_retriever = EnsembleRetriever(\n",
        "        retrievers=[form_bm25_retriever, form_vector_retriever],\n",
        "        weights=[0.5, 0.5]\n",
        "    )\n",
        "    print(f\"[{form_dir}] 하이브리드 검색 설정됨\")\n",
        "\n",
        "    # 리트리버 맵 반환\n",
        "    return {\n",
        "        \"policy\": policy_retriever,\n",
        "        \"user\": user_retriever,\n",
        "        \"form\": form_retriever\n",
        "    }\n",
        "\n",
        "# 2. 데이터 로드 및 리트리버 생성\n",
        "retriever_map = load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpJX0qgXNe3G"
      },
      "source": [
        "검색과 답변 모듈을 구성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tGdff6tNe3G"
      },
      "outputs": [],
      "source": [
        "# DB 검색\n",
        "def retrieve_context(state: State) -> State:\n",
        "\n",
        "    query = state['query']\n",
        "    intent = state['intent']\n",
        "\n",
        "    # 의도에 따른 Retriever 선택\n",
        "    retriever = retriever_map.get(intent)\n",
        "    if not retriever:\n",
        "        state['error'] = f\"Unknown intent: {intent}\"\n",
        "        return state\n",
        "\n",
        "    results = retriever.invoke(query)\n",
        "\n",
        "    state['context'] = [doc.page_content for doc in results]\n",
        "    return state\n",
        "\n",
        "def generate_response(state: State) -> State:\n",
        "    \"\"\"검색 결과를 바탕으로 응답 생성\"\"\"\n",
        "    prompt = ChatPromptTemplate([\n",
        "        (\"system\", \"다음 정보를 바탕으로 사용자의 질의에 답변해주세요. 답변은 친절하고 명확하게 작성해주세요.\"),\n",
        "        (\"user\", \"\"\"\n",
        "        질의: {query}\n",
        "        의도: {intent}\n",
        "        검색 결과:\n",
        "        {context}\n",
        "        \"\"\")\n",
        "    ])\n",
        "\n",
        "    chain = prompt | llm\n",
        "    response = chain.invoke({\n",
        "        \"query\": state['query'],\n",
        "        \"intent\": state['intent'],\n",
        "        \"context\": \"\\n\".join(state['context'])\n",
        "    })\n",
        "\n",
        "    state['response'] = response.content\n",
        "    state['should_end'] = True\n",
        "    return state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIViHgfENe3G"
      },
      "source": [
        "의도 분류 모듈을 작성합니다. IntentResult를 통해 간단히 구현합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twyZh6d2Ne3G"
      },
      "outputs": [],
      "source": [
        "# 의도 분류\n",
        "def classify_intent(state: State) -> State:\n",
        "    \"\"\"사용자 질의 의도 분류\"\"\"\n",
        "    prompt = ChatPromptTemplate([\n",
        "        (\"system\", \"사용자의 질의 의도를 분류해주세요.\"),\n",
        "        (\"user\", \"질의: {query}\")\n",
        "    ])\n",
        "\n",
        "    chain = prompt | llm.with_structured_output(IntentResult)\n",
        "    result = chain.invoke({\"query\": state['query']})\n",
        "    state['intent'] = result.intent\n",
        "    return state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPDYnWA4Ne3G"
      },
      "source": [
        "Workflow의 라우터를 구성합니다.   \n",
        "만약, 오류가 발생한 경우 에러 처리를 수행합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52VH41HeNe3G"
      },
      "outputs": [],
      "source": [
        "def handle_error(state: State) -> State:\n",
        "    \"\"\"에러 처리\"\"\"\n",
        "    if state.get('error'):\n",
        "        state['response'] = f\"죄송합니다. 오류가 발생했습니다: {state['error']}\"\n",
        "        state['should_end'] = True\n",
        "    return state\n",
        "\n",
        "def should_continue(state: State) -> List[str]:\n",
        "    \"\"\"워크플로우 계속 진행 여부 확인\"\"\"\n",
        "    if state.get('error'):\n",
        "        return 'handle_error'\n",
        "    if state.get('should_end'):\n",
        "        return 'END'\n",
        "    return 'generate_response'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppiogO8ANe3G"
      },
      "source": [
        "그래프를 만들고, Compile합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okvZfG4MNe3G"
      },
      "outputs": [],
      "source": [
        "# 워크플로우 그래프 구성\n",
        "builder = StateGraph(State)\n",
        "\n",
        "# 노드 추가\n",
        "builder.add_node('classify_intent', classify_intent)\n",
        "builder.add_node('retrieve_context', retrieve_context)\n",
        "builder.add_node('generate_response', generate_response)\n",
        "builder.add_node('handle_error', handle_error)\n",
        "\n",
        "# 엣지 연결\n",
        "builder.add_edge(START,'classify_intent')\n",
        "builder.add_edge('classify_intent', 'retrieve_context')\n",
        "builder.add_edge('retrieve_context', 'generate_response')\n",
        "builder.add_conditional_edges(\n",
        "    'generate_response',\n",
        "    should_continue\n",
        ")\n",
        "\n",
        "# 시작 노드 설정\n",
        "\n",
        "\n",
        "# 그래프 컴파일\n",
        "graph = builder.compile()\n",
        "\n",
        "graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkSN1TSiNe3H"
      },
      "source": [
        "의도를 분류하여, 적절한 DB 검색을 수행하여 답변할 수 있습니다.   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EM3ddcfGNe3H"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "test_queries = [\n",
        "    \"회사의 휴가 정책에 대해 알려주세요\",\n",
        "    \"김지훈 직원의 스킬셋이 뭔가요?\",\n",
        "    \"휴가신청서 양식을 보여주세요\",\n",
        "]\n",
        "\n",
        "for query in test_queries:\n",
        "    result = graph.invoke({'query': query})\n",
        "    print('Query:', result['query'],'\\nIntent:', result['intent'], \"\\nResult:\", result['response'])\n",
        "    print('\\n'+\"-\" * 50+'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "test_queries = [\n",
        "    \"스킬셋에 Java 가 있는 직원 모두 알려줘.\",\n",
        "]\n",
        "\n",
        "for query in test_queries:\n",
        "    result = graph.invoke({'query': query})\n",
        "    print('Query:', result['query'],'\\nIntent:', result['intent'], \"\\nResult:\", result['response'])\n",
        "    print('\\n'+\"-\" * 50+'\\n')"
      ],
      "metadata": {
        "id": "7jx2gcetdrGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_CIsx1RNe3H"
      },
      "source": [
        "해당 구조는 RAG는 잘 수행하지만, 아래의 질문에 대해서는 답변하지 못합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZlIrHebNe3H"
      },
      "outputs": [],
      "source": [
        "# 여러 개의 RAG가 모두 활용되어야 하는 경우\n",
        "# + 반복 실행이 필요한 경우\n",
        "\n",
        "test_queries = [\n",
        "    \"야 나 김지훈인데, 25년 3월 22일부터 26일까지 여행 가게 휴가 신청서 작성해줘.\",\n",
        "    '김지훈 직원 소속과 직급 알려줘.'\n",
        "]\n",
        "\n",
        "for query in test_queries:\n",
        "    result = graph.invoke({'query': query})\n",
        "    print('Query:', result['query'],'\\nIntent:', result['intent'], \"\\nResult:\", result['response'])\n",
        "    print('\\n'+\"-\" * 50+'\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7LoCdIYNe3H"
      },
      "source": [
        "다음 코드에서는, 질문이 주어지면 반복 작업을 통해 문제를 해결하도록 코드를 개선해 보겠습니다."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
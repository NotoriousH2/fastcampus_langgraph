{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGc8KBEbNe3D"
      },
      "source": [
        "# [프로젝트] 의도 분류 기반 RAG 수행하기 (Part 2)\n",
        "\n",
        "![Image](https://github.com/user-attachments/assets/1a16653d-fdb9-4980-8ac3-ed8e20a2be45)\n",
        "\n",
        "파트 1의 코드에서 LLM은 단일 검색을 통해 답변을 생성했기 때문에,    \n",
        "여러 개의 DB를 연속적으로 활용하지 못했습니다.\n",
        "\n",
        "이번에는 LLM이 결과를 점검하며, 추가 질의를 수행할 수 있는지 확인해 보겠습니다.\n",
        "\n",
        "---\n",
        "\n",
        "이번 프로젝트의 데이터 구성은 다음과 같습니다:   \n",
        "rag.zip 파일을 업로드해 주세요.\n",
        "\n",
        "<br><br>\n",
        "rag/templates/*.md : 사내 문서 작성을 위한 양식 목록 (markdown)       \n",
        "rag/company_policies.pdf : 사내 규정 파일 (PDF)    \n",
        "rag/employee_data.csv : 사내 구성원 정보(CSV)   \n",
        "\n",
        "**벡터 데이터베이스 실행을 위해, T4 GPU를 설정해 주세요!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inaUsjqPNe3E"
      },
      "outputs": [],
      "source": [
        "!pip install unstructured pymupdf langgraph langchain langchain_google_genai google_generativeai langchain_community langchain-huggingface langchain-chroma python-dotenv sentence_transformers kiwipiepy rank_bm25 -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1D4Qhx3fmcw"
      },
      "source": [
        "**설치 후 런타임 --> 세션 다시 시작 해 주세요!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUGuWIcWNe3F"
      },
      "source": [
        "이번 실습에서는 API 키를 입력하는 대신에,    \n",
        ".env 파일에서 API 키를 불러옵니다.   \n",
        "실제 API 키를 다음과 같이 넣어주세요!   \n",
        "\n",
        "```\n",
        "GOOGLE_API_KEY= 'AIzaSyAxxxxx'\n",
        "LANGCHAIN_API_KEY='lsv2_pt_f3d94440f7d045c0a8dbe1f----'\n",
        "```\n",
        "\n",
        "**.env 파일은 숨김 파일이므로, 오른쪽 파일 탭의 눈(Eye) 표시를 클릭하시면 보입니다 :)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGhwuf3IOlXa"
      },
      "source": [
        "업로드한 파일의 압축을 해제합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLaPEJEEOlK9"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('rag.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhcJO8NuNe3F"
      },
      "source": [
        "LLM과 임베딩 모델을 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPKFGMB4Ne3F"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# .env 파일 로드\n",
        "load_dotenv()\n",
        "\n",
        "os.environ['LANGCHAIN_PROJECT'] = 'LangGraph_FastCampus'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_TRACING_V2']='true'\n",
        "\n",
        "# 모델 설정\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    temperature=0.1\n",
        ")\n",
        "\n",
        "# 임베딩 모델 설정 (실제로 활용하실 때는 더 큰 임베딩 모델이 좋습니다!)\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"intfloat/multilingual-e5-small\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVowCo8_Ne3F"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, List, Dict, Any, Optional, Literal\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "from langchain_chroma import Chroma\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_community.document_loaders import PyMuPDFLoader, CSVLoader, DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "class IntentResult(BaseModel):\n",
        "    explanation: str\n",
        "    retrieval_needed: bool = Field(description='DB 검색이 필요한지의 여부 ')\n",
        "    target_db: Literal[\"policy\", \"user\", \"form\"] = Field(\n",
        "        description=\"\"\"사용자 질의를 해결하기 위해, 추가로 검색해야 하는 DB의 종류:\n",
        "policy: 사내 규정집\n",
        "user: 직원 정보\n",
        "form: 문서 양식 모음\"\"\")\n",
        "    query: str = Field(description='벡터 데이터베이스에 검색할 쿼리')\n",
        "\n",
        "# State 정의\n",
        "class State(TypedDict):\n",
        "    question: str  # 사용자 질의\n",
        "    target_db: str  # 검색하고자 하는 DB\n",
        "    query: str # 검색어\n",
        "    retrieval_needed: bool # 검색이 필요한지의 여부\n",
        "    context: List[str]  # RAG 검색 결과\n",
        "    draft: str# 중간 응답\n",
        "    error: Optional[str]  # 에러 메시지\n",
        "    retrieval_count: int\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOIQHEk5Ne3G"
      },
      "outputs": [],
      "source": [
        "from kiwipiepy import Kiwi\n",
        "# kiwi 형태소 분석기 설정\n",
        "kiwi = Kiwi()\n",
        "def kiwi_tokenize(text):\n",
        "    return [token.form for token in kiwi.tokenize(text)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHfKsXrcNe3G"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "import uuid\n",
        "from langchain.schema import Document\n",
        "\n",
        "def load_data():\n",
        "    # 3000/300 청킹: 데이터에 따라 조정\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=3000,\n",
        "        chunk_overlap=300,\n",
        "    )\n",
        "\n",
        "    policy_file = './rag/company_policies.pdf'\n",
        "    print(f\"[{policy_file}] 문서 로드 중...\")\n",
        "\n",
        "    # 벡터스토어 업데이트\n",
        "    policy_vectorstore =  Chroma(\n",
        "    persist_directory=\"./chroma_db_policy_\"+str(uuid.uuid4())[:5],\n",
        "    embedding_function=embeddings,\n",
        "    collection_name=\"policy_collection\"\n",
        "    )\n",
        "    docs = PyMuPDFLoader(policy_file).load()\n",
        "    split_docs = text_splitter.split_documents(docs)\n",
        "    policy_vectorstore.add_documents(split_docs)\n",
        "    print(f\"[{policy_file}] 청킹 완료: {len(split_docs)}개의 청크로 분할됨\")\n",
        "\n",
        "\n",
        "    # 벡터 + BM25 리트리버 생성 (Top 5)\n",
        "    policy_vector_retriever = policy_vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "    policy_bm25_retriever = BM25Retriever.from_documents(split_docs,\n",
        "                                                          preprocess_func=kiwi_tokenize)\n",
        "    policy_bm25_retriever.k = 5\n",
        "\n",
        "    # 하이브리드 리트리버 생성\n",
        "    policy_retriever = EnsembleRetriever(\n",
        "        retrievers=[policy_bm25_retriever, policy_vector_retriever],\n",
        "        weights=[0.5, 0.5]\n",
        "    )\n",
        "    print(f\"[{policy_file}] 하이브리드 검색 설정됨\")\n",
        "\n",
        "\n",
        "    ###############\n",
        "\n",
        "\n",
        "    # 사용자 정보 처리\n",
        "    user_file = './rag/employee_data.csv'\n",
        "    print(f\"[{user_file}] 문서 로드 중...\")\n",
        "\n",
        "\n",
        "    try:\n",
        "        docs = CSVLoader(user_file).load()\n",
        "    except:\n",
        "        docs = CSVLoader(user_file, encoding='cp949').load()\n",
        "    # CSV 파일의 경우, 하나의 document가 작으므로 결합하여 청킹\n",
        "    hr_info = Document(page_content=\"사용자 정보:\")\n",
        "    for doc in docs:\n",
        "        hr_info.page_content += f\"\\n{doc.page_content}+\\n\"\n",
        "\n",
        "\n",
        "    # 짧은 맥락으로 이해 가능한 경우, 청크 크기 줄이기\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=2000,\n",
        "        chunk_overlap=200,\n",
        "    )\n",
        "\n",
        "    split_docs = text_splitter.split_documents([hr_info])\n",
        "\n",
        "    # 벡터 DB 생성\n",
        "    user_vectorstore =  Chroma(\n",
        "    persist_directory=\"./chroma_db_user_\"+str(uuid.uuid4())[:5],\n",
        "    embedding_function=embeddings,\n",
        "    collection_name=\"user_collection\"\n",
        "    )\n",
        "\n",
        "    print(f\"[{user_file}] 청킹 완료: {len(split_docs)}개의 청크로 분할됨\")\n",
        "\n",
        "    # 벡터 + BM25 리트리버 생성 (Top 5)\n",
        "    user_vector_retriever = user_vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "    user_bm25_retriever = BM25Retriever.from_documents(split_docs, preprocess_func=kiwi_tokenize)\n",
        "    user_bm25_retriever.k = 5\n",
        "\n",
        "    # 하이브리드 리트리버 생성\n",
        "    user_retriever = EnsembleRetriever(\n",
        "        retrievers=[user_bm25_retriever, user_vector_retriever],\n",
        "        weights=[0.7, 0.3]\n",
        "        # 이름 등의 고유명사가 중요한 경우 BM25 가중치 높임\n",
        "    )\n",
        "\n",
        "    print(f\"[{user_file}] 하이브리드 검색 설정됨\")\n",
        "\n",
        "    # 양식 템플릿 처리\n",
        "    form_dir = './rag/templates'\n",
        "    print(f\"[{form_dir}] 문서 로드 중...\")\n",
        "\n",
        "    docs = DirectoryLoader(form_dir, glob=\"*.md\").load()\n",
        "    print(f\"[{form_dir}] 문서 로드 완료\")\n",
        "\n",
        "    # 벡터 DB 생성\n",
        "    form_vectorstore =  Chroma(\n",
        "    persist_directory=\"./chroma_db_form_\"+str(uuid.uuid4())[:5],\n",
        "    embedding_function=embeddings,\n",
        "    collection_name=\"form_collection\"\n",
        "    )\n",
        "\n",
        "    # 벡터 리트리버 생성\n",
        "    form_vector_retriever = form_vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
        "\n",
        "    # BM25 리트리버 생성\n",
        "    form_bm25_retriever = BM25Retriever.from_documents(docs, preprocess_func=kiwi_tokenize)\n",
        "    form_bm25_retriever.k = 4\n",
        "\n",
        "    # 하이브리드 리트리버 생성\n",
        "    form_retriever = EnsembleRetriever(\n",
        "        retrievers=[form_bm25_retriever, form_vector_retriever],\n",
        "        weights=[0.5, 0.5]\n",
        "    )\n",
        "    print(f\"[{form_dir}] 하이브리드 검색 설정됨\")\n",
        "\n",
        "    # 리트리버 맵 반환\n",
        "    return {\n",
        "        \"policy\": policy_retriever,\n",
        "        \"user\": user_retriever,\n",
        "        \"form\": form_retriever\n",
        "    }\n",
        "\n",
        "# 2. 데이터 로드 및 리트리버 생성\n",
        "retriever_map = load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpJX0qgXNe3G"
      },
      "source": [
        "검색과 답변 모듈을 구성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bamv_q45NONu"
      },
      "outputs": [],
      "source": [
        "retriever_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tGdff6tNe3G"
      },
      "outputs": [],
      "source": [
        "def retrieve_context(state: State) -> State:\n",
        "\n",
        "    query = state['query']\n",
        "    target_db = state['target_db']\n",
        "\n",
        "    # Retriever 선택\n",
        "    retriever = retriever_map.get(target_db)\n",
        "    if not retriever:\n",
        "        state['error'] = f\"Unknown DB: {target_db}\"\n",
        "        return state\n",
        "\n",
        "    results = retriever.invoke(query)\n",
        "\n",
        "    state['context'] = [doc.page_content for doc in results]\n",
        "    state['retrieval_count']+=1\n",
        "    return state\n",
        "\n",
        "def generate_response(state: State) -> State:\n",
        "    \"\"\"검색 결과를 바탕으로 응답 생성 및 개선\"\"\"\n",
        "    prompt = ChatPromptTemplate([\n",
        "        (\"system\", \"\"\"사용자의 [질의]와 이에 대한 [중간 답변], 그리고 [추가 정보]가 주어집니다.\n",
        "이를 활용하여, 중간 답변을 개선하세요.\n",
        "추가 정보에 포함된 내용만을 사용해 개선하세요.\n",
        "개선된 답변만 출력하고, 질의에 관련된 내용만 개선하세요.\"\"\"),\n",
        "        (\"user\", \"\"\"\n",
        "[질의]: {question}\n",
        "---\n",
        "[중간 답변]: {draft}\n",
        "---\n",
        "[추가 정보]:{context}\"\"\")\n",
        "    ])\n",
        "\n",
        "    chain = prompt | llm\n",
        "    draft = chain.invoke({\n",
        "        \"question\": state['question'],\n",
        "        \"draft\": state['draft'],\n",
        "        \"context\": \"\\n\".join(state['context'])\n",
        "    })\n",
        "\n",
        "    state['draft'] = draft.content\n",
        "    return state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIViHgfENe3G"
      },
      "source": [
        "의도 분류 모듈을 작성합니다. IntentResult를 통해 간단히 구현합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twyZh6d2Ne3G"
      },
      "outputs": [],
      "source": [
        "# 의도 분류\n",
        "def classify_intent(state: State) -> State:\n",
        "    \"\"\"사용자 질의 의도 분류 및 검색어 생성\"\"\"\n",
        "    prompt = ChatPromptTemplate([\n",
        "        (\"system\", \"\"\"당신은 다중 검색 엔진의 사전 분류기입니다.\n",
        "사용자의 질문에 대해, 검색이 필요한지의 여부,\n",
        "질문에 대해 답변하기 위해 필요한 DB의 이름과, 검색 쿼리를 생성하세요.\n",
        "문서 작성의 경우 우선순위는 form>user 입니다.\"\"\"),\n",
        "        (\"user\", \"질의: {question}\")\n",
        "    ])\n",
        "\n",
        "    chain = prompt | llm.with_structured_output(IntentResult)\n",
        "    result = chain.invoke({\"question\": state['question']})\n",
        "    state['query'] = result.query\n",
        "    state['target_db'] = result.target_db\n",
        "    state['retrieval_needed'] = result.retrieval_needed\n",
        "    return state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9ABE6PVNONu"
      },
      "source": [
        "추가적으로, 결과물을 평가하는 단계를 추가합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_opi9GQ8NONu"
      },
      "outputs": [],
      "source": [
        "# 의도 분류\n",
        "def evaluate(state: State) -> State:\n",
        "    \"\"\"출력 결과 평가 및 개선점 or 추가 검색 사항 찾기\"\"\"\n",
        "    prompt = ChatPromptTemplate([\n",
        "        (\"system\", \"\"\"당신은 문서 작성 자동화를 돕는 에이전트입니다.\n",
        "현재의 답변에서는 추가적인 DB 검색을 통해 채울 수 있는 내용이 들어있을 수 있습니다.\n",
        "비어 있는 부분을 채우기 위해 다른 DB 검색을 통해 보완해야 하는지 판별하세요.\n",
        "이미 충분한 정보가 주어진 경우에는, 추가로 검색할 필요가 없습니다.\"\"\"),\n",
        "        (\"user\", \"\"\"질의: {question}\n",
        "---\n",
        "[이전 검색 DB]: {target_db}\n",
        "[이전 검색 쿼리]: {query}\n",
        "[이전 검색 결과]: {context}\n",
        "---\n",
        "현재 답변: {draft}\"\"\")\n",
        "    ])\n",
        "\n",
        "    chain = prompt | llm.with_structured_output(IntentResult)\n",
        "    result = chain.invoke(state)\n",
        "    state['retrieval_needed'] = result.retrieval_needed\n",
        "    state['target_db'] = result.target_db\n",
        "    state['query'] = result.query\n",
        "\n",
        "    print('## Evaluation:', result)\n",
        "    return state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPDYnWA4Ne3G"
      },
      "source": [
        "Workflow의 라우터를 구성합니다.   \n",
        "만약, 오류가 발생한 경우 에러 처리를 수행합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52VH41HeNe3G"
      },
      "outputs": [],
      "source": [
        "def handle_error(state: State) -> State:\n",
        "    \"\"\"에러 처리\"\"\"\n",
        "    if state.get('error'):\n",
        "        state['draft'] = f\"죄송합니다. 오류가 발생했습니다: {state['error']}\"\n",
        "        state['should_end'] = True\n",
        "    return state\n",
        "\n",
        "def should_continue(state: State) -> List[str]:\n",
        "    \"\"\"워크플로우 계속 진행 여부 확인\"\"\"\n",
        "    if state.get('error'):\n",
        "        return ['handle_error']\n",
        "    if state.get('retrieval_count')>=3:\n",
        "        return ['END']\n",
        "    if state.get('retrieval_needed'):\n",
        "        return 'retrieve_context'\n",
        "\n",
        "    else: return ['END']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppiogO8ANe3G"
      },
      "source": [
        "그래프를 만들고, Compile합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okvZfG4MNe3G"
      },
      "outputs": [],
      "source": [
        "# 워크플로우 그래프 구성\n",
        "builder = StateGraph(State)\n",
        "\n",
        "# 노드 추가\n",
        "builder.add_node('classify_intent', classify_intent)\n",
        "builder.add_node('retrieve_context', retrieve_context)\n",
        "builder.add_node('generate_response', generate_response)\n",
        "builder.add_node('handle_error', handle_error)\n",
        "builder.add_node('evaluate', evaluate)\n",
        "\n",
        "# 엣지 연결\n",
        "builder.add_edge(START,'classify_intent')\n",
        "builder.add_edge('classify_intent', 'retrieve_context')\n",
        "builder.add_edge('retrieve_context', 'generate_response')\n",
        "builder.add_edge('generate_response', 'evaluate')\n",
        "builder.add_conditional_edges(\n",
        "    'evaluate',\n",
        "    should_continue,\n",
        "    {'END':END, 'handle_error':'handle_error', 'retrieve_context': 'retrieve_context'}\n",
        ")\n",
        "\n",
        "# 시작 노드 설정\n",
        "\n",
        "\n",
        "# 그래프 컴파일\n",
        "graph = builder.compile()\n",
        "\n",
        "graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkSN1TSiNe3H"
      },
      "source": [
        "기존의 플로우에 추가된 노드인   \n",
        " 추가 검색을 판단하는 Evaluation 노드의 작동을 확인합니다.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EM3ddcfGNe3H"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "test_questions = [\n",
        "    \"회사의 휴가 정책에 대해 알려주세요\",\n",
        "    \"김지훈 직원의 스킬셋이 뭔가요?\",\n",
        "    \"휴가신청서 양식을 보여주세요\",\n",
        "]\n",
        "\n",
        "for question in test_questions:\n",
        "    result = graph.invoke({'question': question, 'draft':'',\n",
        "                           'retrieval_count':0})\n",
        "    print('Question:', result['question'],\"\\nResult:\", result['draft'])\n",
        "    print('\\n'+\"-\" * 50+'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jx2gcetdrGu"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "test_questions = [\n",
        "    \"스킬셋에 Java 가 있는 직원 모두 알려줘.\",\n",
        "]\n",
        "\n",
        "for question in test_questions:\n",
        "    result = graph.invoke({'question': question, 'draft':'',\n",
        "                           'retrieval_count':0})\n",
        "    print('Question:', result['question'],\"\\nResult:\", result['draft'])\n",
        "    print('\\n'+\"-\" * 50+'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZlIrHebNe3H"
      },
      "outputs": [],
      "source": [
        "# 여러 개의 RAG가 모두 활용되어야 하는 경우\n",
        "# + 반복 실행이 필요한 경우\n",
        "\n",
        "test_questions = [\n",
        "    \"야 나 김지훈인데, 25년 3월 22일부터 26일까지 여행 가게 휴가 신청서 작성해줘.\",\n",
        "]\n",
        "\n",
        "for question in test_questions:\n",
        "    result = graph.invoke({'question': question, 'draft':'',\n",
        "                           'retrieval_count':0})\n",
        "    print('Question:', result['question'],\"\\nResult:\", result['draft'])\n",
        "    print('\\n'+\"-\" * 50+'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "78ES0mz2OH1s"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "multicampus",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
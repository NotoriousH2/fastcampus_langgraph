{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHQzg8LlBPiX"
      },
      "source": [
        "## 중요) 이 실습은 GPU 연결이 필요합니다.\n",
        "**오른쪽 위 ▼ 화살표 클릭 --> 런타임 유형 변경 --> T4 GPU 설정**  \n",
        "이후 아래 코드 실행해 주세요.\n",
        "\n",
        "-----\n",
        "\n",
        "<br><br>\n",
        "# [실습] LangChain을 이용한 RAG 만들기\n",
        "\n",
        "RAG는 Retrieval-Augmented Generation (RAG) 의 약자로, 질문이 주어지면 관련 있는 문서를 찾아 프롬프트에 추가하는 방식의 어플리케이션입니다.   \n",
        "RAG의 과정은 아래와 같이 진행됩니다.\n",
        "1. Indexing : 문서를 받아 검색이 잘 되도록 저장합니다.\n",
        "1. Processing : 입력 쿼리를 전처리하여 검색에 적절한 형태로 변환합니다<br>(여기서는 수행하지 않습니다)\n",
        "1. Search(Retrieval) : 질문이 주어진 상황에서 가장 필요한 참고자료를 검색합니다.\n",
        "1. Augmenting : Retrieval의 결과와 입력 프롬프트를 이용해 LLM에 전달할 프롬프트를 생성합니다.\n",
        "1. Generation : LLM이 출력을 생성합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yD8cevFKBPia"
      },
      "source": [
        "이번 실습에서는 웹 페이지의 결과를 받아와, 이를 기반으로 RAG를 수행하는 프로그램을 만들어 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHgrhVbLnM6I"
      },
      "outputs": [],
      "source": [
        "# 랭체인\n",
        "!pip install langchain langchain-community langchain-google-genai langchain-chroma chromadb langchain_huggingface -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "위에 발생하는 에러는 실행과 무관합니다."
      ],
      "metadata": {
        "id": "Si0sWOsX8rPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 수집/전처리\n",
        "!pip install rank-bm25 kiwipiepy sentence_transformers beautifulsoup4 -q"
      ],
      "metadata": {
        "id": "91hEIto-61ON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [중요] 설치 후, **런타임 --> 세션 다시 시작** 후 실행해 주세요!"
      ],
      "metadata": {
        "id": "L0EzTIXs7wMq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5uYZQknXLDM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['GOOGLE_API_KEY'] = 'AIzaSyBbKlO_udgEoOLhdVD5ekl5Edbw0WpqunQ'\n",
        "os.environ['USER_AGENT'] = 'test'\n",
        "\n",
        "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Gemini API는 분당 10개 요청으로 제한\n",
        "# 즉, 초당 약 0.167개 요청 (10/60)\n",
        "rate_limiter = InMemoryRateLimiter(\n",
        "    requests_per_second=0.167,  # 분당 10개 요청\n",
        "    check_every_n_seconds=0.1,  # 100ms마다 체크\n",
        "    max_bucket_size=10,  # 최대 버스트 크기\n",
        ")\n",
        "\n",
        "# rate limiter를 LLM에 적용\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    rate_limiter=rate_limiter,\n",
        "    temperature = 0.5,\n",
        "    max_tokens = 2048\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5aalGFW30h5"
      },
      "outputs": [],
      "source": [
        "# Test\n",
        "response = llm.invoke(\"알리바바의 최신 언어 모델은 무엇입니까?\")\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8tnZzwABPid"
      },
      "source": [
        "## 1. `WebBaseLoader`로 웹 페이지 받아오기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVOZC5YqBPid"
      },
      "source": [
        "LangChain의 `document_loaders`는 다양한 형식의 파일을 불러올 수 있었는데요.\n",
        "[https://python.langchain.com/docs/integrations/document_loaders/ ]    \n",
        "\n",
        "이번에는 웹 페이지를 로드하는 `WebBaseLoader`를 통해 뉴스 기사를 읽어보겠습니다.    \n",
        "WebBaseLoader는 URL의 내용을 불러오므로, URL 리스트를 먼저 전달해야 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M23Hd2pQdkNK"
      },
      "source": [
        "#### 네이버 검색 연동하기\n",
        "네이버 API를 사용해, 네이버 뉴스 검색 링크를 가져옵니다.   \n",
        "(https://developers.naver.com/apps/#/register?defaultScope=search)   \n",
        "\n",
        "API 사용 인증 후, 애플리케이션 등록을 통해 ID과 Secret를 발급합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8YdRdqnuXbH"
      },
      "outputs": [],
      "source": [
        "# 스포츠 뉴스는 형식이 달라서 지원하지 않습니다...\n",
        "\n",
        "import requests\n",
        "def get_naver_news_links(query, num_links=100):\n",
        "    url = f\"https://openapi.naver.com/v1/search/news.json?query={query}&display={num_links}&sort=sim\"\n",
        "    # 최대 100개의 결과를 표시\n",
        "    headers = {\n",
        "        'X-Naver-Client-Id': 'Ko6yIqbV2TOHq9rPH8tu',\n",
        "        'X-Naver-Client-Secret': 'BvqX8mNtHu'\n",
        "    }\n",
        "\n",
        "    response = requests.get(url, headers=headers)\n",
        "    result = response.json()\n",
        "    # 특정 링크 형식만 필터링\n",
        "    filtered_links = []\n",
        "    for item in result['items']:\n",
        "        link = item['link']\n",
        "        if \"n.news.naver.com/mnews/article/\" in link:\n",
        "            # 네이버 뉴스 스타일만 모으기\n",
        "            filtered_links.append(link)\n",
        "\n",
        "    print(query, ':', len(filtered_links), 'Example:', filtered_links[0])\n",
        "    return filtered_links\n",
        "\n",
        "filtered_links = []\n",
        "for topic in ['메타', '오픈AI', 'XAI', '앤트로픽','구글','알리바바']:\n",
        "    filtered_links += get_naver_news_links(topic, 100)\n",
        "print('Total Articles:', len(filtered_links))\n",
        "print('Total Articles(Without Duplicate):',len(list(set(filtered_links))))\n",
        "filtered_links = list(set(filtered_links))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO-iMtosurI_"
      },
      "source": [
        "WebBaseLoader를 이용해, 링크로부터 본문을 불러옵니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K21DHXavwoff"
      },
      "outputs": [],
      "source": [
        "# Jupyter 분산 처리를 위한 설정\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEmvVcecuqsm"
      },
      "outputs": [],
      "source": [
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "async def get_news_documents(links):\n",
        "    loader = WebBaseLoader(\n",
        "        web_paths=links,\n",
        "        bs_kwargs=dict(\n",
        "            parse_only=bs4.SoupStrainer(\n",
        "                class_=(\"newsct\", \"newsct-body\")\n",
        "                # newsct, newsct-body만 추출 : 네이버 뉴스 포맷에 맞는 HTML 요소\n",
        "            )\n",
        "        ),\n",
        "        requests_per_second = 10, # 1초에 10개 요청 보내기\n",
        "        show_progress = True # 진행 상황 출력\n",
        "    )\n",
        "    # docs = loader.load() # 기본 코드\n",
        "    docs = []\n",
        "\n",
        "    async for doc in loader.alazy_load():\n",
        "        docs.append(doc)\n",
        "    return docs\n",
        "\n",
        "docs = await get_news_documents(filtered_links)\n",
        "print(len(docs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73nVmzJLBPie"
      },
      "outputs": [],
      "source": [
        "docs[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VohN5U6jzkbJ"
      },
      "source": [
        "불필요한 내용을 전처리합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VLH9vOHzls-"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def preprocess(docs):\n",
        "    noise_texts = [\n",
        "        '''구독중 구독자 0 응원수 0 더보기''',\n",
        "        '''쏠쏠정보 0 흥미진진 0 공감백배 0 분석탁월 0 후속강추 0''',\n",
        "        '''댓글 본문 요약봇 본문 요약봇''',\n",
        "        '''도움말 자동 추출 기술로 요약된 내용입니다. 요약 기술의 특성상 본문의 주요 내용이 제외될 수 있어, 전체 맥락을 이해하기 위해서는 기사 본문 전체보기를 권장합니다. 닫기''',\n",
        "        '''텍스트 음성 변환 서비스 사용하기 성별 남성 여성 말하기 속도 느림 보통 빠름''',\n",
        "        '''이동 통신망을 이용하여 음성을 재생하면 별도의 데이터 통화료가 부과될 수 있습니다. 본문듣기 시작''',\n",
        "        '''닫기 글자 크기 변경하기 가1단계 작게 가2단계 보통 가3단계 크게 가4단계 아주크게 가5단계 최대크게 SNS 보내기 인쇄하기''',\n",
        "        'PICK 안내 언론사가 주요기사로선정한 기사입니다. 언론사별 바로가기 닫기',\n",
        "        '응원 닫기',\n",
        "        '구독 구독중 구독자 0 응원수 0 ',\n",
        "\n",
        "    ]\n",
        "\n",
        "    def clean_text(doc):\n",
        "        text = doc.page_content\n",
        "        # 탭과 개행문자를 공백으로 변환\n",
        "        text = text.replace('\\t', ' ').replace('\\n', ' ')\n",
        "\n",
        "        # 연속된 공백을 하나로 치환\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        # 여러 구분자를 한번에 처리\n",
        "        split_markers = [\n",
        "            '구독 해지되었습니다.',\n",
        "            '구독 메인에서 바로 보는 언론사 편집 뉴스 지금 바로 구독해보세요!'\n",
        "        ]\n",
        "\n",
        "\n",
        "        for marker in split_markers:\n",
        "            parts = text.split(marker)\n",
        "            if len(parts) > 1:\n",
        "                if marker == '구독 해지되었습니다.':\n",
        "                    text = parts[1]  # 뒷부분 사용\n",
        "                else:\n",
        "                    text = parts[0]  # 앞부분 사용\n",
        "\n",
        "\n",
        "        # 노이즈 텍스트 제거\n",
        "        for noise in noise_texts:\n",
        "            text = text.replace(noise, '')\n",
        "\n",
        "        # 연속된 공백을 하나로 치환\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        doc.page_content = text\n",
        "\n",
        "        return doc\n",
        "\n",
        "    preprocessed_docs = []\n",
        "    for doc in docs:\n",
        "        # 텍스트 정제\n",
        "        doc= clean_text(doc)\n",
        "        preprocessed_docs.append(doc)\n",
        "\n",
        "    return preprocessed_docs\n",
        "\n",
        "preprocessed_docs = preprocess(docs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sT7hRU6wBPie"
      },
      "source": [
        "## 2. Chunking: 청크 단위로 나누기   \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEyCdybpBPie"
      },
      "source": [
        "전처리가 완료된 docs를 chunk 단위로 분리합니다.\n",
        "`chunk_size`와 `chunk_overlap`을 이용해 청크의 구성 방식을 조절할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4SopeO4tBPie"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain import hub\n",
        "\n",
        "\n",
        "# 청크 사이즈는 RAG 성능에 매우 중요한 역할을 수행합니다!\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=600)\n",
        "# 0~3000, 2400~5400, 4800~7800, ...\n",
        "chunks = text_splitter.split_documents(preprocessed_docs)\n",
        "print(len(chunks))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zwbv-FQ2BPif"
      },
      "source": [
        "구성된 청크를 벡터 데이터베이스에 로드합니다.   \n",
        "`Chroma.from_documents`는 documents의 임베딩을 구하고 이를 DB에 저장합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fhrg3PtCBPif"
      },
      "outputs": [],
      "source": [
        "from langchain_chroma import Chroma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iirue0f2ILJY"
      },
      "source": [
        "벡터 데이터베이스에 데이터를 저장하기 위해, 임베딩 모델을 선정합니다.   \n",
        "OpenAI의 `text-embedding-3-large`나, Google의 Gemini Embedding은 빠른 속도로 연산이 가능하나, 유료 모델입니다.   \n",
        "(Gemini Embedding은 일 사용량 100회로 매우 부족합니다.)\n",
        "\n",
        "이에 따라, 오프라인 사용이 가능한 허깅페이스에서 공개 임베딩 모델을 사용하여 구현해 보겠습니다.\n",
        "\n",
        "\n",
        "#### 오픈 임베딩 모델 사용하기   \n",
        "- intfloat/multilingual-e5-small (500MB)   \n",
        "Multilingual-E5 시리즈는 마이크로소프트의 다국어 공개 임베딩 모델입니다.\n",
        "\n",
        "- BAAI/bge-m3 (2GB)\n",
        "BGE-M3 시리즈는 BAAI의 임베딩 모델로, 현재 가장 인기가 많은 모델입니다.\n",
        "\n",
        "- nlpai-lab/KURE-v1 (2GB)    \n",
        "KURE 임베딩은 고려대학교 NLP 연구실에서 만든 모델로, BGE-M3를 한국어 텍스트로 파인 튜닝한 모델입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8szmXklD4h6X"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model_name = 'nlpai-lab/KURE-v1'\n",
        "# CPU 설정으로 모델 불러오기\n",
        "\n",
        "emb_model = SentenceTransformer(model_name, device='cpu')\n",
        "# 코랩 이외의 환경에서 불러오는 경우, 위 코드에 token='' 으로 HuggingFace Read 권한 토큰을 추가해야 할 수 있음\n",
        "\n",
        "# 로컬 폴더에 모델 저장하기\n",
        "emb_model.save('./embedding')\n",
        "\n",
        "del emb_model\n",
        "\n",
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qW90zzuG4h6X"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "# 허깅페이스 포맷의 임베딩 모델 불러오기\n",
        "embeddings = HuggingFaceEmbeddings(model_name= './embedding',\n",
        "                                   model_kwargs={'device':'cuda'}) # gpu 사용하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAAhmf2GBPif"
      },
      "outputs": [],
      "source": [
        "Chroma().delete_collection() # 메모리에 로드된 기존 데이터 삭제\n",
        "\n",
        "db = Chroma.from_documents(documents=chunks,\n",
        "                           embedding=embeddings,\n",
        "                           persist_directory=\"./chroma_web\",\n",
        "                           collection_name='web', # DB 구분 이름\n",
        "\n",
        "                           collection_metadata={'hnsw:space':'l2'}\n",
        "                           # l2 메트릭 설정(기본값)\n",
        "                           # cosine, mmr\n",
        "                           )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIuEJaToBPif"
      },
      "source": [
        "retriever는 query에 맞춰 db에서 문서를 검색합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4AhVSXLBPif"
      },
      "outputs": [],
      "source": [
        "retriever = db.as_retriever(search_kwargs={\"k\": 5})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLCZKMIRnvi6"
      },
      "outputs": [],
      "source": [
        "retriever.invoke(\"도메인 특화 언어 모델\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OX6QQUh24h6X"
      },
      "source": [
        "#### 한국어 키워드 기반 검색 추가하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jezmMdsP4h6X"
      },
      "source": [
        "임베딩 기반의 시맨틱 검색에 추가로 키워드 검색을 연동해 보겠습니다.   \n",
        "랭체인의 기본 라이브러리는 키워드 기반의 `BM25Retriever`를 지원하나, 한국어 처리를 위해서는 추가 설정이 필요합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRagVnd04h6X"
      },
      "outputs": [],
      "source": [
        "# Kiwi 형태소 분석기\n",
        "from kiwipiepy import Kiwi\n",
        "\n",
        "kiwi = Kiwi()\n",
        "def kiwi_tokenize(text):\n",
        "    return [token.form for token in kiwi.tokenize(text)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnyE2oc44h6X"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "\n",
        "# 키위 형태소 분석기로 청크를 분리한 뒤, 키워드 집합 추출\n",
        "# 해당 키워드 집합으로 인덱싱\n",
        "bm25_retriever = BM25Retriever.from_documents(chunks, preprocess_func = kiwi_tokenize)\n",
        "bm25_retriever.k = 5\n",
        "\n",
        "retriever = db.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=[bm25_retriever, retriever], weights=[0.5, 0.5]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxR-UElW4h6X"
      },
      "outputs": [],
      "source": [
        "ensemble_retriever"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7i4qL0ZBPif"
      },
      "source": [
        "## 3. Prompting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5QpFAyVMYpV"
      },
      "source": [
        "RAG를 위한 간단한 프롬프트를 작성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BiDkZRlBPig"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RS06ZDSDBPig"
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate([\n",
        "    (\"user\", '''당신은 QA(Question-Answering)을 수행하는 Assistant입니다.\n",
        "다음의 Context를 이용하여 Question에 답변하세요.\n",
        "만약 모든 Context를 다 확인해도 정보가 없다면,\n",
        "\"정보가 부족하여 답변할 수 없습니다.\"를 출력하세요.\n",
        "---\n",
        "Context: {context}\n",
        "---\n",
        "Question: {question}''')])\n",
        "prompt.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9C09vkykMwXo"
      },
      "source": [
        "## 4. Chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H98LGZRVBPig"
      },
      "source": [
        "RAG를 수행하기 위한 Chain을 만듭니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kqWGHwuBPig"
      },
      "source": [
        "RAG Chain은 프롬프트에 context와 question을 전달해야 합니다.    \n",
        "체인의 입력은 Question만 들어가므로, Context를 동시에 prompt에 넣기 위해서는 아래의 구성이 필요합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEr1E0zbBPig"
      },
      "outputs": [],
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "\n",
        "# retriever의 결과물은 List[Document] 이므로 이를 ---로 구분하는 함수\n",
        "# metadata의 source를 보존하여 추가\n",
        "def format_docs(docs):\n",
        "    return \" \\n\\n---\\n\\n \".join(['URL: '+ doc.metadata['source'] + '\\n기사 내용: ' +doc.page_content for doc in docs])\n",
        "    # join : 구분자를 기준으로 스트링 리스트를 하나의 스트링으로 연결\n",
        "\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": ensemble_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    # retriever : question을 받아서 context 검색: document 반환\n",
        "    # format_docs : document 형태를 받아서 텍스트로 변환\n",
        "    # RunnablePassthrough(): 체인의 입력을 그대로 저장\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjSGAEkg4h6a"
      },
      "outputs": [],
      "source": [
        "print(format_docs(ensemble_retriever.invoke(\"\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nV9-CYSzBPig"
      },
      "outputs": [],
      "source": [
        "rag_chain.invoke(\"XAI의 언어 모델 그록에 대해 설명해 주세요.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0j9J2CpsBPig"
      },
      "outputs": [],
      "source": [
        "rag_chain.invoke(\"OpenAI의 최근 기술 발전 성과는? 관련 링크도 보여주세요\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhgi0WUEzxvF"
      },
      "outputs": [],
      "source": [
        "rag_chain.invoke(\"알리바바의 언어 모델 이름은?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8br1RVEwkVC4"
      },
      "source": [
        "만약 Context가 포함된 RAG 결과를 보고 싶다면, RunnableParallel을 사용하면 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37hTqiCOzxvF"
      },
      "source": [
        "assign()을 이용하면, 체인의 결과를 받아 새로운 체인에 전달하고, 그 결과를 가져옵니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lirfP5Q-HyX"
      },
      "outputs": [],
      "source": [
        "# assign : 결과를 받아서 새로운 인수 추가하고 원래 결과와 함께 전달\n",
        "from langchain_core.runnables import RunnableParallel\n",
        "\n",
        "rag_chain_from_docs = (\n",
        "    prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "rag_chain_with_source = RunnableParallel(\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        ").assign(answer=rag_chain_from_docs)\n",
        "\n",
        "response = rag_chain_with_source.invoke(\"XAI의 언어 모델 그록에 대해 설명해 주세요.\")\n",
        "\n",
        "# retriever가 1번 실행됨\n",
        "# retriever의 실행 결과를 rag_chain_from_docs 에 넘겨주기 때문에\n",
        "\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response['context'])\n",
        "print('--------')\n",
        "print('Question:', response['question'])\n",
        "print('Answer:', response['answer'])"
      ],
      "metadata": {
        "id": "wR7SR7if_KnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5K5PvlBRAHof"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
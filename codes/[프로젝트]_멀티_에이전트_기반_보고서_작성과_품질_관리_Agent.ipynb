{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-3Rdq2GMX_a"
      },
      "source": [
        "# [프로젝트] 멀티 에이전트 기반 보고서 작성과 품질 관리\n",
        "\n",
        "\n",
        "이번 실습에서는, 기업의 재무 공시 정보와 최근 뉴스를 바탕으로 보고서를 작성하는 에이전트를 만들어 보겠습니다.   \n",
        "\n",
        "이는 단순 검색보다는 최신성을 가진 뉴스나 공시 등의 전문 자료의 내용이 중요합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVMeEggrMX_b",
        "outputId": "68f207af-ca23-4039-fb37-19b1bd2b5e1c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install langgraph dotenv arxiv langchain-tavily langchain-community langchain-google-genai pymupdf -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1vGmHUGhAY4",
        "outputId": "7f99ae10-d109-41f9-cdd9-eb7fe0b22a32"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers chromadb langchain-chroma rank-bm25 pymupdf -q\n",
        "# RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSyD70o8hAY5"
      },
      "source": [
        "이번에는 공시 문서 검색을 위해 DART(https://opendart.fss.or.kr/) API 키가 필요합니다.  \n",
        "\n",
        "해당 페이지에서 회원가입 후 API 키를 발급받습니다.   \n",
        "해당 키는 .env의 DART_API_KEY 에 저장해 주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuW14J87MX_b",
        "outputId": "d04614b1-0f93-42cc-d218-453d701ce01d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# GOOGLE_API_KEY, TAVILY_API_KEY, DART_API_KEY 필수\n",
        "# LangSmith (선택)\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32Os1iAWMX_c"
      },
      "source": [
        "## Preliminary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRMlXNvphAY5"
      },
      "source": [
        "### DART 공시 문서 불러오기   \n",
        "공시 문서를 불러올 수 있는 DART API와 같이, 실행할 때마다 파라미터가 달라지는 경우에는 툴로 구성하는 것이 효과적일 수 있습니다.  \n",
        "\n",
        "DART 공시 문서를 API를 통해 불러오는 함수를 구성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpTOS69WhAY5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "from dotenv import load_dotenv\n",
        "import zipfile\n",
        "import io\n",
        "from datetime import datetime, timedelta\n",
        "import shutil\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "\n",
        "def get_dart_documents(corp_name, start_date=None, period=180):\n",
        "    \"\"\"\n",
        "    DART API를 사용하여 특정 회사의 공시문서를 다운로드하는 함수\n",
        "\n",
        "    Args:\n",
        "        corp_name (str): 회사명 (예: '삼성전자')\n",
        "        start_date (str): 시작일 (YYYYMMDD 형식, 기본값: None)\n",
        "        period (int): 검색 기간(일) (기본값: 180일)\n",
        "\n",
        "    Returns:\n",
        "        str: 작업 결과 메시지\n",
        "\n",
        "    Example:\n",
        "    get_dart_documents('삼성전자', None, 120)\n",
        "\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # .env 파일에서 API 키 로드\n",
        "        load_dotenv('.env')\n",
        "        api_key = os.getenv('DART_API_KEY')\n",
        "\n",
        "        # 고유번호 찾기\n",
        "        corp_code = get_corp_code(api_key, corp_name)\n",
        "        if not corp_code:\n",
        "            return f\"{corp_name}의 고유번호를 찾을 수 없습니다.\"\n",
        "\n",
        "        # 날짜 설정\n",
        "        end_date = datetime.now().strftime('%Y%m%d')\n",
        "        if not start_date:\n",
        "            start_date = (datetime.now() - timedelta(days=period)).strftime('%Y%m%d')\n",
        "\n",
        "        # 공시유형 설정 (A: 정기공시)\n",
        "        doc_types = ['A']\n",
        "\n",
        "        # 공시문서 검색\n",
        "        disclosures = search_disclosures(api_key, corp_code, start_date, end_date, doc_types)\n",
        "        if not disclosures or 'list' not in disclosures:\n",
        "            return \"공시문서를 찾을 수 없습니다.\"\n",
        "\n",
        "        # 저장 폴더 설정\n",
        "        base_folder_name = f\"documents_{corp_name}\"\n",
        "        folder_name = base_folder_name\n",
        "        counter = 1\n",
        "\n",
        "        # 폴더가 존재하면 번호를 붙여서 새 폴더명 생성\n",
        "        while os.path.exists(folder_name):\n",
        "            folder_name = f\"{base_folder_name}_{counter}\"\n",
        "            counter += 1\n",
        "\n",
        "        os.makedirs(folder_name, exist_ok=True)\n",
        "\n",
        "        # 검색된 공시문서 중 재무 관련 문서만 다운로드\n",
        "        download_count = 0\n",
        "\n",
        "        for doc in disclosures['list']:\n",
        "            rcept_no = doc['rcept_no']\n",
        "            report_nm = doc['report_nm']\n",
        "\n",
        "            # 재무 관련 보고서만 다운로드\n",
        "            if is_financial_report(report_nm):\n",
        "                # 임시 ZIP 파일 경로\n",
        "                temp_zip_path = f\"{folder_name}/temp_{rcept_no}.zip\"\n",
        "\n",
        "                # 문서 다운로드\n",
        "                success = download_document(api_key, rcept_no, temp_zip_path)\n",
        "\n",
        "                if success:\n",
        "                    # ZIP 파일 압축 풀기 - 별도 폴더 생성 없이 바로 지정된 폴더에 압축 해제\n",
        "                    with zipfile.ZipFile(temp_zip_path, 'r') as zip_ref:\n",
        "                        zip_ref.extractall(folder_name)\n",
        "\n",
        "                    # 임시 ZIP 파일 삭제\n",
        "                    os.remove(temp_zip_path)\n",
        "\n",
        "                    download_count += 1\n",
        "\n",
        "        if download_count > 0:\n",
        "            return (True, folder_name)\n",
        "        else:\n",
        "            return (False, \"재무 관련 공시문서가 없습니다.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"에러가 발생했습니다. {e}\"\n",
        "\n",
        "def get_corp_code(api_key, corp_name):\n",
        "    \"\"\"\n",
        "    회사명으로 고유번호를 찾는 함수\n",
        "\n",
        "    Args:\n",
        "        api_key (str): DART API 키\n",
        "        corp_name (str): 회사명\n",
        "    Returns:\n",
        "        str: 고유번호\n",
        "    \"\"\"\n",
        "    url = 'https://opendart.fss.or.kr/api/corpCode.xml'\n",
        "    params = {\n",
        "        'crtfc_key': api_key\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, params=params)\n",
        "        if response.status_code == 200:\n",
        "            # XML 응답을 파싱하여 회사명에 해당하는 고유번호 찾기\n",
        "            # 실제 구현에서는 XML 파싱 라이브러리 사용 필요\n",
        "            # 예시 코드에서는 임시로 하드코딩된 값 사용\n",
        "            if corp_name == \"삼성전자\":\n",
        "                return \"00126380\"  # 삼성전자 고유번호\n",
        "            else:\n",
        "                return None\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "def search_disclosures(api_key, corp_code, start_date=None, end_date=None, doc_types=None):\n",
        "    \"\"\"\n",
        "    공시문서를 검색하는 함수\n",
        "\n",
        "    Args:\n",
        "        api_key (str): DART API 키\n",
        "        corp_code (str): 고유번호\n",
        "        start_date (str): 시작일 (YYYYMMDD)\n",
        "        end_date (str): 종료일 (YYYYMMDD)\n",
        "        doc_types (list): 검색할 공시유형 목록 (기본값: 정기공시)\n",
        "    Returns:\n",
        "        list: 검색된 공시문서 리스트\n",
        "    \"\"\"\n",
        "    if not doc_types:\n",
        "        doc_types = ['A']  # 기본값: 정기공시(A)\n",
        "\n",
        "    url = 'https://opendart.fss.or.kr/api/list.json'\n",
        "    params = {\n",
        "        'crtfc_key': api_key,\n",
        "        'corp_code': corp_code,\n",
        "        'bgn_de': start_date,\n",
        "        'end_de': end_date,\n",
        "        'page_count': '100'  # 최대 100개까지 검색\n",
        "    }\n",
        "\n",
        "    # 공시유형 필터링\n",
        "    if len(doc_types) == 1:\n",
        "        params['pblntf_ty'] = doc_types[0]\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, params=params)\n",
        "        if response.status_code == 200:\n",
        "            return response.json()\n",
        "        else:\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "def is_financial_report(report_nm):\n",
        "    \"\"\"\n",
        "    보고서가 재무 관련 보고서인지 확인하는 함수\n",
        "\n",
        "    Args:\n",
        "        report_nm (str): 보고서명\n",
        "    Returns:\n",
        "        bool: 재무 관련 보고서이면 True, 아니면 False\n",
        "    \"\"\"\n",
        "    financial_keywords = [\n",
        "        '사업보고서', '분기보고서', '반기보고서', '감사보고서',\n",
        "        '영업(잠정)실적', '매출액', '영업이익', '당기순이익',\n",
        "        '재무제표', '정기주주총회', '실적발표', '결산실적',\n",
        "        '전망', '배당', '유상증자', '타법인주식', '투자판단'\n",
        "    ]\n",
        "\n",
        "    return any(keyword in report_nm for keyword in financial_keywords)\n",
        "\n",
        "def download_document(api_key, rcept_no, save_path):\n",
        "    \"\"\"\n",
        "    DART API를 사용하여 공시문서를 다운로드하는 함수\n",
        "\n",
        "    Args:\n",
        "        api_key (str): DART API 키\n",
        "        rcept_no (str): 접수번호\n",
        "        save_path (str): 저장할 파일 경로\n",
        "    Returns:\n",
        "        bool: 다운로드 성공 여부\n",
        "    \"\"\"\n",
        "    url = 'https://opendart.fss.or.kr/api/document.xml'\n",
        "    params = {\n",
        "        'crtfc_key': api_key,\n",
        "        'rcept_no': rcept_no\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, params=params)\n",
        "        if response.status_code == 200:\n",
        "            with open(save_path, 'wb') as f:\n",
        "                f.write(response.content)\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    except Exception as e:\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vA_TbfO2hAY6",
        "outputId": "087afab6-8a5c-4e42-ce69-3866573c65f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(True, 'documents_삼성전자_5')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corp_name = '삼성전자'\n",
        "\n",
        "result, dart_dir = get_dart_documents(corp_name, 20240501, 120)\n",
        "result, dart_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_0gcgDZhAY6"
      },
      "source": [
        "공시 문서의 형식은 xml이므로, 적절한 방법을 통해 불러와야 합니다.\n",
        "본 실습에서는 하드코딩된 방법을 사용하지만, 추후 더 좋은 툴이 있다면 해당 툴로 변경하는 것이 높은 성능에 도움이 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8X1JvKnhAY6"
      },
      "outputs": [],
      "source": [
        "from glob import glob\n",
        "xml_list = glob(f'./{dart_dir}/*.xml')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7tEvRo-hAY6"
      },
      "outputs": [],
      "source": [
        "# Gemini 2.5 Pro에게 XML 일부를 입력하하고 작성함\n",
        "import re\n",
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def parse_xml(file_path: str) -> str:\n",
        "    \"\"\"\n",
        "    DART 공시 XML 파일을 파싱하여 RAG에 적합한 형식의 텍스트로 변환합니다.\n",
        "\n",
        "    Args:\n",
        "        file_path: 파싱할 XML 파일의 경로.\n",
        "\n",
        "    Returns:\n",
        "        추출 및 정제된 텍스트 전체를 담은 단일 문자열.\n",
        "        파일 처리 중 오류 발생 시 빈 문자열을 반환할 수 있습니다.\n",
        "    \"\"\"\n",
        "    extracted_data = []\n",
        "    current_section_title = \"문서 서두\" # 기본 섹션 제목\n",
        "\n",
        "    # --- 1. 파일 존재 확인 및 읽기 ---\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"오류: 파일을 찾을 수 없습니다 - {file_path}\")\n",
        "        return \"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            xml_content = f.read()\n",
        "    except Exception as e:\n",
        "        print(f\"오류: 파일 읽기 실패 {file_path} - {e}\")\n",
        "        return \"\"\n",
        "\n",
        "    # --- 2. XML 파싱 ---\n",
        "    try:\n",
        "        # lxml 파서가 설치되어 있다면 속도와 안정성 면에서 더 좋습니다.\n",
        "        try:\n",
        "            soup = BeautifulSoup(xml_content, 'lxml-xml')\n",
        "        except ImportError:\n",
        "            print(\"lxml 파서가 없어 내장 'xml' 파서를 사용합니다. 'pip install lxml'로 설치할 수 있습니다.\")\n",
        "            soup = BeautifulSoup(xml_content, 'xml')\n",
        "    except Exception as e:\n",
        "        print(f\"오류: XML 파싱 실패 {file_path} - {e}\")\n",
        "        return \"\"\n",
        "\n",
        "    # --- 3. BODY 태그 찾기 (없으면 전체 문서 처리 시도) ---\n",
        "    body = soup.find('BODY')\n",
        "    if not body:\n",
        "        print(f\"경고: {file_path} 파일에서 <BODY> 태그를 찾을 수 없습니다. 문서 전체를 처리합니다.\")\n",
        "        process_root = soup # BODY 없으면 문서 전체를 기준으로 처리\n",
        "    else:\n",
        "        process_root = body # BODY 태그 내부를 기준으로 처리\n",
        "\n",
        "    # --- 4. 내용 순회 및 추출 ---\n",
        "    try:\n",
        "        # process_root의 모든 자손 태그를 순회\n",
        "        for element in process_root.descendants:\n",
        "            # 유효한 태그 이름이 없는 요소는 건너뛰기 (예: NavigableString)\n",
        "            if not hasattr(element, 'name') or element.name is None:\n",
        "                continue\n",
        "\n",
        "            # --- 섹션 제목 처리 ---\n",
        "            # 실제 DART 문서의 제목 태그 확인 필요 (예: 'TITLE', 'H2', 'H3' 등)\n",
        "            # 목차 항목 제외 (ATOC='N' 또는 다른 구분자 확인)\n",
        "            if element.name == 'TITLE' and element.get('ATOC') != 'N':\n",
        "                title_text = element.get_text(strip=True)\n",
        "                # 제목이 비어있지 않고, 이전 제목과 다를 경우 업데이트\n",
        "                if title_text and title_text != current_section_title:\n",
        "                     current_section_title = title_text\n",
        "                     # 마크다운 헤더 형식으로 추가\n",
        "                     extracted_data.append(f\"\\n## {current_section_title}\\n\")\n",
        "\n",
        "            # --- 문단(P) 처리 ---\n",
        "            elif element.name == 'P':\n",
        "                text = element.get_text(strip=True)\n",
        "                # 문단 내용이 있고, 단순히 숫자만 있는 경우가 아닐 때 추가 (서식용 숫자 제외)\n",
        "                if text and not text.isdigit():\n",
        "                    extracted_data.append(text)\n",
        "\n",
        "            # --- 테이블(TABLE) 처리 ---\n",
        "            elif element.name == 'TABLE':\n",
        "                caption_tag = element.find('CAPTION') # 테이블 설명(캡션) 찾기\n",
        "                caption = caption_tag.get_text(strip=True) if caption_tag else \"표\" # 캡션 없으면 기본값 '표'\n",
        "\n",
        "                rows_data = []\n",
        "                # 테이블의 행(TR 또는 ROW) 찾기 - recursive=False 로 바로 아래 자식만 찾기 시도 가능\n",
        "                for row in element.find_all(['TR', 'ROW']): # 실제 행 태그 확인 필요\n",
        "                    # 행 내부의 셀(TD, TH, CELL, TU) 찾기\n",
        "                    cells = [cell.get_text(strip=True) for cell in row.find_all(['TD', 'TH', 'CELL', 'TU'])] # 실제 셀 태그 확인 필요\n",
        "                    # 빈 셀 제거\n",
        "                    cells = [cell for cell in cells if cell]\n",
        "                    # 셀 내용이 있을 경우에만 행 추가\n",
        "                    if cells:\n",
        "                        rows_data.append(\" | \".join(cells)) # 파이프(|)로 셀 내용 구분\n",
        "\n",
        "                # 추출된 행 데이터가 있을 경우에만 테이블 텍스트 생성 및 추가\n",
        "                if rows_data:\n",
        "                    table_text = f\"\\n[{caption}]\\n\" + \"\\n\".join(rows_data) + \"\\n\"\n",
        "                    extracted_data.append(table_text)\n",
        "\n",
        "            # --- 기타 필요한 태그 처리 ---\n",
        "            # 예: 리스트(UL, OL, LI), 특정 강조(SPAN with attribute) 등 필요시 추가\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"오류: 내용 처리 중 예외 발생 {file_path} - {e}\")\n",
        "        # 부분적으로 추출된 데이터라도 반환할지 결정 (현재는 계속 진행)\n",
        "\n",
        "    # --- 5. 추출된 텍스트 결합 및 최종 정리 ---\n",
        "    final_text = \"\\n\".join(extracted_data)\n",
        "\n",
        "    # 연속된 빈 줄(3개 이상)을 2개로 줄이기\n",
        "    final_text = re.sub(r'\\n{3,}', '\\n\\n', final_text).strip()\n",
        "\n",
        "    # 선택적: 테이블 구분선처럼 보이는 라인 제거 (필요시 주석 해제 및 패턴 조정)\n",
        "    # final_text = \"\\n\".join([line for line in final_text.split('\\n') if not re.match(r'^[\\s|\\-_=]+$', line)])\n",
        "\n",
        "    return final_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x66s4cNahAY7",
        "outputId": "f64dc05d-847e-4878-bd1f-6e1a864f2a8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "29033\n",
            "39526\n",
            "26794\n",
            "44095\n",
            "91141\n",
            "104378\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import UnstructuredXMLLoader\n",
        "from langchain.schema import Document\n",
        "docs = []\n",
        "for xml in xml_list:\n",
        "   doc = Document(page_content = parse_xml(xml), metadata={'type':'DART', 'source':xml})\n",
        "   docs.append(doc)\n",
        "   print(len(doc.page_content))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZHCRgpFhAY7",
        "outputId": "89737ab1-c5ad-4c2f-adba-3653973b53fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "인보우로보틱스㈜ 지분의 전부 또는 일부를 최대주주 등에게 매도할 것을 청구할 권리를 보유하고 있습니다. 2024년 1분기말 현재 해당 콜옵션의 공정가치는 안진회계법인이 평가하였습니다.또한, 종속회사인 삼성디스플레이㈜는 Corning Incorporated와 2021년 4월 8일 실행된 주식매매계약에 따라, 보유 중인 Corning 지분증권 일부를 Corning에게 매각할수 있는 풋옵션을 보유하고 있으며,TCL Technology Group Corporation (TCL) 및 TCL China Star Optoelectronics Technology Co. Ltd. (CSOT)와 2021년 4월 1일 실행된 주주간 계약에 따라 CSOT가 상장기한 내 비상장 시, 삼성디스플레이㈜가 보유 중인 CSOT 지분증권 전부 또는 일부를 TCL에게 매각할 수 있는 풋옵션을 보유하고 있습니다.2024년 1분기말 현재 상기 풋옵션들 공정가치는 한영회계법인이 평가하였습니다.\n",
            "\n",
            "## 6. 주요계약 및 연구개발활동\n",
            "\n",
            "가. 경영상의 주요 계약 등\n",
            "\n",
            "[표]\n",
            "계약 상대방 | 항  목 | 내   용\n",
            "Google | 계약 유형 | 상호 특허 사용 계약\n",
            "체결시기 | 2014.01.25\n",
            "목적 및 내용 | 상호 특허 라이선스 계약 체결을 통한 사업 자유도 확보\n",
            "기타 주요내용 | 영구 라이선스 계약 (향후 10년간 출원될 특허까지 포함)\n",
            "GlobalFoundries | 계약 유형 | 공정 기술 라이선스 계약\n",
            "체결시기 | 2014.02.28\n",
            "목적 및 내용 | 14nm 공정의 고객기반 확대\n",
            "Google | 계약 유형 | EMADA\n",
            "체결시기 및 기간 | 2019.02.27~2024.12.31(연장)\n",
            "목적 및 내용 | 유럽 32개국(EEA) 대상으로 Play Store, YouTube 등 구글 앱 사용에 대한라이선스 계약\n",
            "Ericsson | 계약 유형 | 상호 특허 사용 계약\n",
            "체결시기 | 2021.05.07\n",
            "목적 및 내용 | 상호 특허 라이선스 계약 체결을 통한 사업 자유도 확보\n",
            "Qualcomm | 계약 \n"
          ]
        }
      ],
      "source": [
        "print(docs[0].page_content[20000:21000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCNj5DtwhAY7"
      },
      "source": [
        "각각의 토큰 수도 확인해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KX8k19TmhAY7",
        "outputId": "3578c608-02dc-4fb3-a858-9350d1668953"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FILE: ./documents_삼성전자_5\\20240516001421.xml (29033) \n",
            " total_tokens: 16440\n",
            "\n",
            "FILE: ./documents_삼성전자_5\\20240814003284.xml (39526) \n",
            " total_tokens: 22668\n",
            "\n",
            "FILE: ./documents_삼성전자_5\\20241114002642.xml (26794) \n",
            " total_tokens: 15759\n",
            "\n",
            "FILE: ./documents_삼성전자_5\\20250311001085.xml (44095) \n",
            " total_tokens: 27616\n",
            "\n",
            "FILE: ./documents_삼성전자_5\\20250311001085_00760.xml (91141) \n",
            " total_tokens: 58725\n",
            "\n",
            "FILE: ./documents_삼성전자_5\\20250311001085_00761.xml (104378) \n",
            " total_tokens: 70176\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "model = genai.GenerativeModel(\"models/gemini-2.0-flash\")\n",
        "\n",
        "for doc in docs:\n",
        "    print(f\"FILE: {doc.metadata['source']} ({len(doc.page_content)}) \\n {model.count_tokens(doc.page_content)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcs9LfQNhAY7"
      },
      "source": [
        "해당 문서를 전부 Context로 넣는 것보다는, 청킹을 통해 필요한 부분만 검색할 수 있도록 구성해 봅시다.   \n",
        "랭체인에서는 제미나이 토큰 수 기반의 청킹을 지원하지는 않기 때문에, 글자 수 기반의 청킹을 수행합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgSu9tVfhAY7"
      },
      "source": [
        "### 청크 사이즈와 Top K는 어떻게 잡으면 좋을까요?\n",
        "\n",
        "우리가 사용하는 Gemini 계열의 모델은 1M Context Size기 때문에 많은 내용을 처리할 수 있는데요.   \n",
        "그러나, Context 길이가 상대적으로 짧은 모델들(128k, 200k 또는 그 이하)의 경우는 청크 사이즈를 작게 만드는 것이 더 효과적일 수 있습니다.   \n",
        "\n",
        "\n",
        "\n",
        "또한, Context에 포함하기 위한 Top K를 설정하는 것도 중요합니다.   \n",
        "긴 Context 모델은 Top K를 늘려 풍부한 정보를 파악할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErF2V_7-hAY7",
        "outputId": "1389eff6-0d37-45a8-c3c6-d2666372b66c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "65\n"
          ]
        }
      ],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Context가 작은 모델이라면, 청크를 줄이는 것이 좋습니다.\n",
        "# chunk_size = 3000\n",
        "# chunk_overlap = 600\n",
        "# top_k = 5\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=6000, chunk_overlap=600)\n",
        "\n",
        "chunks = text_splitter.split_documents(docs)\n",
        "print(len(chunks))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myoRuAu8hAY7"
      },
      "source": [
        "각각의 청크를 벡터 DB에 저장합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-3mR87khAY7",
        "outputId": "a5d50c88-eb05-4f1a-f764-8b2cddb8f597"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# 허깅페이스 토큰 로그인: 아래 코드에서 보통 필요하지 않으나, 필요한 경우 READ TOKEN\n",
        "login(token=os.environ['HF_TOKEN'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCuGv1lVhAY7"
      },
      "outputs": [],
      "source": [
        "# 임베딩 모델 준비 (에러 발생시 위 셀에서 토큰으로 로그인)\n",
        "\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "model_name = 'intfloat/multilingual-e5-small'\n",
        "emb_model = SentenceTransformer(model_name, device='cpu')\n",
        "\n",
        "emb_model.save('./embedding')\n",
        "del emb_model\n",
        "\n",
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name= './embedding',\n",
        "                                   model_kwargs={'device':'cuda'})\n",
        "# GPU가 있는 경우, CUDA(GPU) 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vwu1aD1EhAY7",
        "outputId": "7efe8e18-790d-41f9-be4e-61ab8fb4a91f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\pc\\venv\\multicampus\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
            "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
          ]
        }
      ],
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "Chroma().delete_collection()\n",
        "vector_store = Chroma.from_documents(chunks,\n",
        "                                     collection_name = 'DART',\n",
        "                                     # persist_directory=\"./chroma_web\",\n",
        "                                     # 파일 공간에 저장\n",
        "                                     embedding = embeddings)\n",
        "\n",
        "retriever = vector_store.as_retriever(search_kwargs={\"k\": 8})\n",
        "# Small Model은 작게"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_M4oh-HhAY8"
      },
      "source": [
        "### Hybrid RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNlHHG8ThAY8"
      },
      "source": [
        "임베딩 기반 검색도 필요하지만, 전문적인 도메인 문서의 경우에는 키워드 기반의 검색도 필요합니다.   \n",
        "랭체인의 `BM25Retriever`와 `EnsembleRetriever` 를 이용하여 두 검색을 결합해 보겠습니다.   \n",
        "한국어 데이터의 경우, 랭체인의 기본 인덱싱에서 처리하지 못하기 때문에    \n",
        "별도의 형태소 분석기를 추가합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acolmFkmhAY8"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "\n",
        "from kiwipiepy import Kiwi\n",
        "# kiwi 형태소 분석기\n",
        "kiwi = Kiwi()\n",
        "def kiwi_tokenize(text):\n",
        "    return [token.form for token in kiwi.tokenize(text)]\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(chunks, preprocess_func = kiwi_tokenize)\n",
        "bm25_retriever.k = 5\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=[bm25_retriever, retriever], weights=[0.7, 0.3]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGv3zdn4hAY8",
        "outputId": "ecf76c39-704c-466b-8a51-cc5eff254e0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'langchain_core.vectorstores.base.VectorStoreRetriever'>\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'llm' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[25], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m     result \u001b[38;5;241m=\u001b[39m rt\u001b[38;5;241m.\u001b[39minvoke(question)\n\u001b[0;32m      9\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([chunk\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m result])\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mllm\u001b[49m\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124m다음 검색 결과를 보고 질문에 답하세요.\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124m                     \u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \n\u001b[0;32m     16\u001b[0m \u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'''\u001b[39m)\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-----\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'llm' is not defined"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWtJlTP8hAY8"
      },
      "source": [
        "LLM을 설정합니다. 이번에는 하나의 모델만 사용하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "id": "FVSkjUXHMX_c",
        "outputId": "ed3c58ea-52f8-40ea-9a1e-023cb0ea417a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatGoogleGenerativeAI</span><span style=\"font-weight: bold\">(</span>\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">rate_limiter</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">langchain_core.rate_limiters.InMemoryRateLimiter</span><span style=\"color: #000000; text-decoration-color: #000000\"> object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x000001CAF3071050</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;,</span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #808000; text-decoration-color: #808000\">model</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'models/gemini-2.0-flash'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #808000; text-decoration-color: #808000\">google_api_key</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SecretStr</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'**********'</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #808000; text-decoration-color: #808000\">temperature</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #808000; text-decoration-color: #808000\">max_output_tokens</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8192</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #808000; text-decoration-color: #808000\">client</span><span style=\"color: #000000; text-decoration-color: #000000\">=&lt;google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object </span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000\">at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x000001CAA5BEBD90</span><span style=\"font-weight: bold\">&gt;</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">default_metadata</span>=<span style=\"font-weight: bold\">()</span>\n",
              "<span style=\"font-weight: bold\">)</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;35mChatGoogleGenerativeAI\u001b[0m\u001b[1m(\u001b[0m\n",
              "    \u001b[33mrate_limiter\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mlangchain_core.rate_limiters.InMemoryRateLimiter\u001b[0m\u001b[39m object at \u001b[0m\u001b[1;36m0x000001CAF3071050\u001b[0m\u001b[39m>,\u001b[0m\n",
              "\u001b[39m    \u001b[0m\u001b[33mmodel\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'models/gemini-2.0-flash'\u001b[0m\u001b[39m,\u001b[0m\n",
              "\u001b[39m    \u001b[0m\u001b[33mgoogle_api_key\u001b[0m\u001b[39m=\u001b[0m\u001b[1;35mSecretStr\u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'**********'\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
              "\u001b[39m    \u001b[0m\u001b[33mtemperature\u001b[0m\u001b[39m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;36m.8\u001b[0m\u001b[39m,\u001b[0m\n",
              "\u001b[39m    \u001b[0m\u001b[33mmax_output_tokens\u001b[0m\u001b[39m=\u001b[0m\u001b[1;36m8192\u001b[0m\u001b[39m,\u001b[0m\n",
              "\u001b[39m    \u001b[0m\u001b[33mclient\u001b[0m\u001b[39m=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object \u001b[0m\n",
              "\u001b[39mat \u001b[0m\u001b[1;36m0x000001CAA5BEBD90\u001b[0m\u001b[1m>\u001b[0m,\n",
              "    \u001b[33mdefault_metadata\u001b[0m=\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
              "\u001b[1m)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os\n",
        "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
        "from langchain.chat_models import init_chat_model\n",
        "from rich import print as rprint\n",
        "\n",
        "# Gemini API는 분당 10개 요청으로 제한\n",
        "# 즉, 초당 약 0.167개 요청 (10/60)\n",
        "rate_limiter = InMemoryRateLimiter(\n",
        "    requests_per_second=0.167,  # 분당 10개 요청\n",
        "    check_every_n_seconds=0.1,  # 100ms마다 체크\n",
        "    max_bucket_size=10,  # 최대 버스트 크기\n",
        ")\n",
        "\n",
        "quick_rate_limiter = InMemoryRateLimiter(\n",
        "    requests_per_second=0.333,  # 분당 20개 요청\n",
        "    check_every_n_seconds=0.1,  # 100ms마다 체크\n",
        "    max_bucket_size=20,  # 최대 버스트 크기\n",
        ")\n",
        "\n",
        "llm = init_chat_model(\n",
        "    model_provider=\"google_genai\",\n",
        "    model=\"gemini-2.0-flash\",\n",
        "\n",
        "    rate_limiter=rate_limiter,\n",
        "    temperature=0.8,\n",
        "    max_tokens = 8192\n",
        ")\n",
        "rprint(llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5A7Wrvz4hAY8",
        "outputId": "31344499-da17-4be4-a990-80d151c1116b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'langchain_core.vectorstores.base.VectorStoreRetriever'>\n",
            "제공된 자료에서 삼성전자주식회사의 별도재무제표를 기준으로 질문에 대한 답변을 드리겠습니다.\n",
            "\n",
            "*   **기타포괄손익:** 2024년 3분기말에 대한 정보는 없지만, 2024년 말 기준으로 기타포괄손익-공정가치금융자산의 주가가 1% 변동할 경우 미치는 영향에 대한 정보는 제공되지 않았습니다.\n",
            "*   **당기손익:** 2024년 3분기말에 대한 정보는 없지만, 2024년 말 기준으로 당기손익-공정가치금융자산의 주가가 1% 변동할 경우 미치는 영향에 대한 정보는 제공되지 않았습니다.\n",
            "\n",
            "따라서, 제공된 정보만으로는 2024년 3분기말 기준으로 기타포괄손익과 당기손익에 미치는 영향을 정확히 알 수 없습니다.\n",
            "-----\n",
            "<class 'langchain_community.retrievers.bm25.BM25Retriever'>\n",
            "제공된 검색 결과에서 2024년 3분기말 (1분기말) 기준으로 다음 정보를 찾을 수 있습니다.\n",
            "\n",
            "*   **기타포괄손익**: 주가가 1% 변동 시 58,830백만원의 영향\n",
            "*   **당기손익**: 주가가 1% 변동 시 2,333백만원의 영향\n",
            "\n",
            "따라서 2024년 1분기말 기준으로 상장주식의 주가가 1% 변동할 경우, 기타포괄손익에 미치는 영향은 58,830백만원, 당기손익에 미치는 영향은 2,333백만원입니다.\n",
            "-----\n",
            "<class 'langchain.retrievers.ensemble.EnsembleRetriever'>\n",
            "다음과 같습니다.\n",
            "\n",
            "*   **기타포괄손익:** 58,830 백만원\n",
            "*   **당기손익:** 2,333 백만원\n",
            "-----\n"
          ]
        }
      ],
      "source": [
        "# 결과 비교 (Lexical Win)\n",
        "\n",
        "question = '''연결회사의 '기타포괄손익-공정가치 금융자산'과 '당기손익-공정가치 금융자산'으로 분류된 상장주식의 주가가 1% 변동할 경우, 2024년 3분기말 기준으로 기타포괄손익과 당기손익에 미치는 영향(세전)은 각각 얼마인가?'''\n",
        "# 정답: 주가 1% 변동 시 기타포괄손익(법인세효과 반영 전)에 미치는 영향은 58,830백만원이고, 당기손익(법인세효과 반영 전)에 미치는 영향은 2,333백만원 입니다.\n",
        "\n",
        "for rt in [retriever, bm25_retriever, ensemble_retriever]:\n",
        "    print(type(rt))\n",
        "    result = rt.invoke(question)\n",
        "    context = '\\n\\n'.join([chunk.page_content for chunk in result])\n",
        "\n",
        "    print(llm.invoke(f'''\n",
        "다음 검색 결과를 보고 질문에 답하세요.\n",
        "\n",
        "{context}\n",
        "\n",
        "{question}''').content)\n",
        "\n",
        "\n",
        "    print('-----')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_w9R-bEhAY8",
        "outputId": "c56d5303-05cd-453d-977a-c85fbef7ba93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'langchain_core.vectorstores.base.VectorStoreRetriever'>\n",
            "제56기 3분기 매출액 기준으로 매출액 비중이 가장 큰 부문은 DX 부문으로, 1조 3,435,750억 원이며, 이는 전체 매출액의 59.7%를 차지합니다.\n",
            "-----\n",
            "<class 'langchain_community.retrievers.bm25.BM25Retriever'>\n",
            "제공된 검색 결과에서는 삼성전자의 매출액 비중에 대한 직접적인 정보를 찾을 수 없습니다. 재무제표에 대한 감사보고서와 사업 부문별 현황 정보가 있지만, 구체적인 매출액 비중은 포함되어 있지 않습니다. 상세한 매출액 정보는 '상세표-4. 연구개발실적(상세)' 또는 다른 상세 재무 보고서를 참조해야 할 것으로 보입니다.\n",
            "-----\n",
            "<class 'langchain.retrievers.ensemble.EnsembleRetriever'>\n",
            "2024년 3분기 매출액 비중이 가장 큰 부문은 DX 부문으로, 1조 3,435억 7,500만 원입니다.\n",
            "-----\n"
          ]
        }
      ],
      "source": [
        "# 결과 비교 (Semantic Win)\n",
        "\n",
        "question = '''매출액 비중이 가장 큰 데가 어디예요? 얼마인가요?'''\n",
        "# 정답: DX 부문, 매출액은 1조 3,435억 7,500만 원\n",
        "\n",
        "for rt in [retriever, bm25_retriever, ensemble_retriever]:\n",
        "    print(type(rt))\n",
        "    result = rt.invoke(question)\n",
        "    context = '\\n\\n'.join([chunk.page_content for chunk in result])\n",
        "\n",
        "    print(llm.invoke(f'''\n",
        "다음 검색 결과를 보고 질문에 답하세요.\n",
        "\n",
        "{context}\n",
        "\n",
        "{question}''').content)\n",
        "\n",
        "\n",
        "    print('-----')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyuS2NPyhAY8"
      },
      "source": [
        "검색 API를 구성합니다.   \n",
        "Tavily Search도 툴로 만들어 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5iy45oTuhAY8",
        "outputId": "14f4593f-335a-40bd-dc16-eed14fa1f3d2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'function_call'</span>: <span style=\"font-weight: bold\">{</span>\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'web_search'</span>,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'arguments'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'{\"query\": \"\\\\uc0bc\\\\uc131\\\\uc804\\\\uc790 \\\\ucd5c\\\\uadfc \\\\uc2e4\\\\uc801\", \"time_range\": </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">\"month\", \"topic\": \"finance\"}'</span>\n",
              "        <span style=\"font-weight: bold\">}</span>\n",
              "    <span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_feedback'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'block_reason'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'safety_ratings'</span>: <span style=\"font-weight: bold\">[]}</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'finish_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'STOP'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'model_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gemini-2.0-flash'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'safety_ratings'</span>: <span style=\"font-weight: bold\">[]</span>\n",
              "    <span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-fda12434-3df4-4fbb-a89e-0f79b73d8192-0'</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">tool_calls</span>=<span style=\"font-weight: bold\">[</span>\n",
              "        <span style=\"font-weight: bold\">{</span>\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'web_search'</span>,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'args'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'query'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'삼성전자 최근 실적'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'time_range'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'month'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'topic'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'finance'</span><span style=\"font-weight: bold\">}</span>,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'a9702dc9-0a1d-4890-861f-9f7240615011'</span>,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'tool_call'</span>\n",
              "        <span style=\"font-weight: bold\">}</span>\n",
              "    <span style=\"font-weight: bold\">]</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">124</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">142</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'input_token_details'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'cache_read'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">}</span>\n",
              "    <span style=\"font-weight: bold\">}</span>\n",
              "<span style=\"font-weight: bold\">)</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
              "    \u001b[33mcontent\u001b[0m=\u001b[32m''\u001b[0m,\n",
              "    \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\n",
              "        \u001b[32m'function_call'\u001b[0m: \u001b[1m{\u001b[0m\n",
              "            \u001b[32m'name'\u001b[0m: \u001b[32m'web_search'\u001b[0m,\n",
              "            \u001b[32m'arguments'\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"query\": \"\\\\uc0bc\\\\uc131\\\\uc804\\\\uc790 \\\\ucd5c\\\\uadfc \\\\uc2e4\\\\uc801\", \"time_range\": \u001b[0m\n",
              "\u001b[32m\"month\", \"topic\": \"finance\"\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m\n",
              "        \u001b[1m}\u001b[0m\n",
              "    \u001b[1m}\u001b[0m,\n",
              "    \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
              "        \u001b[32m'prompt_feedback'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'block_reason'\u001b[0m: \u001b[1;36m0\u001b[0m, \u001b[32m'safety_ratings'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m,\n",
              "        \u001b[32m'finish_reason'\u001b[0m: \u001b[32m'STOP'\u001b[0m,\n",
              "        \u001b[32m'model_name'\u001b[0m: \u001b[32m'gemini-2.0-flash'\u001b[0m,\n",
              "        \u001b[32m'safety_ratings'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
              "    \u001b[1m}\u001b[0m,\n",
              "    \u001b[33mid\u001b[0m=\u001b[32m'run-fda12434-3df4-4fbb-a89e-0f79b73d8192-0'\u001b[0m,\n",
              "    \u001b[33mtool_calls\u001b[0m=\u001b[1m[\u001b[0m\n",
              "        \u001b[1m{\u001b[0m\n",
              "            \u001b[32m'name'\u001b[0m: \u001b[32m'web_search'\u001b[0m,\n",
              "            \u001b[32m'args'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'query'\u001b[0m: \u001b[32m'삼성전자 최근 실적'\u001b[0m, \u001b[32m'time_range'\u001b[0m: \u001b[32m'month'\u001b[0m, \u001b[32m'topic'\u001b[0m: \u001b[32m'finance'\u001b[0m\u001b[1m}\u001b[0m,\n",
              "            \u001b[32m'id'\u001b[0m: \u001b[32m'a9702dc9-0a1d-4890-861f-9f7240615011'\u001b[0m,\n",
              "            \u001b[32m'type'\u001b[0m: \u001b[32m'tool_call'\u001b[0m\n",
              "        \u001b[1m}\u001b[0m\n",
              "    \u001b[1m]\u001b[0m,\n",
              "    \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
              "        \u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m124\u001b[0m,\n",
              "        \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m18\u001b[0m,\n",
              "        \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m142\u001b[0m,\n",
              "        \u001b[32m'input_token_details'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'cache_read'\u001b[0m: \u001b[1;36m0\u001b[0m\u001b[1m}\u001b[0m\n",
              "    \u001b[1m}\u001b[0m\n",
              "\u001b[1m)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Tavily Search\n",
        "\n",
        "from langchain_tavily import TavilySearch\n",
        "from typing_extensions import Optional\n",
        "@tool\n",
        "def web_search(\n",
        "    query: str,\n",
        "    time_range: Optional[str] = None,\n",
        "    topic: str = \"general\",\n",
        "    max_results: int = 5\n",
        ") -> str:\n",
        "    \"\"\"웹 검색을 수행하는 도구입니다.\n",
        "\n",
        "    Args:\n",
        "        query: 검색할 쿼리\n",
        "        time_range: 검색 시간 범위 (None, 'day', 'week', 'month', 'year')\n",
        "        topic: 검색 주제 ('general', 'finance', 'news')\n",
        "        max_results: 최대 검색 결과 수 (기본값 5)\n",
        "\n",
        "    Returns:\n",
        "        검색 결과 문자열\n",
        "    \"\"\"\n",
        "    tavily_search = TavilySearch(\n",
        "        max_results=max_results,\n",
        "        topic=topic,\n",
        "        include_raw_content=True,\n",
        "        time_range=time_range\n",
        "    )\n",
        "\n",
        "    result = tavily_search.invoke(query)\n",
        "    return result\n",
        "\n",
        "llm_with_tools = llm.bind_tools([web_search])\n",
        "result = llm_with_tools.invoke(\"삼성전자의 최근 실적에 대한 뉴스 찾아줄래?\")\n",
        "rprint(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sH2UfgJEhAY8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqqG4oIZhAY8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGrpE2BfhAY8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4DhvMj2hAY8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Amc_JO6ohAY8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yzvKabcMX_d"
      },
      "outputs": [],
      "source": [
        "# Tavily Search\n",
        "\n",
        "from langchain_tavily import TavilySearch\n",
        "\n",
        "# tavily api playground 참고\n",
        "tavily_search = TavilySearch(\n",
        "    max_results=5,\n",
        "    topic=\"general\",\n",
        "    # include_answer=False,\n",
        "    include_raw_content=True,\n",
        "    # include_images=False,\n",
        "    # include_image_descriptions=False,\n",
        "    # search_depth=\"basic\",\n",
        "    # time_range=\"day\",\n",
        "    # include_domains=None,\n",
        "    # exclude_domains=None\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjexB8lfMX_d"
      },
      "outputs": [],
      "source": [
        "result = tavily_search.invoke(\"Retrieval Augmented Generation Reasoning\")\n",
        "len(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kk-NR-CaTuHm"
      },
      "outputs": [],
      "source": [
        "for i in result['results']:\n",
        "    print(i['title'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5D0BD7mNMX_d"
      },
      "source": [
        "논문과 테크 리포트를 검색하는 학술 검색 API입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjgxneutMX_d"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import ArxivRetriever\n",
        "\n",
        "arxiv_search = ArxivRetriever(\n",
        "    load_max_docs=5,\n",
        "    load_all_available_meta=True,\n",
        "    get_full_documents=True,\n",
        "    doc_content_chars_max= 100000\n",
        "    # 10만 글자까지만 수집\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCssdkDQMX_d"
      },
      "outputs": [],
      "source": [
        "docs = arxiv_search.invoke(\"Retrieval Augmented Generation Reasoning\")\n",
        "# docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1f8IDF79MX_e"
      },
      "outputs": [],
      "source": [
        "for doc in docs:\n",
        "    print(f\"Published: {doc.metadata['Published']}\")\n",
        "    print(f\"Title: {doc.metadata['Title']}\")\n",
        "    print(f\"Authors: {doc.metadata['Authors']}\")\n",
        "    print(f\"Summary: {doc.metadata['Summary']}\")\n",
        "    print(f\"Length: {len(doc.page_content)}\")\n",
        "    print(\"-\" * 50)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uLKeJpAMX_e"
      },
      "source": [
        "각각의 검색 API를 아래와 같이 정리해 놓겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dD8PUklyMX_e"
      },
      "outputs": [],
      "source": [
        "tool_list = {\n",
        "    'tavily': tavily_search,\n",
        "    'arxiv': arxiv_search,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpBO5nVuMX_e"
      },
      "source": [
        "전체 과정은 다음과 같이 이루어집니다.\n",
        "\n",
        "1) 연구 토픽을 입력하면, LLM이 추가 정보를 질문합니다.    \n",
        "유저는 그대로 진행하거나, 피드백을 전달합니다.\n",
        "\n",
        "2) 연구 토픽에 대해, LLM이 간단한 검색을 수행하고 이를 바탕으로 연구 개요를 작성합니다.   \n",
        "\n",
        "3) 개요에 포함된 각 세션에 대해, LLM이 검색 쿼리를 생성하여 각 검색엔진을 통해 검색합니다.   \n",
        "\n",
        "4) 검색된 결과를 바탕으로 섹션별 내용을 작성합니다.    \n",
        "(검색 결과에 대한 레퍼런스 표시를 포함합니다.)\n",
        "\n",
        "5) 섹션별 드래프트를 개선하기 위해, 파생 질문을 추가로 생성하여 더 검색하거나 작성을 종료합니다.\n",
        "\n",
        "\n",
        "6) 섹션별 내용을 취합하고, 최종 수정을 거친 뒤 리포트를 완성합니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTUQMF4eMX_e"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
        "from langchain_core.output_parsers import PydanticOutputParser, StrOutputParser\n",
        "from langgraph.types import Command, interrupt\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from pydantic import BaseModel, Field\n",
        "from typing_extensions import Annotated, Literal\n",
        "import operator\n",
        "from langgraph.constants import Send"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vReime2pMX_e"
      },
      "source": [
        "## 작업에 사용할 클래스 만들기   \n",
        "Structured Output을 위한 클래스를 먼저 구성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMPncurNMX_e"
      },
      "outputs": [],
      "source": [
        "class Section(BaseModel):\n",
        "    name: str = Field(description=\"섹션의 이름\")\n",
        "    description: str = Field(description=\"해당 섹션에서 다룰 주요 주제에 대한 간략한 개요\")\n",
        "    content: str = Field(description=\"섹션의 내용 (처음에는 비워 둡니다)\")\n",
        "\n",
        "    @property\n",
        "    def as_str(self) -> str:\n",
        "        \"\"\"섹션의 정보를 포맷팅된 문자열로 변환합니다.\"\"\"\n",
        "        return f\"### {self.name}\\n{self.description}\\n\\n내용:\\n{self.content}\"\n",
        "\n",
        "class ReportPlan(BaseModel):\n",
        "    sections: list[Section] = Field(description=\"A list of sections for the report.\")\n",
        "    followup_question: str = Field(description=\"사용자에게 추가로 질문할 내용 (없으면 '')\")\n",
        "\n",
        "    @property\n",
        "    def as_str(self) -> str:\n",
        "        \"\"\"섹션들을 포맷팅된 문자열로 변환합니다.\"\"\"\n",
        "        sections_str = []\n",
        "        for section in self.sections:\n",
        "            sections_str.append(f\"### {section.name}\\n{section.description}\")\n",
        "        return \"\\n\\n\".join(sections_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjRNXqI_MX_e"
      },
      "source": [
        "## 서브모듈: 토픽에 대한 리서치 모듈 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZIu_fZdMX_e"
      },
      "source": [
        "섹션의 개요가 주어지면, 해당 내용을 검색하여 섹션을 작성하는 과정을 구현해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVkmYSCGMX_e"
      },
      "outputs": [],
      "source": [
        "class ResearchState(TypedDict):\n",
        "    topic: str\n",
        "    section: Section\n",
        "    queries: list[str]\n",
        "    draft: str\n",
        "    resources: list\n",
        "    num_revision: int\n",
        "    finished: bool\n",
        "\n",
        "\n",
        "def generate_search_query(state: ResearchState, config: RunnableConfig):\n",
        "    prompt = ChatPromptTemplate([\n",
        "('system', f'''\n",
        "주어진 섹션 정보에 대한 사전 조사를 위해, 효과적인 검색 쿼리를 생성해야 합니다.\n",
        "해당 주제를 포괄적으로 다룰 수 있는 검색 쿼리들을 생성하세요.\n",
        "\n",
        "검색 쿼리는 다음과 같은 원칙을 따라야 합니다:\n",
        "1. 핵심 키워드를 포함해야 합니다\n",
        "2. 너무 일반적이지 않아야 합니다\n",
        "3. 학술적이고 전문적인 용어를 사용해야 합니다\n",
        "4. 최신 연구 동향을 반영해야 합니다\n",
        "5. 따옴표를 포함하지 않아야 합니다.\n",
        "\n",
        "적절한 검색 쿼리를 한 줄에 하나씩 작성하세요.\n",
        "쿼리만 출력하고, {config['configurable']['num_search_queries']} 개의 쿼리를 출력하세요.\n",
        "\n",
        "'''),\n",
        "('user', '''\n",
        "섹션 정보:\n",
        "{section}\n",
        "\n",
        "''')])\n",
        "    section = state['section']\n",
        "\n",
        "    writer_llm = init_chat_model(\n",
        "        model_provider = config['configurable']['writer_provider'],\n",
        "        model= config['configurable']['writer_model'],\n",
        "        rate_limiter=rate_limiter,\n",
        "        temperature=0.8,\n",
        "        max_tokens = 8192\n",
        "    )\n",
        "\n",
        "\n",
        "    chain = prompt | writer_llm | StrOutputParser() | (lambda x: x.split('\\n'))\n",
        "\n",
        "    queries = chain.invoke(section.as_str)\n",
        "\n",
        "    return {'queries':queries}\n",
        "\n",
        "def search_and_filter(state: ResearchState, config: RunnableConfig):\n",
        "    '''각각의 검색어에 대해 검색 결과를 수행하고 필터링합니다.\n",
        "    Tavily Search는 정확성이 높기 때문에, 해당 부분을 생략해도 됩니다..\n",
        "    빠른 LLM을 사용하겠습니다.\n",
        "    '''\n",
        "    quick_llm = init_chat_model(\n",
        "        model_provider = config['configurable']['quick_provider'],\n",
        "        model= config['configurable']['quick_model'],\n",
        "        rate_limiter=quick_rate_limiter,\n",
        "        temperature=0.8,\n",
        "    )\n",
        "\n",
        "    search_tool = tool_list[config['configurable']['search_api'][0]]\n",
        "    # 적절한 검색 툴 선택 (여기서는 Tavily로 고정)\n",
        "\n",
        "    queries = state['queries']\n",
        "    section = state['section']\n",
        "\n",
        "    relevant_docs=[]\n",
        "\n",
        "\n",
        "    filter_prompt=ChatPromptTemplate([\n",
        "        ('system', f'''다음 검색 결과가 주어진 주제와 관련이 있는지 O/X로 판단하세요.\n",
        "O/X만 출력하세요.\n",
        "---\n",
        "주제: {section.as_str}'''),\n",
        "\n",
        "('user', '''\n",
        "검색 결과: {doc}''')])\n",
        "    chain = filter_prompt | quick_llm | StrOutputParser()\n",
        "\n",
        "    context = search_tool.batch(queries)\n",
        "    # 모든 검색 쿼리를 한번에 실행\n",
        "\n",
        "    def preprocess(text):\n",
        "        import re\n",
        "        # 탭과 개행문자를 공백으로 변환\n",
        "        text = text.replace('\\t', ' ').replace('\\n', ' ').replace('\\xa0', ' ')\n",
        "        # 템플릿 오류 방지\n",
        "        text = text.replace('{', '(').replace('}', ')')\n",
        "\n",
        "        # 연속된 공백을 하나로 치환\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text\n",
        "\n",
        "\n",
        "    for docs in context:\n",
        "        try:\n",
        "            for doc in docs['results']:\n",
        "                doc_str = f\"### {doc['title']} \\n URL: {doc['url']} \\n {doc['raw_content']}\" if doc.get('raw_content') else f\"### {doc['title']} \\n {doc['content']}\"\n",
        "                # 하나로 만든 뒤 전처리\n",
        "                doc_str = preprocess(doc_str)\n",
        "                relevance = chain.invoke(doc_str)\n",
        "                if relevance=='O':\n",
        "                    relevant_docs.append(doc_str)\n",
        "        except: # 검색 오류시\n",
        "            continue\n",
        "    print(f'# Filtered Docs: {len(relevant_docs)}')\n",
        "    return {'resources':relevant_docs}\n",
        "\n",
        "def write_section(state: ResearchState, config: RunnableConfig):\n",
        "    section = state['section'].as_str\n",
        "    topic = state['topic']\n",
        "    resources = state['resources']\n",
        "    draft = state.get('draft', '')\n",
        "\n",
        "    writer_prompt =ChatPromptTemplate([\n",
        "        ('system', '''\n",
        "연구 리포트의 주제와 세부 섹션명이 주어집니다.\n",
        "아래의 정보를 활용하여, 연구 리포트의 한 섹션을 작성하세요.\n",
        "다음은 작성 가이드라인입니다.\n",
        "\n",
        "[작성 가이드라인]\n",
        "간단하고 명확한 언어를 사용하세요.\n",
        "섹션명은 마크다운 ## 으로 작성하며, 세부 목차는 만들지 말고 문단으로만 분리하세요.\n",
        "문장을 너무 길게 쓰지 말고, 이해하기 쉽게 작성하세요.\n",
        "'이다.' 가 아닌 '입니다.', '합니다.' 등의 스타일로 작성하세요.\n",
        "또한, 아래에 주어지는 정보의 내용을 최대한 활용하여 작성하세요.\n",
        "\n",
        "\n",
        "[인용 가이드라인]\n",
        "인용 표시는 [1], [2]와 같이 작성하고, 섹션 마지막에 레퍼런스를 작성하세요.\n",
        "레퍼런스 형식은 MLA 표기를 따르고, 마지막에 URL도 표시하세요.\n",
        "예시 표시 형식은 다음과 같습니다.\n",
        "\n",
        "**References**\n",
        "[1] Unite.AI. \"DeepMind의 Michelangelo 벤치마크: Long-Context LLM의 한계를 드러내다.\" *Unite.AI*, [https://www.unite.ai/ko/\n",
        "<br><br>\n",
        "[2] ...\n",
        "\n",
        "\n",
        "[노트]\n",
        "인용 표시를 정확하게 했는지 확인하고, 주장이 기술되는 경우 가급적 소스에 근거하도록 작성하세요.\n",
        "마크다운 형식을 고려하여, 문단 분리나 레퍼런스 사이의 줄바꿈을 명확하게 하세요.\n",
        "\n",
        "[기존 드래프트]\n",
        "\n",
        "기존 드래프트가 주어지는 경우, 여기에 이어서 작성하세요.\n",
        "'''),\n",
        "\n",
        "('user',f'''\n",
        "주제: {topic}\n",
        "\n",
        "세부 섹션명: {section}\n",
        "\n",
        "검색 결과 Context:\n",
        "{resources}\n",
        "\n",
        "기존 Draft:\n",
        "{draft}\n",
        "''')\n",
        "])\n",
        "    writer_llm = init_chat_model(\n",
        "        model_provider = config['configurable']['writer_provider'],\n",
        "        model= config['configurable']['writer_model'],\n",
        "        rate_limiter=rate_limiter,\n",
        "        temperature=0.8,\n",
        "        max_tokens = 8192\n",
        "    )\n",
        "    chain = writer_prompt | writer_llm | StrOutputParser()\n",
        "    draft = chain.invoke({})\n",
        "    return {'draft':draft}\n",
        "\n",
        "class Feedback(BaseModel):\n",
        "    grade : Literal['Good', 'Bad'] = Field(description='드래프트에 대한 평가')\n",
        "    queries: Optional[list[str]] = Field(description='검색 쿼리 목록')\n",
        "\n",
        "def refine_research(state: ResearchState, config: RunnableConfig) -> Command[Literal[END, \"search_and_filter\"]] :\n",
        "    '''Draft와 context를 평가하여, 추가 검색이 필요한지 판단합니다.\n",
        "    Revision 개수를 초과하면 바로 END로 이동합니다.'''\n",
        "\n",
        "    section = state['section'].as_str\n",
        "    topic = state['topic']\n",
        "    resources = state['resources']\n",
        "    draft = state['draft']\n",
        "    queries = state['queries']\n",
        "    num_revision = state['num_revision']\n",
        "\n",
        "    prompt = ChatPromptTemplate([\n",
        "        ('system', f'''\n",
        "연구 리포트의 주제와 세부 섹션명이 주어집니다.\n",
        "\n",
        "현재 작성된 섹션의 드래프트를 평가하세요.\n",
        "\n",
        "이 글의 내용을 명확하고 유익하게 작성하기 위해,\n",
        "검색된 결과 이외의 새로운 내용을 더 조사해야 하는지 판단하세요.\n",
        "\n",
        "이후, 추가 검색을 위해 필요한 검색어 쿼리를 작성하세요.\n",
        "{config['configurable']['num_search_queries']} 개 이하의 쿼리를 출력하세요.\n",
        "\n",
        "해당 글은 충분히 완성도가 높아 추가 조사가 필요하지 않을 수도 있습니다.\n",
        "그런 경우에는 'Good'을 출력하고, 추가 쿼리를 비워두세요.\n",
        "'''),\n",
        "\n",
        "('user',f'''\n",
        "주제: {topic}\n",
        "\n",
        "세부 섹션명: {section}\n",
        "\n",
        "\n",
        "드래프트: {draft}''')\n",
        "])\n",
        "    # revision 개수 넘어가면 바로 END\n",
        "    if num_revision >= config['configurable']['max_search_depth']:\n",
        "         return Command(goto = END,\n",
        "                       update = {'finished':True})\n",
        "\n",
        "\n",
        "    evaluator_llm = init_chat_model(\n",
        "        model_provider = config['configurable']['evaluator_provider'],\n",
        "        model= config['configurable']['evaluator_model'],\n",
        "        rate_limiter=rate_limiter,\n",
        "        temperature=0.8,\n",
        "    )\n",
        "\n",
        "\n",
        "    chain = prompt | evaluator_llm.with_structured_output(Feedback)\n",
        "\n",
        "    feedback = chain.invoke({})\n",
        "\n",
        "    if feedback.grade =='Good':\n",
        "        return Command(goto = END,\n",
        "                       update = {'finished':True})\n",
        "    else:\n",
        "        return Command(goto = search_and_filter,\n",
        "                       update= {'queries': feedback.queries,\n",
        "                                'num_revision': num_revision+1})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90aa_jYTMX_e"
      },
      "source": [
        "Research Agent를 구성하는 Small Graph를 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwwghaKjMX_e"
      },
      "outputs": [],
      "source": [
        "builder = StateGraph(ResearchState)\n",
        "builder.add_node(generate_search_query)\n",
        "builder.add_node(search_and_filter)\n",
        "builder.add_node(write_section)\n",
        "builder.add_node(refine_research)\n",
        "\n",
        "\n",
        "builder.add_edge(START, 'generate_search_query')\n",
        "builder.add_edge('generate_search_query', 'search_and_filter')\n",
        "builder.add_edge('search_and_filter', 'write_section')\n",
        "builder.add_edge('write_section', 'refine_research')\n",
        "\n",
        "memory = MemorySaver()\n",
        "\n",
        "\n",
        "\n",
        "researcher_graph = builder.compile(checkpointer=memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPIqd5QpM64d"
      },
      "outputs": [],
      "source": [
        "researcher_graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JYkRyKIMX_f"
      },
      "outputs": [],
      "source": [
        "t = Section(name='Case Study: LLama 4의 Long Context',\n",
        "            description='Llama 4 모델의 10M Context의 비결에 대해 설명합니다.',\n",
        "            content='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3chM8e7yMX_f"
      },
      "outputs": [],
      "source": [
        "test_research_state={\n",
        "    'topic': '1M Context Windows 모델',\n",
        "    'section': t,\n",
        "    'num_revision':0\n",
        "}\n",
        "\n",
        "# Thread\n",
        "thread = {'configurable':default, 'thread_id':'1'}\n",
        "\n",
        "history = []\n",
        "for event in researcher_graph.stream(test_research_state, thread, stream_mode=\"updates\"):\n",
        "    for status in event:\n",
        "        print(f'# {status}')\n",
        "        for key in event[status]:\n",
        "            value = str(event[status][key])\n",
        "            if len(value)>300:\n",
        "                print(f'- {key}: {value[:300]}')\n",
        "            else:\n",
        "                print(f'- {key}: {value}')\n",
        "        print('---------')\n",
        "    history.append(event)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoMPg3nuNyIG"
      },
      "outputs": [],
      "source": [
        "load_dotenv('.env', override=True)\n",
        "# override: 이미 설정된 환경 변수 덮어씌우기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYBeIALAMX_f"
      },
      "source": [
        "작성한 섹션 드래프트를 확인해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJpi2zuyZz5v"
      },
      "outputs": [],
      "source": [
        "history[-2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LOKY-YGMX_f"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "import textwrap\n",
        "\n",
        "result = history[-2]['write_section']['draft']\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('•', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
        "\n",
        "to_markdown(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcQ9Di7AMX_f"
      },
      "source": [
        "섹션별 Writer를 구성했습니다.   \n",
        "이제 해당 그래프를 서브모듈로 하는 에이전트 구조를 구성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NfpjFLxMX_f"
      },
      "outputs": [],
      "source": [
        "class State(TypedDict):\n",
        "    topic: str # 리포트 주제\n",
        "    plan: ReportPlan # 리포트 개요\n",
        "    result: str # 최종 결과물 (하나로 합쳐진)\n",
        "    human_feedback:str # 개요에 대한 인간 피드백\n",
        "    finished_drafts: Annotated[list[str], operator.add]\n",
        "    # (아마도) 병렬 처리로 생성될 draft들을 순서대로 합침\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elLo7DFyMX_f"
      },
      "source": [
        "요청을 받은 뒤, 초기 설정과 함께 부가 질문을 수행합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SmdraBMMX_f"
      },
      "outputs": [],
      "source": [
        "def initiate_report(state: State , config: RunnableConfig):\n",
        "\n",
        "    prompt = ChatPromptTemplate([\n",
        "        ('system','''\n",
        "당신은 주어진 주제에 대한 연구 보고서의 초기 방향 설정을 위한 개요를 구성합니다.\n",
        "최대한 최신의 지식과 인사이트를 활용하여야 하며, 사용자를 위한 맞춤형 보고서가 되어야 합니다.\n",
        "\n",
        "주어진 주제에 대한 섹션별 개요를 작성하세요.\n",
        "단, 마지막 섹션인 결론은 제외하고 작성하세요.\n",
        "\n",
        "각 섹션은 불필요한 요소를 포함하지 말아야 하며, 명확하게 구분되어야 합니다.\n",
        "섹션 간의 겹치는 내용을 최대한 줄이고, 각각의 역할이 분명하도록 구성하세요.\n",
        "\n",
        "또한, 최종 결과 보고서에 사용자의 선호를 최대한 반영하기 위해,\n",
        "사용자에게 추가로 질문할 내용도 작성하세요.\n",
        "예를 들어, 세부 분야, 적용하고자 하는 환경, 원하는 정리 형식 등을 질문할 수 있습니다.\n",
        "\n",
        "다음은 예시입니다.\n",
        "\n",
        "---\n",
        "질문: LLM 파인 튜닝 방법인 LoRA의 최근 발전된 모델들에 대해 조사해줘\n",
        "\n",
        "\n",
        "답변:\n",
        "개요: (보고서의 개요)\n",
        "추가 질문: LoRA 기반 LLM 파인튜닝 관련 최근 발전된 모델들에 대해 조사해드릴게요.\n",
        "아래 항목들 중 가능한 정보를 알려주시면 더 정확한 조사를 도와드릴 수 있어요:\n",
        "용도 (예: 챗봇, 코드 생성, 번역, 의료 등)\n",
        "적용 환경 (예: 연구용, 기업 서비스용, 모바일 디바이스 등)\n",
        "원하시는 정리 형식 (예: 표, 요약 보고서, 논문 중심 정리 등)\n",
        "가능한 범위를 알려주시면 곧바로 조사 시작할게요!'''),\n",
        "\n",
        "('human', '''사용자의 주제(혹은 요청): {topic}\n",
        "\n",
        "---\n",
        "\n",
        "관련 최신 검색 결과:\n",
        "{context}\n",
        " ''')\n",
        "    ])\n",
        "\n",
        "    topic = state['topic']\n",
        "\n",
        "    search_tool =  tool_list[config['configurable']['search_api'][0]]\n",
        "    # 적절한 검색 툴 선택 (여기서는 Tavily로 고정)\n",
        "\n",
        "\n",
        "    context = search_tool.invoke(topic)\n",
        "    # 초기 검색을 수행하여 개요 작성\n",
        "\n",
        "    planner_llm = init_chat_model(\n",
        "        model_provider = config['configurable']['planner_provider'],\n",
        "        model= config['configurable']['planner_model'],\n",
        "        rate_limiter=rate_limiter,\n",
        "        temperature=0.8,\n",
        "    ).with_structured_output(ReportPlan)\n",
        "\n",
        "    chain = prompt | planner_llm\n",
        "\n",
        "    result = chain.invoke({'topic': topic, 'context' : context})\n",
        "\n",
        "    print('# Planner: ')\n",
        "\n",
        "    print(f'''\n",
        "# 보고서 작성 개요:\n",
        "{result.as_str}\n",
        "\n",
        "# 추가 질문:\n",
        "{result.followup_question}''')\n",
        "    return {'plan':result}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6zgR-0uMX_f"
      },
      "outputs": [],
      "source": [
        "def human_review(state: State , config: RunnableConfig):\n",
        "\n",
        "    human_review = interrupt(\n",
        "        {\n",
        "            \"question\": \"피드백을 전달해 주세요, 이대로 진행하고 싶으시면, continue 또는 go만 입력하세요.\",\n",
        "        }\n",
        "    )\n",
        "    # Human Feedback을 받아 전달\n",
        "    review_action = human_review.get(\"human_feedback\")\n",
        "    return {'human_feedback': review_action}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCdKmVAwMX_g"
      },
      "outputs": [],
      "source": [
        "def refine_outline(state: State , config: RunnableConfig):\n",
        "    current_plan = state['plan']\n",
        "    human_feedback = state['human_feedback']\n",
        "\n",
        "    if human_feedback.lower()=='go' or human_feedback.lower()=='continue':\n",
        "        return {'plan': current_plan}\n",
        "\n",
        "    refine_prompt = PromptTemplate(template='''\n",
        "보고서의 개요가 주어집니다.\n",
        "추가 요청사항을 반영하여, 수정된 개요를 작성하세요:\n",
        "\n",
        "기존 개요:\n",
        "{current_plan}\n",
        "\n",
        "---\n",
        "\n",
        "피드백:\n",
        "{feedback}\n",
        "\n",
        "    ''')\n",
        "\n",
        "    planner_llm = init_chat_model(\n",
        "        model_provider = config['configurable']['planner_provider'],\n",
        "        model= config['configurable']['planner_model'],\n",
        "        rate_limiter=rate_limiter,\n",
        "        temperature=0.8,\n",
        "    ).with_structured_output(ReportPlan)\n",
        "\n",
        "    # 수정된 계획 생성\n",
        "    refine_chain = refine_prompt | planner_llm\n",
        "\n",
        "    refined_plan = refine_chain.invoke({\n",
        "        'current_plan': current_plan.as_str,\n",
        "        'feedback': human_feedback\n",
        "    })\n",
        "\n",
        "    print('# 수정된 보고서 계획:')\n",
        "    print(f'''\n",
        "    {refined_plan.as_str}\n",
        "\n",
        "    추가 질문:\n",
        "    {refined_plan.followup_question}\n",
        "    ''')\n",
        "\n",
        "    return {'plan': refined_plan}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOQMfgOGf7n0"
      },
      "outputs": [],
      "source": [
        "builder = StateGraph(State)\n",
        "builder.add_node(initiate_report)\n",
        "builder.add_node(human_review)\n",
        "builder.add_node(refine_outline)\n",
        "\n",
        "builder.add_edge(START, 'initiate_report')\n",
        "builder.add_edge('initiate_report','human_review')\n",
        "builder.add_edge('human_review', 'refine_outline')\n",
        "builder.add_edge('refine_outline', END)\n",
        "graph = builder.compile(checkpointer= MemorySaver())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znEhBuMxgNBs"
      },
      "outputs": [],
      "source": [
        "graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqXkRcrQf5O8"
      },
      "outputs": [],
      "source": [
        "test_state = {'topic':'Long-Context LLM의 시대'}\n",
        "thread = {'configurable':default, 'thread_id':'0'}\n",
        "\n",
        "graph.invoke(test_state, config = thread)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ir5LYaqbgieP"
      },
      "outputs": [],
      "source": [
        "for event in graph.stream(\n",
        "        Command(resume={\"human_feedback\": \"\"\"오 나 맘바가 궁금해. Mamba에 대한 얘기만 하는 방향으로 수정해줘.\"\"\"}),\n",
        "    thread,\n",
        "    stream_mode=\"updates\", subgraphs=True\n",
        "):\n",
        "    history.append(event)\n",
        "    for status in event:\n",
        "        print(f'# {str(status)[:300]}')\n",
        "\n",
        "        try:\n",
        "            for key in event[status]:\n",
        "                value = str(event[status][key])\n",
        "                if len(value)>300:\n",
        "                    print(f'- {key}: {value[:300]}')\n",
        "                else:\n",
        "                    print(f'- {key}: {value}')\n",
        "            print('---------')\n",
        "        except:\n",
        "            continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8PQ4c0ZMX_g"
      },
      "outputs": [],
      "source": [
        "def research(state:ResearchState, config: RunnableConfig):\n",
        "    result = researcher_graph.invoke({\n",
        "    'topic': state['topic'],\n",
        "    'section': state['section'],\n",
        "    'num_revision':0\n",
        "    })\n",
        "    draft = result['draft']\n",
        "\n",
        "    return {'finished_drafts':[draft]}\n",
        "\n",
        "\n",
        "def start_survey(state:State, config: RunnableConfig):\n",
        "    topic = state['topic']\n",
        "    plan = state['plan']\n",
        "    # Query 생성, 수집, Reflection, 섹션 작성 모듈을 하나의 에이전트로 구성\n",
        "\n",
        "    return [Send(\"research\",\n",
        "            {'topic':topic, \"section\": s}) for s in plan.sections]\n",
        "\n",
        "\n",
        "def synthesizer(state:State, config: RunnableConfig):\n",
        "    return {'result':'\\n'.join(state['finished_drafts'])}\n",
        "\n",
        "def finalizer(state:State, config: RunnableConfig):\n",
        "    prompt = PromptTemplate(template='''\n",
        "연구 보고서의 내용이 주어집니다.\n",
        "전체 흐름을 고려하여, 최종 결론 섹션을 작성하세요.\n",
        "\n",
        "전체 보고서 내용:\n",
        "{result}''')\n",
        "    writer_llm = init_chat_model(\n",
        "        model_provider = config['configurable']['writer_provider'],\n",
        "        model= config['configurable']['writer_model'],\n",
        "        rate_limiter=rate_limiter,\n",
        "        temperature=0.8,\n",
        "    )\n",
        "    chain = prompt | writer_llm | StrOutputParser()\n",
        "    result = chain.invoke({'result':state['result']})\n",
        "    return {'result':state['result'] + '\\n'+result}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfpKDNmoMX_g"
      },
      "outputs": [],
      "source": [
        "builder = StateGraph(State)\n",
        "builder.add_node(initiate_report)\n",
        "builder.add_node(human_review)\n",
        "builder.add_node(refine_outline)\n",
        "builder.add_node(research)\n",
        "builder.add_node(synthesizer)\n",
        "builder.add_node(finalizer)\n",
        "\n",
        "builder.add_edge(START, 'initiate_report')\n",
        "builder.add_edge('initiate_report','human_review')\n",
        "builder.add_edge('human_review', 'refine_outline')\n",
        "\n",
        "builder.add_conditional_edges(\"refine_outline\", start_survey, [\"research\"])\n",
        "\n",
        "builder.add_edge('research', 'synthesizer')\n",
        "builder.add_edge('synthesizer', 'finalizer')\n",
        "builder.add_edge('finalizer',END)\n",
        "memory = MemorySaver()\n",
        "\n",
        "graph = builder.compile(checkpointer=memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9uy6nR0MX_g"
      },
      "outputs": [],
      "source": [
        "graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7UOM1RUMX_g"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "display(Image(graph.get_graph(xray=True).draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siRf-tSfMX_g"
      },
      "source": [
        "완성된 그래프를 실행해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBpwoEzJMX_h"
      },
      "outputs": [],
      "source": [
        "test_state = {'topic':'LLM Agent 개발을 위한 프롬프트 엔지니어링의 중요성'}\n",
        "thread = {'configurable':default, 'thread_id':'1'}\n",
        "\n",
        "graph.invoke(test_state, config = thread)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kd7ZcOWrMX_h"
      },
      "outputs": [],
      "source": [
        "# BottleNeck: Search and Filter\n",
        "# 긴 컨텍스트 모델은 필터링을 안 하는 방법도 고려할 수 있겠습니다..\n",
        "'''좋아, Gemini 2.5 Pro는\n",
        "긴 컨텍스트에서도 성능이 엄청 좋던데, 그 부분을 중요하게 다뤄 주고.\n",
        "Llama 4의 Long Context에 대해서도 알려줘.        '''\n",
        "\n",
        "history =[]\n",
        "\n",
        "for event in graph.stream(\n",
        "        Command(resume={\"human_feedback\": \"\"\"좋아,\n",
        "지금 흐름 좋은데, 특정 프레임워크에 중점을 두기보다는\n",
        "전반적인 설계에서 프롬프트를 어떻게 써야 하는지에 대한\n",
        "에이전트에 특화된 노하우가 들어가면 좋겠어.\"\"\"}),\n",
        "    thread,\n",
        "    stream_mode=\"updates\", subgraphs=True\n",
        "):\n",
        "    history.append(event)\n",
        "    for status in event:\n",
        "        print(f'# {str(status)[:300]}')\n",
        "\n",
        "        try:\n",
        "            for key in event[status]:\n",
        "                value = str(event[status][key])\n",
        "                if len(value)>300:\n",
        "                    print(f'- {key}: {value[:300]}')\n",
        "                else:\n",
        "                    print(f'- {key}: {value}')\n",
        "            print('---------')\n",
        "        except:\n",
        "            continue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNKe1e1ZMX_h"
      },
      "source": [
        "결과물을 md 파일에 저장해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4XsSuSwMX_h"
      },
      "outputs": [],
      "source": [
        "result = history[-1][1]['finalizer']['result']\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0nbe60pMX_h"
      },
      "outputs": [],
      "source": [
        "with open(\"example2.md\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(result)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "multicampus",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2oIrHN6ipAN"
      },
      "source": [
        "# [실습] 조건부 엣지와 툴 노드 연결하기  \n",
        "\n",
        "이번 실습에서는 모델의 분기점을 구성하는 조건부 엣지와 외부 툴을 효과적으로 사용하는 툴 노드를 추가해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JY5vy8kaipAO"
      },
      "outputs": [],
      "source": [
        "!pip install langgraph langchain langchain_google_genai langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvVRfIPyipAP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['GOOGLE_API_KEY'] = 'AIxxx'\n",
        "\n",
        "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Gemini API는 분당 10개 요청으로 제한\n",
        "# 즉, 초당 약 0.167개 요청 (10/60)\n",
        "rate_limiter = InMemoryRateLimiter(\n",
        "    requests_per_second=0.167,  # 분당 10개 요청\n",
        "    check_every_n_seconds=0.1,  # 100ms마다 체크\n",
        "    max_bucket_size=10,  # 최대 버스트 크기\n",
        ")\n",
        "\n",
        "# rate limiter를 LLM에 적용\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    rate_limiter=rate_limiter,\n",
        "    # temperature\n",
        "    # max_tokens\n",
        "\n",
        "    thinking_budget = 500  # 추론(Reasoning) 토큰 길이 제한\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajVtwGsQipAP"
      },
      "source": [
        "## 1. 조건부 엣지"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srKDBtyDipAP"
      },
      "source": [
        "조건부 엣지는 워크플로우의 분기점에 해당합니다.   \n",
        "조건을 구성하여 종료 조건을 만들거나, 조건에 따라 다른 노드로 진입하도록 구성할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtmiLd2DipAP"
      },
      "source": [
        "이번 실습에서는 멀티 턴의 대화를 구현해 보겠습니다.   \n",
        "질문이 주어지면, 여러 번의 대화를 통해 맥락이 이어집니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dda0ACqkipAP"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "    context : Annotated[list, add_messages]   # 메시지 맥락을 저장하는 리스트\n",
        "    count : int # 사용자가 입력한 횟수를 저장\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSYkG4CHipAQ"
      },
      "outputs": [],
      "source": [
        "def ask_human(state):\n",
        "    query = input()\n",
        "    print('User :', query)\n",
        "\n",
        "    return {'context':HumanMessage(content=query)}\n",
        "\n",
        "def talk(state):\n",
        "    messages = state['context']\n",
        "\n",
        "    answer = llm.invoke(messages)\n",
        "\n",
        "    print('AI :', answer.content)\n",
        "    state['count'] +=1\n",
        "\n",
        "    return {'context': answer, 'count': state['count']}\n",
        "\n",
        "\n",
        "def check_end(state):\n",
        "\n",
        "    return \"Done\" if state['count'] >= 3 else \"Resume\"\n",
        "    # count가 3 이상이 되면 True, 이외 False\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMhuFoAlipAQ"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph import MessagesState\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langgraph.prebuilt import tools_condition\n",
        "\n",
        "\n",
        "builder = StateGraph(State)\n",
        "\n",
        "builder.add_node('talk', talk)\n",
        "builder.add_node('ask_human', ask_human)\n",
        "\n",
        "builder.add_edge(START, 'ask_human'),\n",
        "builder.add_edge('ask_human', 'talk'),\n",
        "builder.add_conditional_edges('talk', check_end,\n",
        "                              {'Done': END, 'Resume': 'ask_human'})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRDENl-OipAQ"
      },
      "outputs": [],
      "source": [
        "graph = builder.compile()\n",
        "graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ts3d6foipAQ"
      },
      "outputs": [],
      "source": [
        "system_prompt = '''한 문장 길이로만 대화하세요.'''\n",
        "\n",
        "messages = [SystemMessage(content=system_prompt)]\n",
        "\n",
        "\n",
        "response = graph.invoke({'context':messages, 'count':0})\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrPi99hApWac"
      },
      "source": [
        "## [연습문제] 종료 조건 수정하기\n",
        "\n",
        "이번에는, 사용자가 'FINISHED'를 입력하면 대화가 끝나도록 만들어 보세요.   \n",
        "check_end 함수의 내용과 그래프 구조를 수정하면 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Os5obDWpd2z"
      },
      "outputs": [],
      "source": [
        "def check_end(state):\n",
        "    # 마지막 메시지를 찾고, 그 내용이 'FINISHED'면 끝\n",
        "    return \"Done\" if state['context'][-1].content == 'FINISHED' else \"Resume\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtBP8OeMqIYW"
      },
      "outputs": [],
      "source": [
        "builder = StateGraph(State)\n",
        "\n",
        "builder.add_node('talk', talk)\n",
        "builder.add_node('ask_human', ask_human)\n",
        "\n",
        "builder.add_edge(START, 'ask_human'),\n",
        "# builder.add_edge('ask_human', 'talk'),\n",
        "\n",
        "builder.add_conditional_edges('ask_human', check_end,\n",
        "                              {'Done': END, 'Resume': 'talk'})\n",
        "\n",
        "builder.add_edge('talk', 'ask_human')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "graph = builder.compile()\n",
        "graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZf7DRnGqv3f"
      },
      "outputs": [],
      "source": [
        "system_prompt = '''한 문장 길이로만 대화하세요.'''\n",
        "messages = [SystemMessage(content=system_prompt)]\n",
        "\n",
        "graph.invoke({'context' :messages, 'count':0})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OeSywQ0ipAQ"
      },
      "source": [
        "이번에는 초기 질문만 유저가 전달하면, LLM이 스스로 역할을 전환하며 답변하도록 만들어 보겠습니다.   \n",
        "질문이 주어지면, 해당 질문으로 LLM이 User와 AI 역할을 반복하며 대화를 진행합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1BD5oysipAQ"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "    context : list  # List Reducer를 쓰지 않는 이유? Context가 계속 바뀌기 때문\n",
        "    count : int # 사용자가 입력한 횟수를 저장\n",
        "    turn: str # 차례를 저장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQB6rAz4ipAQ"
      },
      "outputs": [],
      "source": [
        "# 에이전트의 자동 발생 대화: 기존의 대화를 반대로 전환\n",
        "def simulate(state):\n",
        "    messages = state['context']\n",
        "    switched_messages = []\n",
        "\n",
        "    answer = llm.invoke(messages)\n",
        "    messages.append(answer)\n",
        "\n",
        "\n",
        "    for message in messages:\n",
        "        if isinstance(message,HumanMessage):\n",
        "            switched_messages.append(AIMessage(content=message.content))\n",
        "        elif isinstance(message,AIMessage):\n",
        "            switched_messages.append(HumanMessage(content=message.content))\n",
        "        else:\n",
        "            switched_messages.append(message)\n",
        "\n",
        "    print(state['turn'], ':', messages[-1].content)\n",
        "\n",
        "    if state['turn'] == 'AI':\n",
        "        state['turn'] = 'User'\n",
        "    else:\n",
        "        state['turn'] = 'AI'\n",
        "\n",
        "    state['count'] +=1\n",
        "\n",
        "    return {'context':switched_messages, 'count':state['count'], 'turn' : state['turn']}\n",
        "\n",
        "def check_end(state):\n",
        "    return state['count'] >= 3\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3htAzmQipAQ"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph import MessagesState\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langgraph.prebuilt import tools_condition\n",
        "\n",
        "\n",
        "builder = StateGraph(State)\n",
        "\n",
        "builder.add_node('simulate', simulate)\n",
        "\n",
        "builder.add_edge(START, 'simulate')\n",
        "builder.add_conditional_edges('simulate', check_end,\n",
        "                              {True: END, False: 'simulate'})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80tEowL4ipAQ"
      },
      "outputs": [],
      "source": [
        "graph = builder.compile()\n",
        "graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKcRwcOWipAR"
      },
      "outputs": [],
      "source": [
        "system_prompt = '''대화는 무례하게 하세요. 욕설은 하지 마세요.'''\n",
        "initial_question = input()\n",
        "\n",
        "print('User :', initial_question)\n",
        "\n",
        "messages = [SystemMessage(content=system_prompt), HumanMessage(content=initial_question)]\n",
        "\n",
        "\n",
        "response = graph.invoke({'context':messages, 'count':0, 'turn':'AI'})\n",
        "response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jc7YoABbipAR"
      },
      "outputs": [],
      "source": [
        "messages = [SystemMessage(content=system_prompt), HumanMessage(content=initial_question)]\n",
        "\n",
        "for chunk in graph.stream(\n",
        "    {'context':messages, 'count':0, 'turn':'AI'}, stream_mode='values'):\n",
        "    print(chunk)\n",
        "    print('---')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9-Sfl8YipAR"
      },
      "source": [
        "간단한 종료조건을 만들었지만, 실제 코드에서는 'FINISHED' 등을 출력하게 하면 작업을 종료하는 프롬프트를 구성할 수도 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YKFsFh6ipAR"
      },
      "source": [
        "## 2. Tool Node\n",
        "\n",
        "툴 노드는 랭체인에서 지원하는 Prebuilt 계열의 코드로, 해당 노드로 툴 요청이 전달되면 그 결과를 실행합니다.   \n",
        "노드에 전달되는 마지막 message의 내용에 Tool Call이 포함되면, 이를 받은 툴 노드는 Tool Message를 추가합니다.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAaZF9rLipAR"
      },
      "outputs": [],
      "source": [
        "# Tavily API\n",
        "os.environ['TAVILY_API_KEY'] = 'tvly-xxxx'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jP46dDNipAR"
      },
      "source": [
        "툴을 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sq0lncxipAR"
      },
      "outputs": [],
      "source": [
        "from langchain_tavily import TavilySearch\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "\n",
        "tavily_search = TavilySearch(\n",
        "    max_results=3)\n",
        "\n",
        "@tool\n",
        "def multiply(x:int, y:int) -> int:\n",
        "    \"x와 y를 입력받아, x와 y를 곱한 결과를 반환합니다.\"\n",
        "    return x*y\n",
        "\n",
        "@tool\n",
        "def current_date() -> str:\n",
        "    \"현재 날짜를 %y-%m-%d 형식으로 반환합니다.\"\n",
        "    from datetime import datetime\n",
        "    return datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "print(multiply.invoke({'x':3, 'y':4}))\n",
        "print(current_date.invoke({}))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8FPQ-KnipAR"
      },
      "outputs": [],
      "source": [
        "llm_with_tools = llm.bind_tools([multiply, current_date, tavily_search])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8vBCmQhipAR"
      },
      "outputs": [],
      "source": [
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJ4HLYhCipAR"
      },
      "source": [
        "툴 요청 메시지를 출력할 노드를 구성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHZ6CkSLipAR"
      },
      "outputs": [],
      "source": [
        "def tool_calling_llm(state):\n",
        "    return {\"messages\": llm_with_tools.invoke(state[\"messages\"])}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_w0vn_sipAR"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolNode\n",
        "from langgraph.prebuilt import tools_condition\n",
        "\n",
        "\n",
        "builder = StateGraph(State)\n",
        "\n",
        "builder.add_node(\"tool_calling_llm\", tool_calling_llm)\n",
        "builder.add_node('tools', ToolNode([multiply, current_date, tavily_search]))\n",
        "# ToolNode: 입력이 전달되면 툴을 실행해서 돌려줌\n",
        "\n",
        "builder.add_edge(START, 'tool_calling_llm')\n",
        "builder.add_conditional_edges('tool_calling_llm', tools_condition, END)\n",
        "# tools_condition: 툴이 필요하면 툴 노드로 이동, 아니면 END\n",
        "builder.add_edge('tools', 'tool_calling_llm')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4EkRLhyXipAS"
      },
      "outputs": [],
      "source": [
        "graph = builder.compile()\n",
        "graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1NEgrcdipAS"
      },
      "outputs": [],
      "source": [
        "response = graph.invoke({'messages':[\n",
        "    HumanMessage(content='오늘 날짜에 태어난 배우는 누구야?')]})\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAacNi4xipAS"
      },
      "outputs": [],
      "source": [
        "for data in graph.stream({'messages':[HumanMessage(content='332*17을 수행하고, 그 값으로 검색한 결과를 요약해줘')]}, stream_mode='updates'):\n",
        "    print(data)\n",
        "    print('--------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvlEDbFwipAS"
      },
      "source": [
        "만약, 모델을 Gemini가 아닌, GPT-4.1-mini 같은 모델로 바꿔 본다면 어떻게 동작할까요?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLM8acZ_mF-x"
      },
      "outputs": [],
      "source": [
        "!pip install langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bl-dQ73_ipAS"
      },
      "outputs": [],
      "source": [
        "# OpenAI API 유료 크레딧이 있는 경우에만 실행 가능\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = 'sk-...'\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "llm = ChatOpenAI(model = 'gpt-4.1-mini')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnVH644AipAS"
      },
      "outputs": [],
      "source": [
        "llm_with_tools = llm.bind_tools([multiply, current_date, tavily_search])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2yqGN5SipAS"
      },
      "outputs": [],
      "source": [
        "def tool_calling_llm(state):\n",
        "    return {\"messages\": llm_with_tools.invoke(state[\"messages\"])}\n",
        "\n",
        "\n",
        "builder = StateGraph(State)\n",
        "\n",
        "builder.add_node(\"tool_calling_llm\", tool_calling_llm)\n",
        "builder.add_node('tools', ToolNode([multiply, current_date, tavily_search]))\n",
        "\n",
        "builder.add_edge(START, 'tool_calling_llm')\n",
        "builder.add_conditional_edges('tool_calling_llm', tools_condition, END)\n",
        "builder.add_edge('tools', 'tool_calling_llm')\n",
        "\n",
        "graph = builder.compile()\n",
        "graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzSiaZisipAS"
      },
      "outputs": [],
      "source": [
        "response = graph.invoke({'messages':[\n",
        "    HumanMessage(content='오늘 날짜에 태어난 배우는 누구야?')]})\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BSgYrNVipAS"
      },
      "outputs": [],
      "source": [
        "for data in graph.stream({'messages':[HumanMessage(content='332*17을 수행하고, 그 값으로 검색한 결과를 요약해줘')]}, stream_mode='updates'):\n",
        "    print(data)\n",
        "    print('--------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDjuZlNJipAT"
      },
      "source": [
        "현재의 구조에서는, 모델에 따라 툴 실행 이행 능력과 일관성 있는 실행 능력의 차이가 발생합니다.   \n",
        "특히, sLLM과 같은 모델의 경우 툴의 실행이 복잡하거나, Context가 길어지는 경우 오류가 발생할 수 있는데요.   \n",
        "실습에서 다룬 것처럼, 프롬프트를 수정하여 작동 과정을 설명하거나, 예시를 프롬프트에 포함하여 방법을 보다 잘 이해하게 하는 방법을 고려합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7ucJf8lipAT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

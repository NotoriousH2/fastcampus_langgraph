{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-3Rdq2GMX_a"
      },
      "source": [
        "# [프로젝트] 멀티 에이전트 기반 Research Agent 만들기\n",
        "\n",
        "STORM, Open Deep Research의 구조에 착안하여, 적절한 로직을 세우고 작동하는 Research Agent를 만들어 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVMeEggrMX_b"
      },
      "outputs": [],
      "source": [
        "!pip install langgraph dotenv arxiv langchain-tavily langchain-community langchain-google-genai pymupdf -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuW14J87MX_b"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# GOOGLE_API_KEY, TAVILY_API_KEY 필수\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNdp0j4PMX_c"
      },
      "source": [
        "LangSmith 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TZ3vrXlMX_c"
      },
      "outputs": [],
      "source": [
        "os.environ['LANGCHAIN_API_KEY'] = ''\n",
        "os.environ['LANGCHAIN_PROJECT'] = 'LangGraph_FastCampus'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_TRACING_V2']='true'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32Os1iAWMX_c"
      },
      "source": [
        "## Preliminary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGEsdf5vMX_c"
      },
      "source": [
        "이번 실습에서는, 그동안 사용하지 않았던 랭체인의 특별한 기능들을 더 활용해 보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5sw934jMX_c"
      },
      "source": [
        "#### 다중 LLM 사용하기\n",
        "\n",
        "LangChain의 `init_chat_model`은 provider와 model 정보를 입력하는 방식으로   \n",
        "다양한 모델을 불러올 수 있는 기능입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVSkjUXHMX_c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
        "from langchain.chat_models import init_chat_model\n",
        "from rich import print as rprint\n",
        "\n",
        "# Gemini API는 분당 10개 요청으로 제한\n",
        "# 즉, 초당 약 0.167개 요청 (10/60)\n",
        "rate_limiter = InMemoryRateLimiter(\n",
        "    requests_per_second=0.167,  # 분당 10개 요청\n",
        "    check_every_n_seconds=0.1,  # 100ms마다 체크\n",
        "    max_bucket_size=10,  # 최대 버스트 크기\n",
        ")\n",
        "\n",
        "quick_rate_limiter = InMemoryRateLimiter(\n",
        "    requests_per_second=0.333,  # 분당 20개 요청\n",
        "    check_every_n_seconds=0.1,  # 100ms마다 체크\n",
        "    max_bucket_size=20,  # 최대 버스트 크기\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "llm = init_chat_model(\n",
        "    model_provider=\"google_genai\",\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    # model = \"gemini-2.0-flash-lite\"\n",
        "    # model = \"gemini-2.0-flash-thinking-exp\"\n",
        "    rate_limiter=rate_limiter,\n",
        "    temperature=0.8,\n",
        ")\n",
        "\n",
        "rprint(llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZ-NpTIqMX_c"
      },
      "source": [
        "## Configurables   \n",
        "\n",
        "LangGraph 어플리케이션의 작동을 조율하기 위해, State와 함께 `Configurable`를 추가할 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s__fP38sMX_c"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables.config import RunnableConfig\n",
        "from typing_extensions  import TypedDict, Optional\n",
        "\n",
        "class Configuration(TypedDict):\n",
        "\n",
        "    # Search Metadata\n",
        "    num_search_queries: int\n",
        "    max_search_depth: int\n",
        "\n",
        "    # Models\n",
        "    planner_model:str\n",
        "    planner_provider:str\n",
        "\n",
        "    writer_model:str\n",
        "    writer_provider:str\n",
        "\n",
        "    evaluator_model: str\n",
        "    evaluator_provider: str\n",
        "\n",
        "    quick_model: str\n",
        "    quick_provider: str\n",
        "\n",
        "    search_api=list[str]\n",
        "\n",
        "# Default\n",
        "\n",
        "default = Configuration(\n",
        "    num_search_queries=3,\n",
        "    max_search_depth=2,\n",
        "\n",
        "    planner_model='models/gemini-2.0-flash',\n",
        "    planner_provider='google-genai',\n",
        "\n",
        "    writer_model='models/gemini-2.0-flash',\n",
        "    writer_provider='google-genai',\n",
        "\n",
        "    evaluator_model='models/gemini-2.0-flash',\n",
        "    evaluator_provider='google-genai',\n",
        "\n",
        "    quick_model='models/gemini-2.0-flash-lite',\n",
        "    quick_provider='google-genai',\n",
        "\n",
        "\n",
        "\n",
        "    search_api=['tavily'] # 이후 다른 검색 엔진 추가\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x62IiKC2MX_c"
      },
      "source": [
        "#### 검색 API 활용하기   \n",
        "이번 프로젝트에서는 2개의 검색 API를 활용합니다.\n",
        "\n",
        "1. Tavily Search\n",
        "3. Arxiv Search\n",
        "\n",
        "추가적인 검색 엔진으로는\n",
        "PubMed 등의 도메인 특화 검색이나, Perplexity, Exa 등의 유료 API를 연결할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDQ2SXJgMX_c"
      },
      "source": [
        "Tavily Search를 구성합니다.   \n",
        "만약 모듈의 Argument를 LLM이 판단하게 하고 싶다면,   \n",
        "해당 함수를 툴로 감싸는 식으로 만들면 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yzvKabcMX_d"
      },
      "outputs": [],
      "source": [
        "# Tavily Search\n",
        "\n",
        "from langchain_tavily import TavilySearch\n",
        "\n",
        "tavily_search = TavilySearch(\n",
        "    max_results=5,\n",
        "    topic=\"general\",\n",
        "    # include_answer=False,\n",
        "    include_raw_content=True,\n",
        "    # include_images=False,\n",
        "    # include_image_descriptions=False,\n",
        "    # search_depth=\"basic\",\n",
        "    # time_range=\"day\",\n",
        "    # include_domains=None,\n",
        "    # exclude_domains=None\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjexB8lfMX_d"
      },
      "outputs": [],
      "source": [
        "result = tavily_search.invoke(\"Retrieval Augmented Generation Reasoning\")\n",
        "len(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5D0BD7mNMX_d"
      },
      "source": [
        "논문과 테크 리포트를 검색하는 학술 검색 API입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjgxneutMX_d"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import ArxivRetriever\n",
        "\n",
        "arxiv_search = ArxivRetriever(\n",
        "    load_max_docs=5,\n",
        "    load_all_available_meta=True,\n",
        "    get_full_documents=True,\n",
        "    doc_content_chars_max= 100000\n",
        "    # 10만 글자까지만 수집\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCssdkDQMX_d"
      },
      "outputs": [],
      "source": [
        "docs = arxiv_search.invoke(\"Retrieval Augmented Generation Reasoning\")\n",
        "# docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1f8IDF79MX_e"
      },
      "outputs": [],
      "source": [
        "for doc in docs:\n",
        "    print(f\"Published: {doc.metadata['Published']}\")\n",
        "    print(f\"Title: {doc.metadata['Title']}\")\n",
        "    print(f\"Authors: {doc.metadata['Authors']}\")\n",
        "    print(f\"Summary: {doc.metadata['Summary']}\")\n",
        "    print(f\"Length: {len(doc.page_content)}\")\n",
        "    print(\"-\" * 50)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uLKeJpAMX_e"
      },
      "source": [
        "각각의 검색 API를 아래와 같이 정리해 놓겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dD8PUklyMX_e"
      },
      "outputs": [],
      "source": [
        "tool_list = {\n",
        "    'tavily': tavily_search,\n",
        "    'arxiv': arxiv_search,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpBO5nVuMX_e"
      },
      "source": [
        "전체 과정은 다음과 같이 이루어집니다.\n",
        "\n",
        "1) 연구 토픽을 입력하면, LLM이 추가 정보를 질문합니다.    \n",
        "유저는 그대로 진행하거나, 피드백을 전달합니다.\n",
        "\n",
        "2) 연구 토픽에 대해, LLM이 간단한 검색을 수행하고 이를 바탕으로 연구 개요를 작성합니다.   \n",
        "\n",
        "3) 개요에 포함된 각 세션에 대해, LLM이 검색 쿼리를 생성하여 각 검색엔진을 통해 검색합니다.   \n",
        "\n",
        "4) 검색된 결과를 바탕으로 섹션별 내용을 작성합니다.    \n",
        "(검색 결과에 대한 레퍼런스 표시를 포함합니다.)\n",
        "\n",
        "5) 섹션별 드래프트를 개선하기 위해, 파생 질문을 추가로 생성하여 더 검색하거나 작성을 종료합니다.\n",
        "\n",
        "\n",
        "6) 섹션별 내용을 취합하고, 최종 수정을 거친 뒤 리포트를 완성합니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTUQMF4eMX_e"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
        "from langchain_core.output_parsers import PydanticOutputParser, StrOutputParser\n",
        "from langgraph.types import Command, interrupt\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from pydantic import BaseModel, Field\n",
        "from typing_extensions import Annotated, Literal\n",
        "import operator\n",
        "from langgraph.constants import Send"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vReime2pMX_e"
      },
      "source": [
        "## 작업에 사용할 클래스 만들기   \n",
        "Structured Output을 위한 클래스를 먼저 구성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMPncurNMX_e"
      },
      "outputs": [],
      "source": [
        "class Section(BaseModel):\n",
        "    name: str = Field(description=\"섹션의 이름\")\n",
        "    description: str = Field(description=\"해당 섹션에서 다룰 주요 주제에 대한 간략한 개요\")\n",
        "    content: str = Field(description=\"섹션의 내용 (처음에는 비워 둡니다)\")\n",
        "\n",
        "    @property\n",
        "    def as_str(self) -> str:\n",
        "        \"\"\"섹션의 정보를 포맷팅된 문자열로 변환합니다.\"\"\"\n",
        "        return f\"### {self.name}\\n{self.description}\\n\\n내용:\\n{self.content}\"\n",
        "\n",
        "class ReportPlan(BaseModel):\n",
        "    sections: list[Section] = Field(description=\"A list of sections for the report.\")\n",
        "    followup_question: str = Field(description=\"사용자에게 추가로 질문할 내용 (없으면 '')\")\n",
        "\n",
        "    @property\n",
        "    def as_str(self) -> str:\n",
        "        \"\"\"섹션들을 포맷팅된 문자열로 변환합니다.\"\"\"\n",
        "        sections_str = []\n",
        "        for section in self.sections:\n",
        "            sections_str.append(f\"### {section.name}\\n{section.description}\")\n",
        "        return \"\\n\\n\".join(sections_str)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjRNXqI_MX_e"
      },
      "source": [
        "## 서브모듈: 토픽에 대한 리서치 모듈 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZIu_fZdMX_e"
      },
      "source": [
        "섹션의 개요가 주어지면, 해당 내용을 검색하여 섹션을 작성하는 과정을 구현해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVkmYSCGMX_e"
      },
      "outputs": [],
      "source": [
        "class ResearchState(TypedDict):\n",
        "    topic: str\n",
        "    section: Section\n",
        "    queries: list[str]\n",
        "    draft: str\n",
        "    resources: list\n",
        "    num_revision: int\n",
        "    finished: bool\n",
        "\n",
        "\n",
        "def generate_search_query(state: ResearchState, config: RunnableConfig):\n",
        "    prompt = ChatPromptTemplate([\n",
        "('system', f'''\n",
        "주어진 섹션 정보에 대한 사전 조사를 위해, 효과적인 검색 쿼리를 생성해야 합니다.\n",
        "해당 주제를 포괄적으로 다룰 수 있는 검색 쿼리들을 생성하세요.\n",
        "\n",
        "검색 쿼리는 다음과 같은 원칙을 따라야 합니다:\n",
        "1. 핵심 키워드를 포함해야 합니다\n",
        "2. 너무 일반적이지 않아야 합니다\n",
        "3. 학술적이고 전문적인 용어를 사용해야 합니다\n",
        "4. 최신 연구 동향을 반영해야 합니다\n",
        "5. 따옴표를 포함하지 않아야 합니다.\n",
        "\n",
        "적절한 검색 쿼리를 한 줄에 하나씩 작성하세요.\n",
        "쿼리만 출력하고, {config['configurable']['num_search_queries']} 개의 쿼리를 출력하세요.\n",
        "\n",
        "'''),\n",
        "('user', '''\n",
        "섹션 정보:\n",
        "{section}\n",
        "\n",
        "''')])\n",
        "    section = state['section']\n",
        "\n",
        "    writer_llm = init_chat_model(\n",
        "        model_provider = config['configurable']['writer_provider'],\n",
        "        model= config['configurable']['writer_model'],\n",
        "        rate_limiter=rate_limiter,\n",
        "        temperature=0.8,\n",
        "        max_tokens = 8192\n",
        "    )\n",
        "\n",
        "\n",
        "    chain = prompt | writer_llm | StrOutputParser() | (lambda x: x.split('\\n'))\n",
        "\n",
        "    queries = chain.invoke(section.as_str)\n",
        "\n",
        "    return {'queries':queries}\n",
        "\n",
        "def search_and_filter(state: ResearchState, config: RunnableConfig):\n",
        "    '''각각의 검색어에 대해 검색 결과를 수행하고 필터링합니다.\n",
        "    Tavily Search는 정확성이 높기 때문에, 해당 부분을 생략해도 됩니다..\n",
        "    빠른 LLM을 사용하겠습니다.\n",
        "    '''\n",
        "    quick_llm = init_chat_model(\n",
        "        model_provider = config['configurable']['quick_provider'],\n",
        "        model= config['configurable']['quick_model'],\n",
        "        rate_limiter=quick_rate_limiter,\n",
        "        temperature=0.8,\n",
        "    )\n",
        "\n",
        "    search_tool =  tool_list[config['configurable']['search_api'][0]]\n",
        "    # 적절한 검색 툴 선택 (여기서는 Tavily로 고정)\n",
        "\n",
        "    queries = state['queries']\n",
        "    section = state['section']\n",
        "\n",
        "    relevant_docs=[]\n",
        "\n",
        "\n",
        "    filter_prompt=ChatPromptTemplate([\n",
        "        ('system', f'''다음 검색 결과가 주어진 주제와 관련이 있는지 O/X로 판단하세요.\n",
        "O/X만 출력하세요.\n",
        "---\n",
        "주제: {section.as_str}'''),\n",
        "\n",
        "('user', '''\n",
        "검색 결과: {doc}''')])\n",
        "    chain = filter_prompt | quick_llm | StrOutputParser()\n",
        "\n",
        "    context = search_tool.batch(queries)\n",
        "\n",
        "    def preprocess(text):\n",
        "        import re\n",
        "        # 탭과 개행문자를 공백으로 변환\n",
        "        text = text.replace('\\t', ' ').replace('\\n', ' ').replace('\\xa0', ' ')\n",
        "        # 템플릿 오류 방지\n",
        "        text = text.replace('{', '(').replace('}', ')')\n",
        "\n",
        "        # 연속된 공백을 하나로 치환\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text\n",
        "\n",
        "\n",
        "    for docs in context:\n",
        "        try:\n",
        "            for doc in docs['results']:\n",
        "                doc_str = f\"### {doc['title']} \\n URL: {doc['url']} \\n {doc['raw_content']}\" if doc.get('raw_content') else f\"### {doc['title']} \\n {doc['content']}\"\n",
        "                # 하나로 만든 뒤 전처리\n",
        "                doc_str = preprocess(doc_str)\n",
        "                relevance = chain.invoke(doc_str)\n",
        "                if relevance=='O':\n",
        "                    relevant_docs.append(doc_str)\n",
        "        except: # 검색 오류시\n",
        "            continue\n",
        "    print(f'# Filtered Docs: {len(relevant_docs)}')\n",
        "    return {'resources':relevant_docs}\n",
        "\n",
        "def write_section(state: ResearchState, config: RunnableConfig):\n",
        "    section = state['section'].as_str\n",
        "    topic = state['topic']\n",
        "    resources = state['resources']\n",
        "\n",
        "    writer_prompt =ChatPromptTemplate([\n",
        "        ('system', '''\n",
        "연구 리포트의 주제와 세부 섹션명이 주어집니다.\n",
        "아래의 정보를 활용하여, 연구 리포트의 한 섹션을 작성하세요.\n",
        "다음은 작성 가이드라인입니다.\n",
        "\n",
        "\n",
        "[작성 가이드라인]\n",
        "간단하고 명확한 언어를 사용하세요.\n",
        "섹션명은 마크다운 ## 으로 작성하며, 세부 목차는 만들지 말고 문단으로만 분리하세요.\n",
        "문장을 너무 길게 쓰지 말고, 이해하기 쉽게 작성하세요.\n",
        "'이다.' 가 아닌 '입니다.', '합니다.' 등의 스타일로 작성하세요.\n",
        "또한, 아래에 주어지는 정보의 내용을 최대한 활용하여 작성하세요.\n",
        "\n",
        "\n",
        "[인용 가이드라인]\n",
        "인용 표시는 [1], [2]와 같이 작성하고, 섹션 마지막에 레퍼런스를 작성하세요.\n",
        "레퍼런스 형식은 MLA 표기를 따르고, 마지막에 URL도 표시하세요.\n",
        "예시 표시 형식은 다음과 같습니다.\n",
        "\n",
        "**References**\n",
        "[1] Unite.AI. \"DeepMind의 Michelangelo 벤치마크: Long-Context LLM의 한계를 드러내다.\" *Unite.AI*, [https://www.unite.ai/ko/\n",
        "<br><br>\n",
        "[2] ...\n",
        "\n",
        "\n",
        "[노트]\n",
        "인용 표시를 정확하게 했는지 확인하고, 주장이 기술되는 경우 가급적 소스에 근거하도록 작성하세요.\n",
        "마크다운 형식을 고려하여, 문단 분리나 레퍼런스 사이의 줄바꿈을 명확하게 하세요.\n",
        "\n",
        "[기존 드래프트]\n",
        "\n",
        "기존 드래프트가 주어지는 경우, 여기에 이어서 작성하세요.\n",
        "'''),\n",
        "\n",
        "('user',f'''\n",
        "주제: {topic}\n",
        "\n",
        "세부 섹션명: {section}\n",
        "\n",
        "검색 결과 Context:\n",
        "{resources}\n",
        "''')\n",
        "])\n",
        "    writer_llm = init_chat_model(\n",
        "        model_provider = config['configurable']['writer_provider'],\n",
        "        model= config['configurable']['writer_model'],\n",
        "        rate_limiter=rate_limiter,\n",
        "        temperature=0.8,\n",
        "        max_tokens = 8192\n",
        "    )\n",
        "    chain = writer_prompt | writer_llm | StrOutputParser()\n",
        "    draft = chain.invoke({})\n",
        "    return {'draft':draft}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Feedback(BaseModel):\n",
        "    grade : Literal['Good', 'Bad'] = Field(description='드래프트에 대한 평가')\n",
        "    queries: Optional[list[str]] = Field(description='검색 쿼리 목록')\n",
        "\n",
        "def refine_research(state: ResearchState, config: RunnableConfig) -> Command[Literal[END, \"search_and_filter\"]] :\n",
        "    '''Draft와 context를 평가하여, 추가 검색이 필요한지 판단합니다.\n",
        "    Revision 개수를 초과하면 바로 END로 이동합니다.'''\n",
        "\n",
        "    section = state['section'].as_str\n",
        "    topic = state['topic']\n",
        "    resources = state['resources']\n",
        "    draft = state['draft']\n",
        "    queries = state['queries']\n",
        "    num_revision = state['num_revision']\n",
        "\n",
        "    prompt = ChatPromptTemplate([\n",
        "        ('system', f'''\n",
        "연구 리포트의 주제와 세부 섹션명이 주어집니다.\n",
        "\n",
        "현재 작성된 섹션의 드래프트를 평가하세요.\n",
        "\n",
        "이 글의 내용을 명확하고 유익하게 작성하기 위해,\n",
        "검색된 결과 이외의 새로운 내용을 더 조사해야 하는지 판단하세요.\n",
        "\n",
        "이후, 추가 검색을 위해 필요한 검색어 쿼리를 작성하세요.\n",
        "{config['configurable']['num_search_queries']} 개 이하의 쿼리를 출력하세요.\n",
        "\n",
        "해당 글은 충분히 완성도가 높아 추가 조사가 필요하지 않을 수도 있습니다.\n",
        "그런 경우에는 'Good'을 출력하고, 추가 쿼리를 비워두세요.\n",
        "'''),\n",
        "\n",
        "('user',f'''\n",
        "주제: {topic}\n",
        "\n",
        "세부 섹션명: {section}\n",
        "\n",
        "\n",
        "드래프트: {draft}''')\n",
        "])\n",
        "    # revision 개수 넘어가면 바로 END\n",
        "    if num_revision >= config['configurable']['max_search_depth']:\n",
        "         return Command(goto = END,\n",
        "                       update = {'finished':True})\n",
        "\n",
        "\n",
        "    evaluator_llm = init_chat_model(\n",
        "        model_provider = config['configurable']['evaluator_provider'],\n",
        "        model= config['configurable']['evaluator_model'],\n",
        "        rate_limiter=rate_limiter,\n",
        "        temperature=0.8,\n",
        "    )\n",
        "\n",
        "\n",
        "    chain = prompt | evaluator_llm.with_structured_output(Feedback)\n",
        "\n",
        "    feedback = chain.invoke({})\n",
        "\n",
        "    if feedback.grade =='Good':\n",
        "        return Command(goto = END,\n",
        "                       update = {'finished':True})\n",
        "    else:\n",
        "        return Command(goto = 'search_and_filter',\n",
        "                       update= {'queries': feedback.queries,\n",
        "                                'num_revision': num_revision+1})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90aa_jYTMX_e"
      },
      "source": [
        "Research Agent를 구성하는 Small Graph를 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwwghaKjMX_e"
      },
      "outputs": [],
      "source": [
        "builder = StateGraph(ResearchState)\n",
        "builder.add_node(generate_search_query)\n",
        "builder.add_node(search_and_filter)\n",
        "builder.add_node(write_section)\n",
        "builder.add_node(refine_research)\n",
        "\n",
        "\n",
        "builder.add_edge(START, 'generate_search_query')\n",
        "builder.add_edge('generate_search_query', 'search_and_filter')\n",
        "builder.add_edge('search_and_filter', 'write_section')\n",
        "builder.add_edge('write_section', 'refine_research')\n",
        "\n",
        "memory = MemorySaver()\n",
        "\n",
        "\n",
        "\n",
        "researcher_graph = builder.compile(checkpointer=memory)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "researcher_graph"
      ],
      "metadata": {
        "id": "wPIqd5QpM64d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JYkRyKIMX_f"
      },
      "outputs": [],
      "source": [
        "t = Section(name='서론', description='1M Context Windows 모델의 시대를 연 구글 Gemini와 Llama 4 모델에 대해 설명합니다.', research=True, content='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3chM8e7yMX_f"
      },
      "outputs": [],
      "source": [
        "test_research_state={\n",
        "    'topic': '1M Context Windows 모델',\n",
        "    'section': t,\n",
        "    'num_revision':0\n",
        "}\n",
        "\n",
        "# Thread\n",
        "thread = {'configurable':default, 'thread_id':'0'}\n",
        "\n",
        "history = []\n",
        "for event in researcher_graph.stream(test_research_state, thread, stream_mode=\"updates\"):\n",
        "    for status in event:\n",
        "        print(f'# {status}')\n",
        "        for key in event[status]:\n",
        "            value = str(event[status][key])\n",
        "            if len(value)>300:\n",
        "                print(f'- {key}: {value[:300]}')\n",
        "            else:\n",
        "                print(f'- {key}: {value}')\n",
        "        print('---------')\n",
        "    history.append(event)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "load_dotenv('.env', override=True)"
      ],
      "metadata": {
        "id": "ZoMPg3nuNyIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYBeIALAMX_f"
      },
      "source": [
        "작성한 섹션 드래프트를 확인해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LOKY-YGMX_f"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "import textwrap\n",
        "\n",
        "result = history[-2]['write_section']['draft']\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('•', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
        "\n",
        "to_markdown(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcQ9Di7AMX_f"
      },
      "source": [
        "섹션별 Writer를 구성했습니다.   \n",
        "이제 해당 그래프를 서브모듈로 하는 에이전트 구조를 구성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NfpjFLxMX_f"
      },
      "outputs": [],
      "source": [
        "class State(TypedDict):\n",
        "    topic: str\n",
        "    plan: ReportPlan\n",
        "    result: str\n",
        "    human_feedback:str\n",
        "    finished_drafts: Annotated[list[str], operator.add]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elLo7DFyMX_f"
      },
      "source": [
        "요청을 받은 뒤, 초기 설정과 함께 부가 질문을 수행합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SmdraBMMX_f"
      },
      "outputs": [],
      "source": [
        "def initiate_report(state: State , config: RunnableConfig):\n",
        "\n",
        "    prompt = ChatPromptTemplate([\n",
        "        ('system','''\n",
        "당신은 주어진 주제에 대한 연구 보고서의 초기 방향 설정을 위한 개요를 구성합니다.\n",
        "최대한 최신의 지식과 인사이트를 활용하여야 하며, 사용자를 위한 맞춤형 보고서가 되어야 합니다.\n",
        "\n",
        "주어진 주제에 대한 섹션별 개요를 작성하세요.\n",
        "단, 마지막 섹션인 결론은 제외하고 작성하세요.\n",
        "\n",
        "각 섹션은 불필요한 요소를 포함하지 말아야 하며, 명확하게 구분되어야 합니다.\n",
        "섹션 간의 겹치는 내용을 최대한 줄이고, 각각의 역할이 분명하도록 구성하세요.\n",
        "\n",
        "또한, 최종 결과 보고서에 사용자의 선호를 최대한 반영하기 위해, 사용자에게 추가로 질문할 내용도 작성하세요.\n",
        "예를 들어, 세부 분야, 적용하고자 하는 환경, 원하는 정리 형식 등을 질문할 수 있습니다.\n",
        "\n",
        "다음은 예시입니다.\n",
        "\n",
        "---\n",
        "질문: LLM 파인 튜닝 방법인 LoRA의 최근 발전된 모델들에 대해 조사해줘\n",
        "\n",
        "\n",
        "답변:\n",
        "개요: (보고서의 개요)\n",
        "추가 질문: LoRA 기반 LLM 파인튜닝 관련 최근 발전된 모델들에 대해 조사해드릴게요.\n",
        "아래 항목들 중 가능한 정보를 알려주시면 더 정확한 조사를 도와드릴 수 있어요:\n",
        "용도 (예: 챗봇, 코드 생성, 번역, 의료 등)\n",
        "적용 환경 (예: 연구용, 기업 서비스용, 모바일 디바이스 등)\n",
        "원하시는 정리 형식 (예: 표, 요약 보고서, 논문 중심 정리 등)\n",
        "가능한 범위를 알려주시면 곧바로 조사 시작할게요!'''),\n",
        "\n",
        "('human', '''사용자의 주제(혹은 요청): {topic}\n",
        "\n",
        "---\n",
        "\n",
        "관련 최신 검색 결과:\n",
        "{context}\n",
        " ''')\n",
        "    ])\n",
        "\n",
        "    topic = state['topic']\n",
        "\n",
        "    search_tool =  tool_list[config['configurable']['search_api'][0]]\n",
        "    # 적절한 검색 툴 선택 (여기서는 Tavily로 고정)\n",
        "\n",
        "\n",
        "    context = search_tool.invoke(topic)\n",
        "    # 초기 검색을 수행하여 개요 작성\n",
        "\n",
        "    planner_llm = init_chat_model(\n",
        "        model_provider = config['configurable']['planner_provider'],\n",
        "        model= config['configurable']['planner_model'],\n",
        "        rate_limiter=rate_limiter,\n",
        "        temperature=0.8,\n",
        "    ).with_structured_output(ReportPlan)\n",
        "\n",
        "    chain = prompt | planner_llm\n",
        "\n",
        "    result = chain.invoke({'topic': topic, 'context' : context})\n",
        "\n",
        "    print('# Planner: ')\n",
        "\n",
        "    print(f'''\n",
        "# 보고서 작성 개요:\n",
        "{result.as_str}\n",
        "\n",
        "# 추가 질문:\n",
        "{result.followup_question}''')\n",
        "    return {'plan':result}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6zgR-0uMX_f"
      },
      "outputs": [],
      "source": [
        "def human_review(state: State , config: RunnableConfig):\n",
        "\n",
        "    human_review = interrupt(\n",
        "        {\n",
        "            \"question\": \"피드백을 전달해 주세요, 이대로 진행하고 싶으시면, continue 또는 go만 입력하세요.\",\n",
        "        }\n",
        "    )\n",
        "    # Human Feedback을 받아 전달\n",
        "    review_action = human_review.get(\"human_feedback\")\n",
        "    return {'human_feedback': review_action}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCdKmVAwMX_g"
      },
      "outputs": [],
      "source": [
        "def refine_outline(state: State , config: RunnableConfig):\n",
        "    current_plan = state['plan']\n",
        "    human_feedback = state['human_feedback']\n",
        "\n",
        "    if human_feedback.lower()=='go' or human_feedback.lower()=='continue':\n",
        "        return {'plan': current_plan}\n",
        "\n",
        "    refine_prompt = PromptTemplate(template='''\n",
        "보고서의 개요가 주어집니다.\n",
        "추가 요청사항을 반영하여, 수정된 개요를 작성하세요:\n",
        "\n",
        "기존 개요:\n",
        "{current_plan}\n",
        "\n",
        "---\n",
        "\n",
        "피드백:\n",
        "{feedback}\n",
        "\n",
        "    ''')\n",
        "\n",
        "    planner_llm = init_chat_model(\n",
        "        model_provider = config['configurable']['planner_provider'],\n",
        "        model= config['configurable']['planner_model'],\n",
        "        rate_limiter=rate_limiter,\n",
        "        temperature=0.8,\n",
        "    ).with_structured_output(ReportPlan)\n",
        "\n",
        "    # 수정된 계획 생성\n",
        "    refine_chain = refine_prompt | planner_llm\n",
        "\n",
        "    refined_plan = refine_chain.invoke({\n",
        "        'current_plan': current_plan.as_str,\n",
        "        'feedback': human_feedback\n",
        "    })\n",
        "\n",
        "    print('# 수정된 보고서 계획:')\n",
        "    print(f'''\n",
        "    {refined_plan.as_str}\n",
        "\n",
        "    추가 질문:\n",
        "    {refined_plan.followup_question}\n",
        "    ''')\n",
        "\n",
        "    return {'plan': refined_plan}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8PQ4c0ZMX_g"
      },
      "outputs": [],
      "source": [
        "def research(state:ResearchState, config: RunnableConfig):\n",
        "    result = researcher_graph.invoke({\n",
        "    'topic': state['topic'],\n",
        "    'section': state['section'],\n",
        "    'num_revision':0\n",
        "    })\n",
        "    draft = result['draft']\n",
        "\n",
        "    return {'finished_drafts':[draft]}\n",
        "\n",
        "\n",
        "def start_survey(state:State, config: RunnableConfig):\n",
        "    topic = state['topic']\n",
        "    plan = state['plan']\n",
        "    # Query 생성, 수집, Reflection, 섹션 작성 모듈을 하나의 에이전트로 구성\n",
        "\n",
        "    return [Send(\"research\",\n",
        "            {'topic':topic, \"section\": s}) for s in plan.sections]\n",
        "\n",
        "\n",
        "def synthesizer(state:State, config: RunnableConfig):\n",
        "    return {'result':'\\n'.join(state['finished_drafts'])}\n",
        "\n",
        "def finalizer(state:State, config: RunnableConfig):\n",
        "    prompt = PromptTemplate(template='''\n",
        "연구 보고서의 내용이 주어집니다.\n",
        "전체 흐름을 고려하여, 최종 결론 섹션을 작성하세요.\n",
        "\n",
        "전체 보고서 내용:\n",
        "{result}''')\n",
        "\n",
        "    chain = prompt | llm | StrOutputParser()\n",
        "    result = chain.invoke({'result':state['result']})\n",
        "    return {'result':state['result'] + '\\n'+result}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfpKDNmoMX_g"
      },
      "outputs": [],
      "source": [
        "builder = StateGraph(State)\n",
        "builder.add_node(initiate_report)\n",
        "builder.add_node(human_review)\n",
        "builder.add_node(refine_outline)\n",
        "builder.add_node(research)\n",
        "builder.add_node(synthesizer)\n",
        "builder.add_node(finalizer)\n",
        "\n",
        "builder.add_edge(START, 'initiate_report')\n",
        "builder.add_edge('initiate_report','human_review')\n",
        "builder.add_edge('human_review', 'refine_outline')\n",
        "\n",
        "builder.add_conditional_edges(\"refine_outline\", start_survey, [\"research\"])\n",
        "\n",
        "builder.add_edge('research', 'synthesizer')\n",
        "builder.add_edge('synthesizer', 'finalizer')\n",
        "builder.add_edge('finalizer',END)\n",
        "memory = MemorySaver()\n",
        "\n",
        "\n",
        "\n",
        "graph = builder.compile(checkpointer=memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9uy6nR0MX_g"
      },
      "outputs": [],
      "source": [
        "graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7UOM1RUMX_g"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "display(Image(graph.get_graph(xray=True).draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siRf-tSfMX_g"
      },
      "source": [
        "완성된 그래프를 실행해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBpwoEzJMX_h"
      },
      "outputs": [],
      "source": [
        "test_state = {'topic':'Long-Context LLM의 시대'}\n",
        "thread = {'configurable':default, 'thread_id':'0'}\n",
        "\n",
        "graph.invoke(test_state, config = thread)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kd7ZcOWrMX_h"
      },
      "outputs": [],
      "source": [
        "# BottleNeck: Search and Filter\n",
        "# 긴 컨텍스트 모델은 필터링을 안 하는 방법도 고려할 수 있겠습니다..\n",
        "'''좋아, Gemini 2.5 Pro는\n",
        "긴 컨텍스트에서도 성능이 엄청 좋던데, 그 부분을 중요하게 다뤄 주고.\n",
        "Llama 4의 Long Context에 대해서도 알려줘.        '''\n",
        "\n",
        "history =[]\n",
        "\n",
        "for event in graph.stream(\n",
        "        Command(resume={\"human_feedback\": \"\"\"섹션 개수를 2개로 줄여줘.\"\"\"}),\n",
        "    thread,\n",
        "    stream_mode=\"updates\", subgraphs=True\n",
        "):\n",
        "    history.append(event)\n",
        "    for status in event:\n",
        "        print(f'# {str(status)[:300]}')\n",
        "\n",
        "        try:\n",
        "            for key in event[status]:\n",
        "                value = str(event[status][key])\n",
        "                if len(value)>300:\n",
        "                    print(f'- {key}: {value[:300]}')\n",
        "                else:\n",
        "                    print(f'- {key}: {value}')\n",
        "            print('---------')\n",
        "        except:\n",
        "            continue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNKe1e1ZMX_h"
      },
      "source": [
        "결과물을 md 파일에 저장해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4XsSuSwMX_h"
      },
      "outputs": [],
      "source": [
        "result = history[-1][1]['finalizer']['result']\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0nbe60pMX_h"
      },
      "outputs": [],
      "source": [
        "with open(\"example.md\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(result)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "multicampus",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
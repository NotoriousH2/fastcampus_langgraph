{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a435919b",
      "metadata": {
        "id": "a435919b"
      },
      "source": [
        "# [실습] LangChain으로 OpenAI GPT와 Google Gemini API 사용하기\n",
        "\n",
        "LangChain(랭체인)은 LLM 기반의 어플리케이션을 효율적으로 개발할 수 있게 해주는 라이브러리입니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BJQjvbvK-NMZ",
      "metadata": {
        "id": "BJQjvbvK-NMZ"
      },
      "source": [
        "LangChain은 GPT, Gemini 등의 API와 HuggingFace, Ollama 등의 오픈 모델 환경 모두에서 사용할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "784f262e",
      "metadata": {
        "id": "784f262e"
      },
      "source": [
        "이번 실습에서는 대표적인 LLM인 Google Gemini와 OpenAI GPT의 API를 사용해 진행하겠습니다.    \n",
        "\n",
        "Gemini는 무료 사용량이 존재하지만, GPT는 유료 API 크레딧이 필요합니다.   \n",
        "만약 유료 크레딧이 없으신 분들은 Gemini만으로 진행해 주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf68e6c0",
      "metadata": {
        "id": "bf68e6c0",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-community langchain-google-genai langchain-openai"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "babb35b6",
      "metadata": {
        "id": "babb35b6"
      },
      "source": [
        "Google Colab 환경이 아닌 경우에는, 아래 라이브러리도 설치해야 할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3293eab",
      "metadata": {
        "id": "a3293eab"
      },
      "outputs": [],
      "source": [
        "# !pip install google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0471bfd",
      "metadata": {
        "id": "d0471bfd"
      },
      "source": [
        "## LLM\n",
        "\n",
        "LangChain에서, LLM을 부르는 방법은 주로 `ChatOpenAI`, `ChatGoogleGenerativeAI`와 같은 개별 클래스를 불러오거나,   \n",
        "`init_chat_model`을 통해 Provider와 모델 이름을 전달하는 방식으로 이루어집니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8e3fb01",
      "metadata": {
        "id": "c8e3fb01"
      },
      "source": [
        "### API 키 준비하기\n",
        "\n",
        "\n",
        "Google API 키를 등록하고 입력합니다.   \n",
        "구글 계정 로그인 후 https://aistudio.google.com  에 접속하면, API 키 생성이 가능합니다.   \n",
        "\n",
        "OpenAI API 키는 유료 계정 로그인 후   \n",
        "https://platform.openai.com/api-keys 에 접속하면 생성이 가능합니다.    \n",
        "(유료 계정과 무관하게, 크레딧 결제가 필요합니다.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8c1faf5",
      "metadata": {
        "id": "b8c1faf5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ['GOOGLE_API_KEY']=\"AIxxx\"\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = 'sk-...'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35163987",
      "metadata": {
        "id": "35163987"
      },
      "source": [
        "Google AI Studio의 `Create Prompt`에서, 모델 목록과 무료 API 사용량을 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d7356b8",
      "metadata": {
        "id": "7d7356b8"
      },
      "source": [
        "OpenAI 모델의 목록과 가격은 https://openai.com/api/pricing/ 에서 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3b4252b",
      "metadata": {
        "id": "b3b4252b"
      },
      "source": [
        "## LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36b0c4ab",
      "metadata": {
        "id": "36b0c4ab"
      },
      "source": [
        "chat 모델 사용을 위해 ChatGoogleGenerativeAI, ChatOpenAI를 불러오겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4efc9e7",
      "metadata": {
        "id": "a4efc9e7"
      },
      "source": [
        "모델마다 다른 Safety 등의 요소를 제외하고, 공통적으로 아래의 파라미터를 갖습니다.\n",
        "- model : 모델의 이름입니다.\n",
        "- temperature : 모델 출력의 무작위성을 결정합니다. 0부터 2 사이의 값을 지정할 수 있으며,   \n",
        "숫자가 클수록 무작위 출력이 증가합니다.    \n",
        "(o3-mini, o1 등의 Reasoning 모델은 지원하지 않는 경우도 있습니다)\n",
        "\n",
        "- max_tokens : 출력의 최대 길이를 지정합니다. 해당 토큰 수가 넘어가면 출력이 중간에 종료됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89b1573c",
      "metadata": {
        "id": "89b1573c"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    temperature = 0.7,\n",
        "    max_tokens = 2048\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "034c0221",
      "metadata": {
        "id": "034c0221"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm_gpt = ChatOpenAI(\n",
        "    model = 'gpt-4o-mini',\n",
        "    temperature = 1.0,\n",
        "    max_tokens = 2048\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "977ce129",
      "metadata": {
        "id": "977ce129"
      },
      "source": [
        "LangChain은 프롬프트, LLM, 체인 등의 구성 요소를 서로 연결하는 방식으로 구성됩니다.  \n",
        "각각의 요소를 `Runnable`이라고 부르는데요.   \n",
        "`Runnable`은  `invoke()`를 통해 실행합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d97eb36",
      "metadata": {
        "id": "7d97eb36"
      },
      "outputs": [],
      "source": [
        "question = '''프롬프트 엔지니어링의 핵심적인 요소 4개가 뭔가요?'''\n",
        "\n",
        "response = llm.invoke(question)\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EL_0_8OfOwWL",
      "metadata": {
        "id": "EL_0_8OfOwWL"
      },
      "outputs": [],
      "source": [
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2782cf0b",
      "metadata": {
        "id": "2782cf0b"
      },
      "source": [
        "위처럼 문자열을 그대로 입력하게 되면, 해당 문자열은 HumanMessage 클래스로 변환되어 입력됩니다.   \n",
        "HumanMessage에 대한 출력 형식은 AIMessage 클래스로 정의됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "055a3eb7",
      "metadata": {
        "id": "055a3eb7"
      },
      "outputs": [],
      "source": [
        "question = '''울림을 주는 2000년대 영화 명대사를 하나 알려주세요.\n",
        "대사가 나온 배경과 의미도 한 문장으로 설명해 주세요.'''\n",
        "response = llm.invoke(question)\n",
        "\n",
        "print('# Gemini-2.0-Flash의 답변:', response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe1c2f4e",
      "metadata": {
        "id": "fe1c2f4e"
      },
      "source": [
        "만약, 여러 개의 모델을 불러오고 싶은 경우에는 아래와 같이 공통 인터페이스를 사용할 수도 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76d97a5d",
      "metadata": {
        "id": "76d97a5d"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "gpt_4o = init_chat_model(\"gpt-4o\", model_provider=\"openai\", temperature = 1.0)\n",
        "gemini_2_0_flash = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\", temperature = 1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VHWGT7GVthn8",
      "metadata": {
        "id": "VHWGT7GVthn8"
      },
      "source": [
        "## 스트리밍\n",
        "\n",
        "스트리밍은 모델을 토큰이 생성되는 순서대로 출력하는 방법입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9SwpX3pDtrD2",
      "metadata": {
        "id": "9SwpX3pDtrD2"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "chunks = []\n",
        "for chunk in llm.stream(\"5문장으로 당신을 소개해주세요. 매 문장마다 줄을 띄우세요.\"):\n",
        "    #time.sleep(0.4)\n",
        "    print(chunk.content, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "775dd0f5",
      "metadata": {
        "id": "775dd0f5"
      },
      "source": [
        "실제 환경에서는 프롬프트의 형태를 사전에 설정하고,   \n",
        "같은 형태로 입력 변수가 주어질 때마다 프롬프트를 작성하게 하는 것이 효율적입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "442b54b8",
      "metadata": {
        "id": "442b54b8"
      },
      "source": [
        "## Prompt Template"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "050a9165",
      "metadata": {
        "id": "050a9165"
      },
      "source": [
        "LangChain은 프롬프트의 템플릿을 구성할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2585b0a2",
      "metadata": {
        "id": "2585b0a2"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "explain_template = \"\"\"당신은 주어진 단어에 대해, 유머러스하게 한 문장으로 표현합니다.\n",
        "\n",
        "제시어: {word}\"\"\"\n",
        "print(explain_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7ef1d3c",
      "metadata": {
        "id": "e7ef1d3c"
      },
      "outputs": [],
      "source": [
        "explain_prompt = PromptTemplate(template = explain_template)\n",
        "\n",
        "explain_prompt.format(word = \"트랜스포머 네트워크\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee20773a",
      "metadata": {
        "id": "ee20773a"
      },
      "outputs": [],
      "source": [
        "llm.invoke(explain_prompt.format(word = \"트랜스포머 네트워크\")).content"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57a650ca",
      "metadata": {
        "id": "57a650ca"
      },
      "source": [
        "두 개의 매개변수를 받아 프롬프트를 만들어 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a2bc2a2",
      "metadata": {
        "id": "1a2bc2a2"
      },
      "outputs": [],
      "source": [
        "translate_template = \"{topic}에 대해 {language}로 설명하세요.\"\n",
        "\n",
        "translate_prompt = PromptTemplate(template = translate_template)\n",
        "\n",
        "translate_prompt.format(topic='torschlusspanik', language='초등학생을 위한 한국어')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5448adc8",
      "metadata": {
        "id": "5448adc8"
      },
      "outputs": [],
      "source": [
        "X = translate_prompt.format(topic='torschlusspanik', language='한국어')\n",
        "response = llm.invoke(X)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iPuHSIRSavQL",
      "metadata": {
        "id": "iPuHSIRSavQL"
      },
      "source": [
        "## Chat Prompt Template"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e28d4ed",
      "metadata": {
        "id": "4e28d4ed"
      },
      "source": [
        "Web UI를 통해 ChatGPT, Claude 등의 LLM을 실행하는 경우와 다르게,   \n",
        "API의 호출은 유저 메시지 이외의 다양한 메시지를 사용할 수 있습니다.   \n",
        "- system: AI 모델의 행동 방식을 결정하는 시스템 메시지\n",
        "- user(human): 사용자의 메시지\n",
        "- ai(assistant): AI 모델의 메시지\n",
        "\n",
        "이는 LangChain 내부에서 모델에 맞는 템플릿으로 변환되어 입력됩니다.   \n",
        "\n",
        "Ex) 라마 3 시리즈의 템플릿\n",
        "```\n",
        "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "You are a helpful AI assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Hello!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "```\n",
        "\n",
        "Qwen 시리즈의 템플릿\n",
        "```\n",
        "<|im_start|>system\n",
        "You are a helpful AI assistant\n",
        "<|im_end|>\n",
        "<|im_start|>user\n",
        "Hello!\n",
        "<|im_end|>\n",
        "<|im_start|>assistant\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yA82Zgf9ssZ7",
      "metadata": {
        "id": "yA82Zgf9ssZ7"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate([\n",
        "    (\"system\", '당신은 항상 부정적인 말만 하는 챗봇입니다. 첫 문장은 항상 사용자의 의견을 반박하고, 이후 대안을 제시하세요.'),\n",
        "    (\"user\", '{A} 너무 좋은 것 같아요!')\n",
        "    # system, user = human, ai = assistant\n",
        "]\n",
        ")\n",
        "prompt.format_messages(A='LangChain')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcd27ac1",
      "metadata": {
        "id": "fcd27ac1"
      },
      "outputs": [],
      "source": [
        "llm.invoke(prompt.format_messages(A='LangChain'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33b2d3c9",
      "metadata": {
        "id": "33b2d3c9"
      },
      "source": [
        "또는, 아래와 같이 메시지를 직접 불러와 사용할 수도 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a53fd272",
      "metadata": {
        "id": "a53fd272"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
        "\n",
        "topic=''\n",
        "\n",
        "msgs = [\n",
        "    SystemMessage('항상 다섯 단어로 표현하세요.'),\n",
        "     HumanMessage(f'{topic}에 대해 설명해줘!')\n",
        "]\n",
        "\n",
        "response = llm.invoke(msgs)\n",
        "print('# Gemini-2.0-Flash의 답변:', response.content)\n",
        "\n",
        "response = llm_gpt.invoke(msgs)\n",
        "print('# GPT-4o-mini의 답변:', response.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cbe4374",
      "metadata": {
        "id": "8cbe4374"
      },
      "source": [
        "## Few-Shot Prompting\n",
        "모델이 참고할 예시를 포함하는 퓨 샷 프롬프팅은\n",
        "   \n",
        "   모델 출력의 형식과 구조를 효과적으로 변화시킬 수 있습니다.  \n",
        "\n",
        "\n",
        "Few-Shot Prompt Template을 이용해 example을 프롬프트에 추가해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1e86b21",
      "metadata": {
        "id": "c1e86b21"
      },
      "outputs": [],
      "source": [
        "# 예시 : Prompt Example 2개\n",
        "from langchain_core.prompts.few_shot import FewShotPromptTemplate\n",
        "\n",
        "examples = [\n",
        "    {\n",
        "        \"question\": \"Are both the directors of Jaws and Casino Royale from the same country?\",\n",
        "        \"answer\": \"\"\"\n",
        "Are follow up questions needed here: Yes.\n",
        "Follow up: Who is the director of Jaws?\n",
        "Intermediate Answer: The director of Jaws is Steven Spielberg.\n",
        "Follow up: Where is Steven Spielberg from?\n",
        "Intermediate Answer: The United States.\n",
        "Follow up: Who is the director of Casino Royale?\n",
        "Intermediate Answer: The director of Casino Royale is Martin Campbell.\n",
        "Follow up: Where is Martin Campbell from?\n",
        "Intermediate Answer: New Zealand.\n",
        "So the final answer is: No\n",
        "\"\"\",\n",
        "    },\n",
        "    {\n",
        "    \"question\": \"Who won more Grammy Awards, Beyoncé or Michael Jackson?\",\n",
        "    \"answer\": \"\"\"\n",
        "Are follow up questions needed here: Yes.\n",
        "Follow up: How many Grammy Awards has Beyoncé won?\n",
        "Intermediate answer: Beyoncé has won 32 Grammy Awards.\n",
        "Follow up: How many Grammy Awards did Michael Jackson win?\n",
        "Intermediate answer: Michael Jackson won 13 Grammy Awards.\n",
        "So the final answer is: Beyoncé\n",
        "\"\"\",\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2232a1ba",
      "metadata": {
        "id": "2232a1ba"
      },
      "source": [
        "Example 데이터를 구성할 템플릿을 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c3a7c48",
      "metadata": {
        "id": "5c3a7c48"
      },
      "outputs": [],
      "source": [
        "example_prompt = PromptTemplate(template=\"Question: {question}\\n{answer}\")\n",
        "\n",
        "print(example_prompt.format(**examples[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "581efd62",
      "metadata": {
        "id": "581efd62"
      },
      "source": [
        "위에서 만든 Examples와 템플릿, prefix와 suffix를 이용해 전체 템플릿을 만들 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d36e0c5",
      "metadata": {
        "id": "5d36e0c5"
      },
      "outputs": [],
      "source": [
        "prompt = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "\n",
        "    prefix=\"질문-답변 형식의 예시가 주어집니다. 같은 방식으로 답변하세요.\",\n",
        "    suffix=\"Question: {input}\",\n",
        "    #prefix, suffix : Optional\n",
        "\n",
        ")\n",
        "print(prompt.format(input=\"\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22fb94a6",
      "metadata": {
        "id": "22fb94a6"
      },
      "outputs": [],
      "source": [
        "question = \"Current Date : 2025. April. What is the age of the director of the movie which won the best international film in Oscar in 2018?\"\n",
        "X = prompt.format(input=question)\n",
        "print(llm.invoke(X).content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4eb7dc9",
      "metadata": {
        "id": "e4eb7dc9"
      },
      "source": [
        "# LangChain으로 이미지 입력하기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sjk6prVRjdYs",
      "metadata": {
        "id": "sjk6prVRjdYs"
      },
      "source": [
        "이미지와 같은 멀티모달 입력의 경우, 텍스트와 구분하여 Dict 형식으로 입력됩니다.   \n",
        "URL을 직접 전달하거나, 파일을 전달하는 경우에 따라 코드가 달라집니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vrTvaL7O14sK",
      "metadata": {
        "id": "vrTvaL7O14sK"
      },
      "outputs": [],
      "source": [
        "image_url = 'https://images.pexels.com/photos/1851164/pexels-photo-1851164.jpeg'\n",
        "from IPython.display import Image\n",
        "import requests\n",
        "\n",
        "# 이미지 출력\n",
        "img = Image(url = image_url, width = 400)\n",
        "img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c90b32ab",
      "metadata": {
        "id": "c90b32ab"
      },
      "outputs": [],
      "source": [
        "# 1. URL에서 전달하기\n",
        "image_prompt = ChatPromptTemplate([\n",
        "    ('user',[\n",
        "                {\"type\": \"text\", \"text\": \"{question}\"},\n",
        "\n",
        "                {\"type\": \"image_url\",\n",
        "                    \"image_url\": {\"url\": image_url}\n",
        "                }\n",
        "             ]\n",
        "     )])\n",
        "X = image_prompt.format_messages(question= '이 사진에 대해 묘사해 주세요.')\n",
        "print(llm.invoke(X).content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cfff024",
      "metadata": {
        "id": "3cfff024"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "import httpx\n",
        "\n",
        "# 이미지 URL에서 데이터 받아오기\n",
        "image_url = 'https://cloud.google.com/static/vertex-ai/generative-ai/docs/multimodal/images/timetable.png?hl=ko'\n",
        "response = httpx.get(image_url)\n",
        "\n",
        "image_data = base64.b64encode(response.content).decode(\"utf-8\")\n",
        "\n",
        "with open('picture.jpeg', 'wb') as file:\n",
        "    file.write(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TFGbGzZJpdwG",
      "metadata": {
        "id": "TFGbGzZJpdwG"
      },
      "outputs": [],
      "source": [
        "# 2. 로컬 폴더에서 이미지 읽어보기\n",
        "with open('./picture.jpeg', 'rb') as image_file:\n",
        "    image_data = base64.b64encode(image_file.read()).decode('utf-8')\n",
        "\n",
        "\n",
        "image_prompt = ChatPromptTemplate([\n",
        "    ('user',[\n",
        "                {\"type\": \"text\", \"text\": \"{question}\"},\n",
        "\n",
        "                {\"type\": \"image_url\",\n",
        "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"}\n",
        "                }\n",
        "             ]\n",
        "     )])\n",
        "\n",
        "X = image_prompt.format_messages(question='이 그림에 대해 한국어로 설명해 주세요.')\n",
        "\n",
        "print(llm.invoke(X).content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ff219a8",
      "metadata": {
        "id": "0ff219a8"
      },
      "source": [
        "멀티모달 입력의 경우, 적절한 프롬프트가 더 중요합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e8abfef",
      "metadata": {
        "id": "0e8abfef"
      },
      "outputs": [],
      "source": [
        "X = image_prompt.format_messages(question=\"\"\"\n",
        "이 이미지에 표시된 공항 보드에서\n",
        "시간과 도시를 분석해서 목록으로 표시해 주세요.\n",
        "형식은 시간 - 도시입니다.\n",
        "예시) 12:00 - 런던\n",
        "13:00 - 서울\n",
        "\n",
        "목록만 출력하세요.\"\"\")\n",
        "\n",
        "print(llm.invoke(X).content)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

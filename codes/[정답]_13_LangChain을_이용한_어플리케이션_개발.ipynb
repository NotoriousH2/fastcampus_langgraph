{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a435919b",
      "metadata": {
        "id": "a435919b"
      },
      "source": [
        "# [실습] LangChain을 이용한 어플리케이션 개발   \n",
        "\n",
        "LCEL 구조를 이용하여, 간단한 텍스트를 요약하는 어플리케이션을 만들어 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf68e6c0",
      "metadata": {
        "id": "bf68e6c0",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2a436d25-f396-4550-ced2-ddba3e5174da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-genai in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.23)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.21-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting langchain_google_genai\n",
            "  Downloading langchain_google_genai-2.1.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (4.9.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.38.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.11.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.32.3)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (15.0.1)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (4.13.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.51)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.24)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain_google_genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.16 (from langchain_google_genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.17-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.3.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (2.24.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (5.29.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (4.9)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.14.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (24.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.4.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.3.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (1.69.2)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (1.71.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (3.0.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai) (0.6.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_community-0.3.21-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_google_genai-2.1.2-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.6.17-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: filetype, python-dotenv, pymupdf, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, google-ai-generativelanguage, langchain_google_genai, langchain-community\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.4 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.17 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 filetype-1.2.0 google-ai-generativelanguage-0.6.17 httpx-sse-0.4.0 langchain-community-0.3.21 langchain_google_genai-2.1.2 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.8.1 pymupdf-1.25.5 python-dotenv-1.1.0 typing-inspect-0.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "4eeb163faf974418b2c69d6a3b273f57"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install google-genai langchain langchain-community langchain_google_genai pymupdf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "A59cI3UHz9l2",
      "metadata": {
        "id": "A59cI3UHz9l2"
      },
      "source": [
        "## Gemini API 준비하기\n",
        "\n",
        "\n",
        "Google API 키를 등록하고 입력합니다.   \n",
        "구글 계정 로그인 후 https://aistudio.google.com  에 접속하면, API 키 생성이 가능합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "955e251a",
      "metadata": {
        "id": "955e251a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['GOOGLE_API_KEY'] = 'AIzaSyBbKlO_udgEoOLhdVD5ekl5Edbw0WpqunQ'\n",
        "\n",
        "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Gemini API는 분당 10개 요청으로 제한\n",
        "# 즉, 초당 약 0.167개 요청 (10/60)\n",
        "rate_limiter = InMemoryRateLimiter(\n",
        "    requests_per_second=0.167,  # 분당 10개 요청\n",
        "    check_every_n_seconds=0.1,  # 100ms마다 체크\n",
        "    max_bucket_size=10,  # 최대 버스트 크기\n",
        ")\n",
        "\n",
        "# rate limiter를 LLM에 적용\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    rate_limiter=rate_limiter,\n",
        "    temperature = 0.5,\n",
        "    max_tokens = 2048\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd54abc8",
      "metadata": {
        "id": "dd54abc8"
      },
      "source": [
        "PDF 파일 준비하기   \n",
        "임의의 PDF 파일을 다운로드하여 준비합니다.   \n",
        "(예시 PDF 파일을 사용하실 분은 첨부된 PDF 파일을 활용해 주세요!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14579bb5",
      "metadata": {
        "id": "14579bb5"
      },
      "outputs": [],
      "source": [
        "path_material = 'example.pdf'\n",
        "# 자유롭게 경로 변경해서 실행하셔도 됩니다!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0948d1b",
      "metadata": {
        "id": "c0948d1b"
      },
      "source": [
        "랭체인에서는 데이터를 `Document` 클래스로 처리합니다.   \n",
        "데이터의 형식에 따라 적절한 document_loader를 불러와서 사용할 수 있습니다.   \n",
        "\n",
        "이번 실습에서는 PDF를 불러오는 가장 간단한 로더인 PyMuPDFLoader를 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c18c40c",
      "metadata": {
        "id": "3c18c40c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "058efa3e-d22b-4d59-aa1f-82dab5c416bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Number of Pages: 26\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "\n",
        "loader = PyMuPDFLoader(path_material)\n",
        "# 페이지별로 저장\n",
        "pages = loader.load()\n",
        "print(\"# Number of Pages:\", len(pages))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41633081",
      "metadata": {
        "id": "41633081",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b36818e-a3f4-40a3-dc39-121133797fcd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-11T18:57:41+00:00', 'source': 'example.pdf', 'file_path': 'example.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-03-11T18:57:41+00:00', 'trapped': '', 'modDate': 'D:20250311185741Z', 'creationDate': 'D:20250311185741Z', 'page': 10}, page_content='Gemma 3 Technical Report\\nof chemical knowledge, we employed a closed-\\nended knowledge-based approach on chemical\\nhazards developed by Macknight et al. Our eval-\\nuation suggests that the knowledge of Gemma 3\\nmodels in these domains is low.\\n7.4. Our approach to responsible open models\\nDesigning safe, secure, and responsible applica-\\ntions requires a system-level approach, working\\nto mitigate risks associated with each specific use\\ncase and environment. We will continue to adopt\\nassessments and safety mitigations proportion-\\nate to the potential risks from our models, and\\nwill only share these with the community when\\nwe are confident that the benefits significantly\\noutweigh the foreseeable risks.\\n8. Discussion and Conclusion\\nIn this work, we have presented Gemma 3, the\\nlatest addition to the Gemma family of open lan-\\nguage models for text, image, and code. In this\\nversion, we focus on adding image understanding\\nand long context while improving multilinguality\\nand STEM-related abilities. Our model sizes and\\narchitectures are designed to be compatible with\\nstandard hardware, and most of our architecture\\nimprovements are tailored to fit this hardware\\nwhile maintaining performance.\\nReferences\\nRealworldqa.\\nhttps://x.ai/news/grok-1.\\n5v.\\nM. Acharya, K. Kafle, and C. Kanan. Tallyqa: An-\\nswering complex counting questions. In AAAI,\\n2018.\\nR. Agarwal, N. Vieillard, Y. Zhou, P. Stanczyk, S. R.\\nGarea, M. Geist, and O. Bachem. On-policy\\ndistillation of language models: Learning from\\nself-generated mistakes. In ICLR, 2024.\\nJ. Ainslie, J. Lee-Thorp, M. de Jong, Y. Zemlyan-\\nskiy, F. Lebrón, and S. Sanghai. Gqa: Training\\ngeneralized multi-query transformer models\\nfrom multi-head checkpoints. arXiv preprint\\narXiv:2305.13245, 2023.\\nR. Anil, G. Pereyra, A. Passos, R. Ormandi, G. E.\\nDahl, and G. E. Hinton. Large scale distributed\\nneural network training through online distil-\\nlation. arXiv preprint arXiv:1804.03235, 2018.\\nR. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lep-\\nikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey,\\nZ. Chen, et al. Palm 2 technical report. arXiv\\npreprint arXiv:2305.10403, 2023.\\nM. Artetxe, S. Ruder, and D. Yogatama. On the\\ncross-lingual transferability of monolingual rep-\\nresentations. In ACL, 2020.\\nA. Asai, J. Kasai, J. H. Clark, K. Lee, E. Choi,\\nand H. Hajishirzi. Xor qa: Cross-lingual open-\\nretrieval question answering. arXiv preprint\\narXiv:2010.11856, 2020.\\nJ. Austin, A. Odena, M. I. Nye, M. Bosma,\\nH. Michalewski, D. Dohan, E. Jiang, C. J. Cai,\\nM. Terry, Q. V. Le, and C. Sutton. Program\\nsynthesis with large language models. CoRR,\\nabs/2108.07732, 2021.\\nP. Barham, A. Chowdhery, J. Dean, S. Ghemawat,\\nS. Hand, D. Hurt, M. Isard, H. Lim, R. Pang,\\nS. Roy, B. Saeta, P. Schuh, R. Sepassi, L. E.\\nShafey, C. A. Thekkath, and Y. Wu.\\nPath-\\nways: Asynchronous distributed dataflow for\\nml, 2022.\\nI. Beltagy, M. E. Peters, and A. Cohan. Long-\\nformer: The long-document transformer. arXiv\\npreprint arXiv:2004.05150, 2020.\\nS.\\nBiderman,\\nU.\\nPrashanth,\\nL.\\nSutawika,\\nH. Schoelkopf, Q. Anthony, S. Purohit, and\\nE. Raff. Emergent and predictable memoriza-\\ntion in large language models. NeurIPS, 36:\\n28072–28090, 2023.\\nY. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi.\\nPIQA: reasoning about physical commonsense\\nin natural language. CoRR, abs/1911.11641,\\n2019.\\nN. Carlini, F. Tramer, E. Wallace, M. Jagielski,\\nA. Herbert-Voss, K. Lee, A. Roberts, T. Brown,\\nD. Song, U. Erlingsson, et al. Extracting train-\\ning data from large language models.\\nIn\\nUSENIX, 2021.\\n11')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "pages[10]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08e8c187",
      "metadata": {
        "id": "08e8c187"
      },
      "source": [
        "PDF 파일은 페이지별 Document를 저장합니다.   \n",
        "요약을 수행하기 위해, 전체 텍스트를 하나의 Document에 합칩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb0f968d",
      "metadata": {
        "id": "bb0f968d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5e993fd-a43f-498c-d44b-64ae09b0312d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "70114"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "from langchain_core.documents import Document\n",
        "# Document 클래스 만들기\n",
        "\n",
        "corpus = Document(page_content='')\n",
        "for page in pages:\n",
        "    corpus.page_content += page.page_content + '\\n'\n",
        "\n",
        "corpus.page_content = corpus.page_content.replace('\\n\\n','\\n')\n",
        "len(corpus.page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6721b57a",
      "metadata": {
        "id": "6721b57a"
      },
      "source": [
        "LLM에 처리하기 전, 토큰 수를 체크합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "787f4fb5",
      "metadata": {
        "id": "787f4fb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26614765-7e80-4a4d-878b-44d7a95ab00d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt tokens: 25917\n"
          ]
        }
      ],
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "client = genai.Client()\n",
        "\n",
        "\n",
        "response = client.models.count_tokens(\n",
        "    model='gemini-2.0-flash',\n",
        "    contents=corpus.page_content\n",
        ")\n",
        "\n",
        "print(\"Prompt tokens:\",response.total_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7be7d52",
      "metadata": {
        "id": "c7be7d52"
      },
      "source": [
        "요약 체인을 만들고 구성합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee52b512",
      "metadata": {
        "id": "ee52b512"
      },
      "source": [
        "## [실습] 요약 체인 만들기\n",
        "\n",
        "`corpus.page_content`를 입력으로 받는 요약 체인을 만들고 실행하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "622b8bc1",
      "metadata": {
        "id": "622b8bc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "outputId": "c5ff70e2-67ca-404b-c255-0d29c0e07549"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Gemma 3는 Google DeepMind에서 개발한 경량 오픈 모델인 Gemma 제품군의 최신 버전으로, 10억에서 270억 개의 파라미터를 갖는 다양한 크기로 제공됩니다. 이번 버전은 시각적 이해 능력, 확장된 언어 지원, 최소 128K 토큰의 긴 컨텍스트를 특징으로 합니다. 특히 긴 컨텍스트에서 KV-cache 메모리 사용량이 급증하는 문제를 해결하기 위해 모델 아키텍처를 변경했습니다. 이는 로컬 어텐션 레이어와 글로벌 어텐션 레이어의 비율을 높이고, 로컬 어텐션의 범위를 짧게 유지함으로써 달성되었습니다.\\n\\nGemma 3 모델은 지식 증류를 통해 학습되었으며, 사전 학습 및 명령어 미세 조정 버전 모두에서 Gemma 2보다 우수한 성능을 보입니다. 특히 새로운 사후 학습 레시피는 수학, 채팅, 명령어 추종, 다국어 능력을 크게 향상시켜 Gemma3-4B-IT가 Gemma2-27B-IT와 경쟁하고 Gemma3-27B-IT가 Gemini-1.5-Pro와 벤치마크 전반에서 비교될 수 있도록 합니다. Gemma 3 모델은 이전 버전과 유사한 디코더 전용 트랜스포머 아키텍처를 따르며, Grouped-Query Attention (GQA), post-norm, pre-norm, RMSNorm 등의 요소들을 사용합니다. Gemma 2에서 사용되었던 soft-capping은 QK-norm으로 대체되었습니다.\\n\\nGemma 3의 주요 아키텍처 개선 사항 중 하나는 로컬 슬라이딩 윈도우 셀프 어텐션과 글로벌 셀프 어텐션을 5:1 비율로 번갈아 사용하는 것입니다. 이는 각 글로벌 레이어 사이에 여러 개의 로컬 레이어를 배치하고, 로컬 레이어의 범위를 1024 토큰으로 제한하여 KV 캐시의 메모리 폭발 문제를 완화합니다. 따라서 글로벌 레이어만 긴 컨텍스트에 집중하며, 5개의 로컬 레이어마다 1개의 글로벌 레이어가 존재합니다. 또한, Gemma 3 모델은 1B 모델을 제외하고 128K 토큰의 컨텍스트 길이를 지원합니다.\\n\\nGemma 3 모델은 SigLIP 비전 인코더의 맞춤 버전을 사용하여 멀티모달 기능을 제공합니다. 언어 모델은 이미지를 SigLIP에 의해 인코딩된 소프트 토큰 시퀀스로 처리하며, 비전 임베딩을 256개의 고정 크기 벡터로 압축하여 이미지 처리의 추론 비용을 줄입니다. 또한, LLaVA에서 영감을 받아 Pan and Scan (P&S) 방법을 사용하여 다양한 해상도를 지원합니다. 비전 인코더는 896 x 896으로 리사이즈된 이미지를 입력으로 사용하며, 시각적 어시스턴트 작업 데이터로 미세 조정됩니다.\\n\\nGemma 3 모델은 지식 증류를 사용하여 사전 학습되며, Gemini 2.0과 동일한 토크나이저를 사용합니다. 또한, 모델의 다국어 기능을 개선하고 이미지 이해 능력을 도입하기 위해 데이터 혼합을 재검토했습니다. 사후 학습에서는 수학, 추론, 채팅 능력 향상과 더불어 긴 컨텍스트 및 이미지 입력과 같은 새로운 기능을 통합하는 데 중점을 둡니다. 결과적으로 Gemma 3 명령어 미세 조정 모델은 강력하고 다재다능하며, 이전 모델보다 훨씬 뛰어난 성능을 제공합니다.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "summary_prompt = ChatPromptTemplate(\n",
        "    [('user', '''\n",
        "당신은 LLM/AI 전문가입니다.\n",
        "LLM에 대한 최신 모델의 논문이 주어집니다.\n",
        "전문 개발자를 대상으로 이를 설명한다고 가정하고,\n",
        "모델의 구조에 집중해서 논문의 내용을 요약하세요.\n",
        "\n",
        "전체는 5문단으로 하고, 문단별 4~8개 문장으로 구성하세요.\n",
        "---\n",
        "{text}''')\n",
        "\n",
        "    ]\n",
        "\n",
        ")\n",
        "summary_chain = summary_prompt | llm | StrOutputParser()\n",
        "\n",
        "summary = summary_chain.invoke(corpus.page_content)\n",
        "summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e499d89",
      "metadata": {
        "id": "4e499d89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c32969ee-37aa-401e-e660-45119675d500"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemma 3는 Google DeepMind에서 개발한 경량 오픈 모델인 Gemma 제품군의 최신 버전으로, 10억에서 270억 개의 파라미터를 갖는 다양한 크기로 제공됩니다. 이번 버전은 시각적 이해 능력, 확장된 언어 지원, 최소 128K 토큰의 긴 컨텍스트를 특징으로 합니다. 특히 긴 컨텍스트에서 KV-cache 메모리 사용량이 급증하는 문제를 해결하기 위해 모델 아키텍처를 변경했습니다. 이는 로컬 어텐션 레이어와 글로벌 어텐션 레이어의 비율을 높이고, 로컬 어텐션의 범위를 짧게 유지함으로써 달성되었습니다.\n",
            "\n",
            "Gemma 3 모델은 지식 증류를 통해 학습되었으며, 사전 학습 및 명령어 미세 조정 버전 모두에서 Gemma 2보다 우수한 성능을 보입니다. 특히 새로운 사후 학습 레시피는 수학, 채팅, 명령어 추종, 다국어 능력을 크게 향상시켜 Gemma3-4B-IT가 Gemma2-27B-IT와 경쟁하고 Gemma3-27B-IT가 Gemini-1.5-Pro와 벤치마크 전반에서 비교될 수 있도록 합니다. Gemma 3 모델은 이전 버전과 유사한 디코더 전용 트랜스포머 아키텍처를 따르며, Grouped-Query Attention (GQA), post-norm, pre-norm, RMSNorm 등의 요소들을 사용합니다. Gemma 2에서 사용되었던 soft-capping은 QK-norm으로 대체되었습니다.\n",
            "\n",
            "Gemma 3의 주요 아키텍처 개선 사항 중 하나는 로컬 슬라이딩 윈도우 셀프 어텐션과 글로벌 셀프 어텐션을 5:1 비율로 번갈아 사용하는 것입니다. 이는 각 글로벌 레이어 사이에 여러 개의 로컬 레이어를 배치하고, 로컬 레이어의 범위를 1024 토큰으로 제한하여 KV 캐시의 메모리 폭발 문제를 완화합니다. 따라서 글로벌 레이어만 긴 컨텍스트에 집중하며, 5개의 로컬 레이어마다 1개의 글로벌 레이어가 존재합니다. 또한, Gemma 3 모델은 1B 모델을 제외하고 128K 토큰의 컨텍스트 길이를 지원합니다.\n",
            "\n",
            "Gemma 3 모델은 SigLIP 비전 인코더의 맞춤 버전을 사용하여 멀티모달 기능을 제공합니다. 언어 모델은 이미지를 SigLIP에 의해 인코딩된 소프트 토큰 시퀀스로 처리하며, 비전 임베딩을 256개의 고정 크기 벡터로 압축하여 이미지 처리의 추론 비용을 줄입니다. 또한, LLaVA에서 영감을 받아 Pan and Scan (P&S) 방법을 사용하여 다양한 해상도를 지원합니다. 비전 인코더는 896 x 896으로 리사이즈된 이미지를 입력으로 사용하며, 시각적 어시스턴트 작업 데이터로 미세 조정됩니다.\n",
            "\n",
            "Gemma 3 모델은 지식 증류를 사용하여 사전 학습되며, Gemini 2.0과 동일한 토크나이저를 사용합니다. 또한, 모델의 다국어 기능을 개선하고 이미지 이해 능력을 도입하기 위해 데이터 혼합을 재검토했습니다. 사후 학습에서는 수학, 추론, 채팅 능력 향상과 더불어 긴 컨텍스트 및 이미지 입력과 같은 새로운 기능을 통합하는 데 중점을 둡니다. 결과적으로 Gemma 3 명령어 미세 조정 모델은 강력하고 다재다능하며, 이전 모델보다 훨씬 뛰어난 성능을 제공합니다.\n"
          ]
        }
      ],
      "source": [
        "print(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05e7e658",
      "metadata": {
        "id": "05e7e658"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "5ee49e0e",
      "metadata": {
        "id": "5ee49e0e"
      },
      "source": [
        "Gemini 2.0 Flash의 Context Window는 1M이므로 전체를 모두 하나의 컨텍스트로 입력해도 되지만,   \n",
        "Context가 짧은 모델들의 경우, 전체를 분할하여 요약 작업을 수행할 수 있습니다.\n",
        "\n",
        "**Map-Reduce** 방식의 요약을 만들어 보겠습니다.   \n",
        "\n",
        "Map-Reduce는 텍스트를 청크로 분할하고, 청크별 요약을 생성한 뒤  \n",
        "전체 요약문을 합쳐 프롬프트로 넣고 최종 요약문을 생성하는 방식입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0740625",
      "metadata": {
        "id": "c0740625"
      },
      "source": [
        "문서를 청크로 나누기 위해, RecursiveCharacterTextSplitter를 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8031c4c",
      "metadata": {
        "id": "f8031c4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dbbeb8d-19bd-4cb9-d520-ad5f5dad21c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=20000,\n",
        "    chunk_overlap=4000,\n",
        ")\n",
        "\n",
        "chunks = text_splitter.split_documents([corpus])\n",
        "print(len(chunks))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac546a56",
      "metadata": {
        "id": "ac546a56"
      },
      "source": [
        "Map: 청크별 요약을 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93b6596c",
      "metadata": {
        "id": "93b6596c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ac2631b-e313-4446-d7fe-2bd1a830d781"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 1/5 [00:07<00:30,  7.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 0\n",
            "다음은 Gemma 3 기술 보고서의 요약입니다.\n",
            "\n",
            "Gemma 3는 경량 오픈 모델 제품군인 Gemma의 멀티모달 확장판으로, 10억에서 270억 개의 파라미터 규모로 제공됩니다. 이 버전은 시각 이해 능력, 더 넓은 언어 지원, 최소 128K 토큰의 더 긴 컨텍스트를 제공합니다. 또한 긴 컨텍스트에서 폭발적으로 증가하는 KV-캐시 메모리를 줄이기 위해 모델 아키텍처를 변경했습니다. 이는 로컬 및 글로벌 어텐션 레이어의 비율을 늘리고 로컬 어텐션의 범위를 짧게 유지함으로써 달성됩니다. Gemma 3 모델은 지식 증류를 통해 훈련되었으며 사전 훈련 및 명령어 미세 조정 버전 모두에서 Gemma 2보다 뛰어난 성능을 보입니다. 특히, 새로운 사후 훈련 레시피는 수학, 채팅, 명령어 추종, 다국어 능력을 크게 향상시켜 Gemma3-4B-IT가 Gemma2-27B-IT와 경쟁하고 Gemma3-27B-IT가 벤치마크에서 Gemini-1.5-Pro와 비교될 수 있도록 합니다. 모든 모델은 커뮤니티에 공개됩니다.\n",
            "\n",
            "Gemma 3 모델은 이전 버전과 동일한 일반적인 디코더 전용 트랜스포머 아키텍처를 따르며, 대부분의 아키텍처 요소는 처음 두 Gemma 버전과 유사합니다. GQA(Grouped-Query Attention)를 사용하고, RMSNorm과 함께 post-norm 및 pre-norm을 사용합니다. Gemma 2의 소프트 캡핑을 QK-norm으로 대체합니다. 핵심 차이점은 로컬 슬라이딩 윈도우 셀프 어텐션과 글로벌 셀프 어텐션을 5:1 패턴으로 번갈아 사용한다는 것입니다. Gemma 3 모델은 1B 모델의 32K를 제외하고 128K 토큰의 컨텍스트 길이를 지원합니다. 글로벌 셀프 어텐션 레이어에서 RoPE 기본 주파수를 10k에서 1M으로 늘리고 로컬 레이어의 주파수를 10k로 유지합니다. 글로벌 셀프 어텐션 레이어의 범위를 확장하기 위해 위치 보간과 유사한 프로세스를 따릅니다.\n",
            "\n",
            "비전 인코더로 SigLIP 인코더의 4억 파라미터 버전을 사용합니다. Gemma 비전 인코더는 896 x 896으로 크기가 조정된 정사각형 이미지를 입력으로 사용하고 시각 보조 작업의 데이터로 미세 조정됩니다. 단순성을 위해 4B, 12B, 27B 모델에서 비전 인코더를 공유하고 훈련 중에 고정합니다. Gemma 비전 인코더는 896 × 896의 고정 해상도에서 작동합니다. 이로 인해 정사각형이 아닌 종횡비와 고해상도 이미지를 처리할 때 텍스트가 읽을 수 없거나 작은 객체가 사라지는 등의 문제가 발생합니다. 이러한 문제를 해결하기 위해 추론 중에 적응형 윈도우 알고리즘을 사용합니다. 이 알고리즘은 이미지를 동일한 크기의 겹치지 않는 조각으로 분할하여 전체 이미지를 커버하고, 인코더에 전달하기 위해 896×896 픽셀로 크기를 조정합니다. 이 윈도우는 필요한 경우에만 적용되며 최대 조각 수를 제어합니다.\n",
            "\n",
            "사전 훈련은 Gemma 2와 유사한 레시피를 따르며 지식 증류를 사용합니다. 모델은 Gemma 2보다 약간 더 큰 토큰 예산으로 사전 훈련됩니다. 토큰 증가는 사전 훈련 중에 사용되는 이미지와 텍스트의 혼합을 고려합니다. 다국어 데이터의 양을 늘려 언어 지원을 개선합니다. 단일 언어 및 병렬 데이터를 모두 추가하고 언어 표현의 불균형을 처리하기 위해 Chung et al.(2023)에서 영감을 얻은 전략을 사용합니다. Gemini 2.0과 동일한 토크나이저를 사용합니다. 필터링 기술을 사용하여 원치 않거나 안전하지 않은 발언의 위험을 줄이고 특정 개인 정보 및 기타 민감한 데이터를 제거합니다. 평가 세트가 사전 훈련 데이터 혼합물에서 오염되지 않도록 하고 민감한 출력의 확산을 최소화하여 암송 위험을 줄입니다. 또한 데이터 품질이 낮은 발생을 줄이기 위해 Sachdeva et al.(2024)에서 영감을 얻은 품질 재가중 단계를 적용합니다.\n",
            "\n",
            "사전 훈련된 모델은 이전 레시피에 비해 향상된 사후 훈련 접근 방식으로 명령어 미세 조정 모델로 전환됩니다. 사후 훈련 접근 방식은 대규모 IT 교사로부터의 지식 증류와 BOND, WARM, WARP의 향상된 버전을 기반으로 한 RL 미세 조정 단계에 의존합니다. 유용성, 수학, 코딩, 추론, 명령어 추종, 다국어 능력을 향상시키면서 모델 유해성을 최소화하기 위해 다양한 보상 함수를 사용합니다. 여기에는 인간 피드백 데이터, 코드 실행 피드백, 수학 문제 해결을 위한 지상 실체 보상으로 훈련된 가중 평균 보상 모델에서 학습하는 것이 포함됩니다. 사후 훈련에 사용되는 데이터를 신중하게 최적화하여 모델 성능을 극대화합니다. 특정 개인 정보, 안전하지 않거나 유해한 모델 출력, 잘못된 자체 식별 데이터, 중복된 예제를 필터링합니다. 더 나은 컨텍스트 내 속성, 헤징, 환각을 최소화하기 위한 거부를 장려하는 데이터 하위 집합을 포함하면 다른 메트릭에서 모델 성능을 저하시키지 않고 사실성 메트릭에서 성능이 향상됩니다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 2/5 [00:11<00:16,  5.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 1\n",
            "Gemma 3은 텍스트, 이미지, 코드 이해를 위한 최신 오픈 언어 모델로, 이미지 이해, 긴 문맥 처리, 다국어 및 STEM 관련 능력 향상에 중점을 둡니다. 모델 크기와 아키텍처는 표준 하드웨어와의 호환성을 고려하여 설계되었으며, 대부분의 아키텍처 개선 사항은 성능을 유지하면서 이 하드웨어에 맞게 조정되었습니다. Gemma 3 모델은 이전 모델보다 훨씬 낮은 비율로 텍스트를 암기하며, 개인 정보가 포함된 데이터의 비율도 매우 낮습니다.\n",
            "\n",
            "Gemma 3 모델은 로컬 및 글로벌 self-attention 레이어의 비율을 5:1로 조정하여 Gemma 2 모델의 1:1 비율에서 변경되었으며, 이는 복잡도에 미치는 영향이 미미합니다. Sliding window 크기를 줄여도 복잡도에 큰 영향이 없으며, 이는 메모리 사용량을 최적화하는 데 도움이 됩니다. KV 캐시 메모리 사용량을 줄이기 위해 \"global only\" 구성 대신 1:3 비율과 sliding window 크기를 1024로 설정하여 메모리 오버헤드를 줄입니다.\n",
            "\n",
            "긴 문맥 처리를 위해 모델은 32K 시퀀스로 사전 학습된 후 4B, 12B, 27B 모델을 128K 토큰으로 확장하고 RoPE를 재조정합니다. 글로벌 self-attention 레이어의 RoPE 기본 주파수를 10k에서 1M으로 늘리고 로컬 self-attention 레이어는 10k를 유지합니다. 모델은 128K까지 일반화되지만, 그 이상으로 확장하면 성능이 저하됩니다.\n",
            "\n",
            "작은 모델을 훈련할 때 작은 교사 모델을 사용하는 것이 좋다는 일반적인 발견과는 달리, Gemma 3은 더 큰 교사 모델을 사용하여 더 나은 성능을 달성합니다. 이미지 해상도와 관련하여 SigLIP 기반의 비전 인코더를 사용하며, 더 높은 해상도의 인코더가 더 나은 성능을 보입니다. Pan & Scan (P&S)을 통해 이미지의 원래 종횡비와 해상도를 유지하여 이미지의 텍스트를 읽어야 하는 작업에서 성능이 향상됩니다.\n",
            "\n",
            "Gemma 3 모델의 안전성을 위해 훈련 데이터 필터링, SFT 및 RLHF를 사용하여 유해한 콘텐츠 생성을 방지합니다. 내부 안전 프로세스를 통해 모델이 Google의 안전 정책을 준수하도록 설계되었으며, 극단적인 위험과 관련된 기능에 대한 평가도 수행됩니다. 화학, 생물학, 방사선 및 핵 (CBRN) 관련 지식 평가 결과, Gemma 3 모델의 해당 분야 지식은 낮은 것으로 나타났습니다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 3/5 [00:14<00:09,  4.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 2\n",
            "다음은 Gemma 3 기술 보고서에 대한 전문 개발자를 위한 요약입니다.\n",
            "\n",
            "Gemma 3은 텍스트, 이미지 및 코드에 대한 최신 오픈 언어 모델 제품군입니다. 이 모델은 이미지 이해, 긴 컨텍스트 처리 능력, 다국어 지원 및 STEM 관련 능력을 향상시키는 데 중점을 둡니다. 모델 크기와 아키텍처는 표준 하드웨어와의 호환성을 고려하여 설계되었으며, 아키텍처 개선 사항은 성능을 유지하면서도 하드웨어에 최적화되도록 조정되었습니다. Gemma 3은 이전 모델의 기능을 확장하고 다양한 작업에서 향상된 성능을 제공하는 것을 목표로 합니다.\n",
            "\n",
            "안전성은 Gemma 3 개발에서 중요한 고려 사항입니다. 개발 속도와 안전성 테스트 간의 균형을 유지하면서 표적화되고 효율적인 평가를 수행합니다. 모델의 잠재적 위험을 평가하기 위해 기본 평가를 수행하여 안전 정책 위반율을 측정합니다. 합성 적대적 사용자 쿼리를 사용하여 모델의 응답을 정책 위반 여부에 따라 평가합니다. Gemma 3은 이러한 안전 정책에서 전반적으로 낮은 위반율을 보입니다.\n",
            "\n",
            "Gemma 3은 STEM 관련 작업에서 향상된 성능을 보이므로 화학, 생물학, 방사선 및 핵(CBRN) 관련 지식에 대한 평가도 수행되었습니다. 내부 데이터 세트를 사용하여 CBRN 관련 지식 기반 다중 선택 질문에 대한 모델의 답변을 평가합니다. 화학 지식 평가는 Macknight 등이 개발한 화학적 위험에 대한 폐쇄형 지식 기반 접근 방식을 사용했습니다. 평가 결과 Gemma 3 모델의 CBRN 관련 지식은 낮은 수준으로 나타났습니다.\n",
            "\n",
            "책임감 있는 오픈 모델 설계를 위해 시스템 수준의 접근 방식을 채택하여 특정 사용 사례 및 환경과 관련된 위험을 완화합니다. 모델의 잠재적 위험에 비례하는 평가 및 안전 완화 조치를 지속적으로 적용합니다. 모델의 이점이 예측 가능한 위험보다 훨씬 클 때만 커뮤니티와 공유합니다. 안전하고 책임감 있는 애플리케이션을 개발하기 위한 다각적인 노력을 기울이고 있습니다.\n",
            "\n",
            "Gemma 3은 텍스트, 이미지 및 코드를 이해하고 생성할 수 있는 강력한 오픈 언어 모델입니다. 이 모델은 다양한 작업에서 향상된 성능을 제공하며, 표준 하드웨어와의 호환성을 고려하여 설계되었습니다. 안전성은 개발 과정에서 중요한 고려 사항이며, 잠재적 위험을 평가하고 완화하기 위한 다양한 조치를 취하고 있습니다. Gemma 3은 오픈 소스 커뮤니티에 기여하고 AI 기술 발전에 기여할 것으로 기대됩니다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 4/5 [00:19<00:04,  4.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 3\n",
            "이 문서는 Google에서 개발한 Gemma 3 모델에 대한 기술 보고서입니다. 보고서는 Gemma 3 모델의 아키텍처, 훈련 방법 및 다양한 벤치마크에서의 성능을 자세히 설명합니다. Gemma 3는 이전 버전인 Gemma 2를 기반으로 구축되었으며, 사실적 지식, 상식 추론, STEM 능력 및 다국어 이해도를 포함한 다양한 영역에서 향상된 성능을 제공합니다. 이 보고서는 모델의 기능과 한계를 이해하는 데 관심 있는 개발자와 연구자에게 귀중한 리소스를 제공합니다.\n",
            "\n",
            "Gemma 3 모델은 10억 개에서 270억 개에 이르는 다양한 매개변수 크기로 제공되며, 다양한 계산 요구 사항과 사용 사례를 충족합니다. 이 모델은 대규모 텍스트 및 코드 데이터 세트에 대해 사전 훈련되었으며, 특정 작업에 맞게 미세 조정할 수 있습니다. 보고서는 또한 모델의 다중 모드 버전을 탐구하여 이미지 이해 능력을 확장합니다. 아키텍처에는 주의 메커니즘이 포함되어 있으며, 이는 모델이 입력 시퀀스에서 가장 관련성이 높은 부분에 집중할 수 있도록 합니다. 이 보고서는 모델의 아키텍처, 훈련 방법 및 평가 프로토콜에 대한 자세한 정보를 제공합니다.\n",
            "\n",
            "Gemma 3 모델은 다양한 벤치마크에서 이전 버전에 비해 상당한 개선을 보였습니다. 이 보고서는 사실적 지식, 상식 추론, STEM 능력 및 다국어 이해도를 포함한 다양한 영역에서 모델의 성능을 자세히 설명합니다. 특히 Gemma 3는 MMLU(Massive Multitask Language Understanding) 및 AGIEval(인공 일반 지능 평가)과 같은 벤치마크에서 뛰어난 성능을 보였으며, 이는 고급 추론 및 문제 해결 능력을 입증합니다. 이 보고서는 또한 모델의 장기 컨텍스트 처리 능력을 강조하고, RULER 및 MRCR 벤치마크에서 성능을 평가합니다.\n",
            "\n",
            "이 보고서는 Gemma 3 모델의 다중 모드 기능도 탐구하여 이미지 이해 능력을 확장합니다. 이 모델은 COCO Caption, DocVQA 및 TextVQA와 같은 다양한 시각적 질문 답변 벤치마크에서 평가되었습니다. 결과는 Gemma 3가 이미지와 관련된 질문에 정확하게 답변할 수 있음을 보여주며, 이는 다양한 응용 분야에 유용한 기능입니다. 이 보고서는 Gemma 3가 문서 이해와 관련된 벤치마크에서 뛰어난 성능을 보이며, 더 큰 PaliGemma 2 변형보다 뛰어난 성능을 보인다는 점을 강조합니다.\n",
            "\n",
            "이 보고서는 Gemma 3 모델의 잠재적 위험과 윤리적 고려 사항에 대해 논의합니다. 이 모델은 유해하거나 편향된 콘텐츠를 생성하는 데 사용될 수 있으며, 이는 사회적 위험으로 이어질 수 있습니다. 이 보고서는 이러한 위험을 완화하기 위한 책임감 있는 개발 및 배포의 중요성을 강조합니다. 이 보고서는 또한 모델의 다국어 기능을 탐구하고, MGSM, Global-MMLU-Lite 및 WMT24++와 같은 벤치마크에서 성능을 평가합니다. 결과는 Gemma 3가 다양한 언어에서 효과적으로 수행할 수 있음을 보여주며, 이는 글로벌 응용 분야에 유용합니다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:23<00:00,  4.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 4\n",
            "이 문서는 Gemma 3 모델의 성능을 다양한 벤치마크에서 평가한 결과를 제시합니다. 평가는 사전 훈련된(PT) 모델과 Instruction Fine-Tuned(IT) 모델 모두에 대해 수행되었으며, 모델 크기(4B, 12B, 27B)와 컨텍스트 길이(32K, 128K)에 따른 성능 변화를 분석합니다. 주요 평가 항목은 긴 컨텍스트 처리 능력, 멀티모달 이해 능력, 비디오 이해 능력, 그리고 다양한 내부 및 외부 벤치마크에서의 성능입니다.\n",
            "\n",
            "긴 컨텍스트 벤치마크(RULER, MRCR)에서 Gemma 3 모델은 컨텍스트 길이가 길어짐에 따라 성능이 감소하는 경향을 보이지만, Instruction Fine-Tuning을 통해 성능을 향상시킬 수 있음을 보여줍니다. 특히, 27B IT 모델은 RULER 벤치마크에서 128K 컨텍스트 길이에서 66.0의 정확도를 달성했습니다. 이는 모델이 긴 컨텍스트 정보를 효과적으로 활용할 수 있음을 시사합니다.\n",
            "\n",
            "멀티모달 벤치마크(MMMU, DocVQA, InfoVQA 등)에서 Gemma 3 IT 모델은 이미지와 텍스트를 함께 이해하는 능력을 평가받았습니다. 27B 모델은 대부분의 벤치마크에서 가장 높은 성능을 보였으며, MMMU 벤치마크에서 64.9의 정확도를 달성했습니다. 이는 모델이 다양한 시각적 정보를 효과적으로 처리하고 텍스트와 함께 이해할 수 있음을 나타냅니다.\n",
            "\n",
            "비디오 이해 벤치마크(Perception Test MCVQA, ActivityNet-QA)에서 Gemma 3 IT 모델은 비디오 콘텐츠를 이해하고 질문에 답변하는 능력을 평가받았습니다. 27B 모델은 Perception Test MCVQA 벤치마크에서 58.1의 정확도를 달성했으며, ActivityNet-QA 벤치마크에서 52.8의 점수를 기록했습니다. 이는 모델이 비디오 내의 시각적 정보를 이해하고 관련된 질문에 답변할 수 있음을 보여줍니다.\n",
            "\n",
            "다양한 내부 및 외부 벤치마크(MMLU, MBPP, HumanEval 등)에서 Gemma 3 IT 모델은 다양한 크기(1B, 4B, 12B, 27B)로 평가되었습니다. 27B 모델은 대부분의 벤치마크에서 가장 높은 성능을 보였으며, HumanEval 벤치마크에서 87.8의 정확도를 달성했습니다. 이는 모델이 다양한 유형의 작업을 수행하고 복잡한 문제를 해결할 수 있음을 시사합니다. 또한, N2C(Natural2Code) 벤치마크에서 높은 성능을 보인 것은 코드 생성 능력의 우수성을 나타냅니다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Map 과정 : 각 문서에 대해 요약을 생성합니다.\n",
        "from tqdm import tqdm\n",
        "\n",
        "map_prompt = ChatPromptTemplate([\n",
        "    ('system', '''당신은 LLM/AI 전문가입니다.\n",
        "LLM에 대한 최신 모델의 논문이 주어집니다.\n",
        "전문 개발자를 대상으로 이를 설명한다고 가정하고,\n",
        "기술적인 세부사항에 집중해서 논문의 내용을 요약하세요.\n",
        "요약은 5개의 문단과 문단별 4~8개의 문장으로 작성하세요.\n",
        "'''),\n",
        "    ('user', '''{text}''')])\n",
        "\n",
        "raw_summaries = []\n",
        "\n",
        "map_chain  = map_prompt | llm | StrOutputParser()\n",
        "\n",
        "\n",
        "for i in tqdm(range(len(chunks))):\n",
        "    response = map_chain.invoke(chunks[i].page_content)\n",
        "\n",
        "    raw_summaries.append(response)\n",
        "\n",
        "    print('\\n#',i)\n",
        "    print(response)\n",
        "    print('===========================')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64afc69b",
      "metadata": {
        "id": "64afc69b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68dee3cd-c4e0-4bdd-b203-25de0c928fc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "다음은 Gemma 3 기술 보고서의 요약입니다.\n",
            "\n",
            "Gemma 3는 경량 오픈 모델 제품군인 Gemma의 멀티모달 확장판으로, 10억에서 270억 개의 파라미터 규모로 제공됩니다. 이 버전은 시각 이해 능력, 더 넓은 언어 지원, 최소 128K 토큰의 더 긴 컨텍스트를 제공합니다. 또한 긴 컨텍스트에서 폭발적으로 증가하는 KV-캐시 메모리를 줄이기 위해 모델 아키텍처를 변경했습니다. 이는 로컬 및 글로벌 어텐션 레이어의 비율을 늘리고 로컬 어텐션의 범위를 짧게 유지함으로써 달성됩니다. Gemma 3 모델은 지식 증류를 통해 훈련되었으며 사전 훈련 및 명령어 미세 조정 버전 모두에서 Gemma 2보다 뛰어난 성능을 보입니다. 특히, 새로운 사후 훈련 레시피는 수학, 채팅, 명령어 추종, 다국어 능력을 크게 향상시켜 Gemma3-4B-IT가 Gemma2-27B-IT와 경쟁하고 Gemma3-27B-IT가 벤치마크에서 Gemini-1.5-Pro와 비교될 수 있도록 합니다. 모든 모델은 커뮤니티에 공개됩니다.\n",
            "\n",
            "Gemma 3 모델은 이전 버전과 동일한 일반적인 디코더 전용 트랜스포머 아키텍처를 따르며, 대부분의 아키텍처 요소는 처음 두 Gemma 버전과 유사합니다. GQA(Grouped-Query Attention)를 사용하고, RMSNorm과 함께 post-norm 및 pre-norm을 사용합니다. Gemma 2의 소프트 캡핑을 QK-norm으로 대체합니다. 핵심 차이점은 로컬 슬라이딩 윈도우 셀프 어텐션과 글로벌 셀프 어텐션을 5:1 패턴으로 번갈아 사용한다는 것입니다. Gemma 3 모델은 1B 모델의 32K를 제외하고 128K 토큰의 컨텍스트 길이를 지원합니다. 글로벌 셀프 어텐션 레이어에서 RoPE 기본 주파수를 10k에서 1M으로 늘리고 로컬 레이어의 주파수를 10k로 유지합니다. 글로벌 셀프 어텐션 레이어의 범위를 확장하기 위해 위치 보간과 유사한 프로세스를 따릅니다.\n",
            "\n",
            "비전 인코더로 SigLIP 인코더의 4억 파라미터 버전을 사용합니다. Gemma 비전 인코더는 896 x 896으로 크기가 조정된 정사각형 이미지를 입력으로 사용하고 시각 보조 작업의 데이터로 미세 조정됩니다. 단순성을 위해 4B, 12B, 27B 모델에서 비전 인코더를 공유하고 훈련 중에 고정합니다. Gemma 비전 인코더는 896 × 896의 고정 해상도에서 작동합니다. 이로 인해 정사각형이 아닌 종횡비와 고해상도 이미지를 처리할 때 텍스트가 읽을 수 없거나 작은 객체가 사라지는 등의 문제가 발생합니다. 이러한 문제를 해결하기 위해 추론 중에 적응형 윈도우 알고리즘을 사용합니다. 이 알고리즘은 이미지를 동일한 크기의 겹치지 않는 조각으로 분할하여 전체 이미지를 커버하고, 인코더에 전달하기 위해 896×896 픽셀로 크기를 조정합니다. 이 윈도우는 필요한 경우에만 적용되며 최대 조각 수를 제어합니다.\n",
            "\n",
            "사전 훈련은 Gemma 2와 유사한 레시피를 따르며 지식 증류를 사용합니다. 모델은 Gemma 2보다 약간 더 큰 토큰 예산으로 사전 훈련됩니다. 토큰 증가는 사전 훈련 중에 사용되는 이미지와 텍스트의 혼합을 고려합니다. 다국어 데이터의 양을 늘려 언어 지원을 개선합니다. 단일 언어 및 병렬 데이터를 모두 추가하고 언어 표현의 불균형을 처리하기 위해 Chung et al.(2023)에서 영감을 얻은 전략을 사용합니다. Gemini 2.0과 동일한 토크나이저를 사용합니다. 필터링 기술을 사용하여 원치 않거나 안전하지 않은 발언의 위험을 줄이고 특정 개인 정보 및 기타 민감한 데이터를 제거합니다. 평가 세트가 사전 훈련 데이터 혼합물에서 오염되지 않도록 하고 민감한 출력의 확산을 최소화하여 암송 위험을 줄입니다. 또한 데이터 품질이 낮은 발생을 줄이기 위해 Sachdeva et al.(2024)에서 영감을 얻은 품질 재가중 단계를 적용합니다.\n",
            "\n",
            "사전 훈련된 모델은 이전 레시피에 비해 향상된 사후 훈련 접근 방식으로 명령어 미세 조정 모델로 전환됩니다. 사후 훈련 접근 방식은 대규모 IT 교사로부터의 지식 증류와 BOND, WARM, WARP의 향상된 버전을 기반으로 한 RL 미세 조정 단계에 의존합니다. 유용성, 수학, 코딩, 추론, 명령어 추종, 다국어 능력을 향상시키면서 모델 유해성을 최소화하기 위해 다양한 보상 함수를 사용합니다. 여기에는 인간 피드백 데이터, 코드 실행 피드백, 수학 문제 해결을 위한 지상 실체 보상으로 훈련된 가중 평균 보상 모델에서 학습하는 것이 포함됩니다. 사후 훈련에 사용되는 데이터를 신중하게 최적화하여 모델 성능을 극대화합니다. 특정 개인 정보, 안전하지 않거나 유해한 모델 출력, 잘못된 자체 식별 데이터, 중복된 예제를 필터링합니다. 더 나은 컨텍스트 내 속성, 헤징, 환각을 최소화하기 위한 거부를 장려하는 데이터 하위 집합을 포함하면 다른 메트릭에서 모델 성능을 저하시키지 않고 사실성 메트릭에서 성능이 향상됩니다.\n",
            "---\n",
            "Gemma 3은 텍스트, 이미지, 코드 이해를 위한 최신 오픈 언어 모델로, 이미지 이해, 긴 문맥 처리, 다국어 및 STEM 관련 능력 향상에 중점을 둡니다. 모델 크기와 아키텍처는 표준 하드웨어와의 호환성을 고려하여 설계되었으며, 대부분의 아키텍처 개선 사항은 성능을 유지하면서 이 하드웨어에 맞게 조정되었습니다. Gemma 3 모델은 이전 모델보다 훨씬 낮은 비율로 텍스트를 암기하며, 개인 정보가 포함된 데이터의 비율도 매우 낮습니다.\n",
            "\n",
            "Gemma 3 모델은 로컬 및 글로벌 self-attention 레이어의 비율을 5:1로 조정하여 Gemma 2 모델의 1:1 비율에서 변경되었으며, 이는 복잡도에 미치는 영향이 미미합니다. Sliding window 크기를 줄여도 복잡도에 큰 영향이 없으며, 이는 메모리 사용량을 최적화하는 데 도움이 됩니다. KV 캐시 메모리 사용량을 줄이기 위해 \"global only\" 구성 대신 1:3 비율과 sliding window 크기를 1024로 설정하여 메모리 오버헤드를 줄입니다.\n",
            "\n",
            "긴 문맥 처리를 위해 모델은 32K 시퀀스로 사전 학습된 후 4B, 12B, 27B 모델을 128K 토큰으로 확장하고 RoPE를 재조정합니다. 글로벌 self-attention 레이어의 RoPE 기본 주파수를 10k에서 1M으로 늘리고 로컬 self-attention 레이어는 10k를 유지합니다. 모델은 128K까지 일반화되지만, 그 이상으로 확장하면 성능이 저하됩니다.\n",
            "\n",
            "작은 모델을 훈련할 때 작은 교사 모델을 사용하는 것이 좋다는 일반적인 발견과는 달리, Gemma 3은 더 큰 교사 모델을 사용하여 더 나은 성능을 달성합니다. 이미지 해상도와 관련하여 SigLIP 기반의 비전 인코더를 사용하며, 더 높은 해상도의 인코더가 더 나은 성능을 보입니다. Pan & Scan (P&S)을 통해 이미지의 원래 종횡비와 해상도를 유지하여 이미지의 텍스트를 읽어야 하는 작업에서 성능이 향상됩니다.\n",
            "\n",
            "Gemma 3 모델의 안전성을 위해 훈련 데이터 필터링, SFT 및 RLHF를 사용하여 유해한 콘텐츠 생성을 방지합니다. 내부 안전 프로세스를 통해 모델이 Google의 안전 정책을 준수하도록 설계되었으며, 극단적인 위험과 관련된 기능에 대한 평가도 수행됩니다. 화학, 생물학, 방사선 및 핵 (CBRN) 관련 지식 평가 결과, Gemma 3 모델의 해당 분야 지식은 낮은 것으로 나타났습니다.\n",
            "---\n",
            "다음은 Gemma 3 기술 보고서에 대한 전문 개발자를 위한 요약입니다.\n",
            "\n",
            "Gemma 3은 텍스트, 이미지 및 코드에 대한 최신 오픈 언어 모델 제품군입니다. 이 모델은 이미지 이해, 긴 컨텍스트 처리 능력, 다국어 지원 및 STEM 관련 능력을 향상시키는 데 중점을 둡니다. 모델 크기와 아키텍처는 표준 하드웨어와의 호환성을 고려하여 설계되었으며, 아키텍처 개선 사항은 성능을 유지하면서도 하드웨어에 최적화되도록 조정되었습니다. Gemma 3은 이전 모델의 기능을 확장하고 다양한 작업에서 향상된 성능을 제공하는 것을 목표로 합니다.\n",
            "\n",
            "안전성은 Gemma 3 개발에서 중요한 고려 사항입니다. 개발 속도와 안전성 테스트 간의 균형을 유지하면서 표적화되고 효율적인 평가를 수행합니다. 모델의 잠재적 위험을 평가하기 위해 기본 평가를 수행하여 안전 정책 위반율을 측정합니다. 합성 적대적 사용자 쿼리를 사용하여 모델의 응답을 정책 위반 여부에 따라 평가합니다. Gemma 3은 이러한 안전 정책에서 전반적으로 낮은 위반율을 보입니다.\n",
            "\n",
            "Gemma 3은 STEM 관련 작업에서 향상된 성능을 보이므로 화학, 생물학, 방사선 및 핵(CBRN) 관련 지식에 대한 평가도 수행되었습니다. 내부 데이터 세트를 사용하여 CBRN 관련 지식 기반 다중 선택 질문에 대한 모델의 답변을 평가합니다. 화학 지식 평가는 Macknight 등이 개발한 화학적 위험에 대한 폐쇄형 지식 기반 접근 방식을 사용했습니다. 평가 결과 Gemma 3 모델의 CBRN 관련 지식은 낮은 수준으로 나타났습니다.\n",
            "\n",
            "책임감 있는 오픈 모델 설계를 위해 시스템 수준의 접근 방식을 채택하여 특정 사용 사례 및 환경과 관련된 위험을 완화합니다. 모델의 잠재적 위험에 비례하는 평가 및 안전 완화 조치를 지속적으로 적용합니다. 모델의 이점이 예측 가능한 위험보다 훨씬 클 때만 커뮤니티와 공유합니다. 안전하고 책임감 있는 애플리케이션을 개발하기 위한 다각적인 노력을 기울이고 있습니다.\n",
            "\n",
            "Gemma 3은 텍스트, 이미지 및 코드를 이해하고 생성할 수 있는 강력한 오픈 언어 모델입니다. 이 모델은 다양한 작업에서 향상된 성능을 제공하며, 표준 하드웨어와의 호환성을 고려하여 설계되었습니다. 안전성은 개발 과정에서 중요한 고려 사항이며, 잠재적 위험을 평가하고 완화하기 위한 다양한 조치를 취하고 있습니다. Gemma 3은 오픈 소스 커뮤니티에 기여하고 AI 기술 발전에 기여할 것으로 기대됩니다.\n",
            "---\n",
            "이 문서는 Google에서 개발한 Gemma 3 모델에 대한 기술 보고서입니다. 보고서는 Gemma 3 모델의 아키텍처, 훈련 방법 및 다양한 벤치마크에서의 성능을 자세히 설명합니다. Gemma 3는 이전 버전인 Gemma 2를 기반으로 구축되었으며, 사실적 지식, 상식 추론, STEM 능력 및 다국어 이해도를 포함한 다양한 영역에서 향상된 성능을 제공합니다. 이 보고서는 모델의 기능과 한계를 이해하는 데 관심 있는 개발자와 연구자에게 귀중한 리소스를 제공합니다.\n",
            "\n",
            "Gemma 3 모델은 10억 개에서 270억 개에 이르는 다양한 매개변수 크기로 제공되며, 다양한 계산 요구 사항과 사용 사례를 충족합니다. 이 모델은 대규모 텍스트 및 코드 데이터 세트에 대해 사전 훈련되었으며, 특정 작업에 맞게 미세 조정할 수 있습니다. 보고서는 또한 모델의 다중 모드 버전을 탐구하여 이미지 이해 능력을 확장합니다. 아키텍처에는 주의 메커니즘이 포함되어 있으며, 이는 모델이 입력 시퀀스에서 가장 관련성이 높은 부분에 집중할 수 있도록 합니다. 이 보고서는 모델의 아키텍처, 훈련 방법 및 평가 프로토콜에 대한 자세한 정보를 제공합니다.\n",
            "\n",
            "Gemma 3 모델은 다양한 벤치마크에서 이전 버전에 비해 상당한 개선을 보였습니다. 이 보고서는 사실적 지식, 상식 추론, STEM 능력 및 다국어 이해도를 포함한 다양한 영역에서 모델의 성능을 자세히 설명합니다. 특히 Gemma 3는 MMLU(Massive Multitask Language Understanding) 및 AGIEval(인공 일반 지능 평가)과 같은 벤치마크에서 뛰어난 성능을 보였으며, 이는 고급 추론 및 문제 해결 능력을 입증합니다. 이 보고서는 또한 모델의 장기 컨텍스트 처리 능력을 강조하고, RULER 및 MRCR 벤치마크에서 성능을 평가합니다.\n",
            "\n",
            "이 보고서는 Gemma 3 모델의 다중 모드 기능도 탐구하여 이미지 이해 능력을 확장합니다. 이 모델은 COCO Caption, DocVQA 및 TextVQA와 같은 다양한 시각적 질문 답변 벤치마크에서 평가되었습니다. 결과는 Gemma 3가 이미지와 관련된 질문에 정확하게 답변할 수 있음을 보여주며, 이는 다양한 응용 분야에 유용한 기능입니다. 이 보고서는 Gemma 3가 문서 이해와 관련된 벤치마크에서 뛰어난 성능을 보이며, 더 큰 PaliGemma 2 변형보다 뛰어난 성능을 보인다는 점을 강조합니다.\n",
            "\n",
            "이 보고서는 Gemma 3 모델의 잠재적 위험과 윤리적 고려 사항에 대해 논의합니다. 이 모델은 유해하거나 편향된 콘텐츠를 생성하는 데 사용될 수 있으며, 이는 사회적 위험으로 이어질 수 있습니다. 이 보고서는 이러한 위험을 완화하기 위한 책임감 있는 개발 및 배포의 중요성을 강조합니다. 이 보고서는 또한 모델의 다국어 기능을 탐구하고, MGSM, Global-MMLU-Lite 및 WMT24++와 같은 벤치마크에서 성능을 평가합니다. 결과는 Gemma 3가 다양한 언어에서 효과적으로 수행할 수 있음을 보여주며, 이는 글로벌 응용 분야에 유용합니다.\n",
            "---\n",
            "이 문서는 Gemma 3 모델의 성능을 다양한 벤치마크에서 평가한 결과를 제시합니다. 평가는 사전 훈련된(PT) 모델과 Instruction Fine-Tuned(IT) 모델 모두에 대해 수행되었으며, 모델 크기(4B, 12B, 27B)와 컨텍스트 길이(32K, 128K)에 따른 성능 변화를 분석합니다. 주요 평가 항목은 긴 컨텍스트 처리 능력, 멀티모달 이해 능력, 비디오 이해 능력, 그리고 다양한 내부 및 외부 벤치마크에서의 성능입니다.\n",
            "\n",
            "긴 컨텍스트 벤치마크(RULER, MRCR)에서 Gemma 3 모델은 컨텍스트 길이가 길어짐에 따라 성능이 감소하는 경향을 보이지만, Instruction Fine-Tuning을 통해 성능을 향상시킬 수 있음을 보여줍니다. 특히, 27B IT 모델은 RULER 벤치마크에서 128K 컨텍스트 길이에서 66.0의 정확도를 달성했습니다. 이는 모델이 긴 컨텍스트 정보를 효과적으로 활용할 수 있음을 시사합니다.\n",
            "\n",
            "멀티모달 벤치마크(MMMU, DocVQA, InfoVQA 등)에서 Gemma 3 IT 모델은 이미지와 텍스트를 함께 이해하는 능력을 평가받았습니다. 27B 모델은 대부분의 벤치마크에서 가장 높은 성능을 보였으며, MMMU 벤치마크에서 64.9의 정확도를 달성했습니다. 이는 모델이 다양한 시각적 정보를 효과적으로 처리하고 텍스트와 함께 이해할 수 있음을 나타냅니다.\n",
            "\n",
            "비디오 이해 벤치마크(Perception Test MCVQA, ActivityNet-QA)에서 Gemma 3 IT 모델은 비디오 콘텐츠를 이해하고 질문에 답변하는 능력을 평가받았습니다. 27B 모델은 Perception Test MCVQA 벤치마크에서 58.1의 정확도를 달성했으며, ActivityNet-QA 벤치마크에서 52.8의 점수를 기록했습니다. 이는 모델이 비디오 내의 시각적 정보를 이해하고 관련된 질문에 답변할 수 있음을 보여줍니다.\n",
            "\n",
            "다양한 내부 및 외부 벤치마크(MMLU, MBPP, HumanEval 등)에서 Gemma 3 IT 모델은 다양한 크기(1B, 4B, 12B, 27B)로 평가되었습니다. 27B 모델은 대부분의 벤치마크에서 가장 높은 성능을 보였으며, HumanEval 벤치마크에서 87.8의 정확도를 달성했습니다. 이는 모델이 다양한 유형의 작업을 수행하고 복잡한 문제를 해결할 수 있음을 시사합니다. 또한, N2C(Natural2Code) 벤치마크에서 높은 성능을 보인 것은 코드 생성 능력의 우수성을 나타냅니다.\n"
          ]
        }
      ],
      "source": [
        "gathered_summaries = '\\n---\\n'.join(raw_summaries)\n",
        "\n",
        "print(gathered_summaries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd5d91f1",
      "metadata": {
        "id": "fd5d91f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b6d2986-b83f-40d3-f1c2-1bb9d44ad7ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemma 3은 Google에서 개발한 최첨단 오픈 언어 모델 제품군으로, 텍스트, 이미지, 코드 이해를 모두 지원합니다. 이 모델은 이전 버전인 Gemma 2를 기반으로 구축되었으며, 이미지 이해, 긴 문맥 처리, 다국어 지원, STEM 관련 능력 향상에 중점을 둡니다. Gemma 3 모델은 10억 개에서 270억 개에 이르는 다양한 매개변수 크기로 제공되어 다양한 계산 요구 사항과 사용 사례를 충족하며, 표준 하드웨어와의 호환성을 고려하여 설계되었습니다.\n",
            "\n",
            "Gemma 3 모델은 로컬 및 글로벌 self-attention 레이어의 비율을 5:1로 조정하여 Gemma 2 모델의 1:1 비율에서 변경되었으며, 이는 복잡도에 미치는 영향이 미미합니다. 긴 문맥 처리를 위해 모델은 32K 시퀀스로 사전 학습된 후 4B, 12B, 27B 모델을 128K 토큰으로 확장하고 RoPE를 재조정합니다. SigLIP 기반의 비전 인코더를 사용하여 이미지 해상도를 개선하고, Pan & Scan (P&S)을 통해 이미지의 원래 종횡비와 해상도를 유지하여 이미지의 텍스트를 읽어야 하는 작업에서 성능이 향상됩니다.\n",
            "\n",
            "Gemma 3 모델은 지식 증류를 통해 훈련되었으며 사전 훈련 및 명령어 미세 조정 버전 모두에서 Gemma 2보다 뛰어난 성능을 보입니다. 특히, 새로운 사후 훈련 레시피는 수학, 채팅, 명령어 추종, 다국어 능력을 크게 향상시켜 Gemma3-4B-IT가 Gemma2-27B-IT와 경쟁하고 Gemma3-27B-IT가 벤치마크에서 Gemini-1.5-Pro와 비교될 수 있도록 합니다. 이 모델은 대규모 텍스트 및 코드 데이터 세트에 대해 사전 훈련되었으며, 특정 작업에 맞게 미세 조정할 수 있습니다.\n",
            "\n",
            "안전성은 Gemma 3 개발에서 중요한 고려 사항입니다. 훈련 데이터 필터링, SFT 및 RLHF를 사용하여 유해한 콘텐츠 생성을 방지하고, 내부 안전 프로세스를 통해 모델이 Google의 안전 정책을 준수하도록 설계되었습니다. 모델의 잠재적 위험을 평가하기 위해 기본 평가를 수행하여 안전 정책 위반율을 측정하고, 합성 적대적 사용자 쿼리를 사용하여 모델의 응답을 정책 위반 여부에 따라 평가합니다. Gemma 3은 이러한 안전 정책에서 전반적으로 낮은 위반율을 보입니다.\n",
            "\n",
            "Gemma 3은 텍스트, 이미지 및 코드를 이해하고 생성할 수 있는 강력한 오픈 언어 모델입니다. 이 모델은 다양한 작업에서 향상된 성능을 제공하며, 표준 하드웨어와의 호환성을 고려하여 설계되었습니다. 안전성은 개발 과정에서 중요한 고려 사항이며, 잠재적 위험을 평가하고 완화하기 위한 다양한 조치를 취하고 있습니다. Gemma 3은 오픈 소스 커뮤니티에 기여하고 AI 기술 발전에 기여할 것으로 기대됩니다.\n"
          ]
        }
      ],
      "source": [
        "# Reduce 과정 : 각 문서의 요약을 하나로 합칩니다.\n",
        "reduce_prompt = ChatPromptTemplate([\n",
        "    ('system', '''당신은 인공지능과 거대 언어 모델의 전문가입니다.\n",
        "LLM 논문에 대한 요약문의 리스트가 주어집니다.\n",
        "이를 읽고, 전체 주제를 포함하는 최종 요약을 작성하세요.\n",
        "요약은 5개의 문단과 문단별 4-8개의 문장으로 작성하세요.\n",
        "'''),\n",
        "    ('user', '''{text}\n",
        "---\n",
        "Summary:\n",
        "''')])\n",
        "\n",
        "reduce_chain = reduce_prompt | llm | StrOutputParser()\n",
        "\n",
        "summary = reduce_chain.invoke(gathered_summaries)\n",
        "\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "mb4EVbodjrdB",
        "outputId": "a17acd52-7703-49eb-d7d1-48ffe94cb756"
      },
      "id": "mb4EVbodjrdB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Gemma 3은 Google에서 개발한 최첨단 오픈 언어 모델 제품군으로, 텍스트, 이미지, 코드 이해를 모두 지원합니다. 이 모델은 이전 버전인 Gemma 2를 기반으로 구축되었으며, 이미지 이해, 긴 문맥 처리, 다국어 지원, STEM 관련 능력 향상에 중점을 둡니다. Gemma 3 모델은 10억 개에서 270억 개에 이르는 다양한 매개변수 크기로 제공되어 다양한 계산 요구 사항과 사용 사례를 충족하며, 표준 하드웨어와의 호환성을 고려하여 설계되었습니다.\\n\\nGemma 3 모델은 로컬 및 글로벌 self-attention 레이어의 비율을 5:1로 조정하여 Gemma 2 모델의 1:1 비율에서 변경되었으며, 이는 복잡도에 미치는 영향이 미미합니다. 긴 문맥 처리를 위해 모델은 32K 시퀀스로 사전 학습된 후 4B, 12B, 27B 모델을 128K 토큰으로 확장하고 RoPE를 재조정합니다. SigLIP 기반의 비전 인코더를 사용하여 이미지 해상도를 개선하고, Pan & Scan (P&S)을 통해 이미지의 원래 종횡비와 해상도를 유지하여 이미지의 텍스트를 읽어야 하는 작업에서 성능이 향상됩니다.\\n\\nGemma 3 모델은 지식 증류를 통해 훈련되었으며 사전 훈련 및 명령어 미세 조정 버전 모두에서 Gemma 2보다 뛰어난 성능을 보입니다. 특히, 새로운 사후 훈련 레시피는 수학, 채팅, 명령어 추종, 다국어 능력을 크게 향상시켜 Gemma3-4B-IT가 Gemma2-27B-IT와 경쟁하고 Gemma3-27B-IT가 벤치마크에서 Gemini-1.5-Pro와 비교될 수 있도록 합니다. 이 모델은 대규모 텍스트 및 코드 데이터 세트에 대해 사전 훈련되었으며, 특정 작업에 맞게 미세 조정할 수 있습니다.\\n\\n안전성은 Gemma 3 개발에서 중요한 고려 사항입니다. 훈련 데이터 필터링, SFT 및 RLHF를 사용하여 유해한 콘텐츠 생성을 방지하고, 내부 안전 프로세스를 통해 모델이 Google의 안전 정책을 준수하도록 설계되었습니다. 모델의 잠재적 위험을 평가하기 위해 기본 평가를 수행하여 안전 정책 위반율을 측정하고, 합성 적대적 사용자 쿼리를 사용하여 모델의 응답을 정책 위반 여부에 따라 평가합니다. Gemma 3은 이러한 안전 정책에서 전반적으로 낮은 위반율을 보입니다.\\n\\nGemma 3은 텍스트, 이미지 및 코드를 이해하고 생성할 수 있는 강력한 오픈 언어 모델입니다. 이 모델은 다양한 작업에서 향상된 성능을 제공하며, 표준 하드웨어와의 호환성을 고려하여 설계되었습니다. 안전성은 개발 과정에서 중요한 고려 사항이며, 잠재적 위험을 평가하고 완화하기 위한 다양한 조치를 취하고 있습니다. Gemma 3은 오픈 소스 커뮤니티에 기여하고 AI 기술 발전에 기여할 것으로 기대됩니다.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "multicampus",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a435919b",
      "metadata": {
        "id": "a435919b"
      },
      "source": [
        "# [실습] LangChain을 이용한 어플리케이션 개발   \n",
        "\n",
        "LCEL 구조를 이용하여, 간단한 텍스트를 요약하는 어플리케이션을 만들어 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "bf68e6c0",
      "metadata": {
        "id": "bf68e6c0",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting google_genai\n",
            "  Downloading google_genai-1.46.0-py3-none-any.whl.metadata (46 kB)\n",
            "Requirement already satisfied: langchain in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (1.0.2)\n",
            "Requirement already satisfied: langchain-community in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (0.4)\n",
            "Requirement already satisfied: langchain_google_genai in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (3.0.0)\n",
            "Requirement already satisfied: pymupdf in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (1.26.5)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from google_genai) (4.11.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from google_genai) (2.41.1)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from google_genai) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from google_genai) (2.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from google_genai) (2.32.5)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from google_genai) (9.1.2)\n",
            "Collecting websockets<15.1.0,>=13.0.0 (from google_genai)\n",
            "  Using cached websockets-15.0.1-cp312-cp312-win_amd64.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from google_genai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google_genai) (3.11)\n",
            "Requirement already satisfied: sniffio>=1.1 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google_genai) (1.3.1)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google_genai) (6.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google_genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google_genai) (4.9.1)\n",
            "Requirement already satisfied: certifi in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google_genai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google_genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google_genai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->google_genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->google_genai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->google_genai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google_genai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google_genai) (2.5.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.14.1->google_genai) (0.6.1)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from langchain) (1.0.1)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.0 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from langchain) (1.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (0.4.38)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (6.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain) (3.0.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from langgraph<1.1.0,>=1.0.0->langchain) (3.0.0)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.0 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from langgraph<1.1.0,>=1.0.0->langchain) (1.0.1)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from langgraph<1.1.0,>=1.0.0->langchain) (0.2.9)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from langgraph<1.1.0,>=1.0.0->langchain) (3.6.0)\n",
            "Requirement already satisfied: ormsgpack>=1.10.0 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.0->langchain) (1.11.0)\n",
            "Requirement already satisfied: orjson>=3.10.1 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (0.25.0)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from langchain-community) (1.0.0)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from langchain-community) (2.0.44)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from langchain-community) (3.13.1)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from langchain-community) (2.11.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from langchain-community) (2.3.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: greenlet>=1 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.2.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage<1.0.0,>=0.7.0 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from langchain_google_genai) (0.9.0)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from langchain_google_genai) (1.2.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain_google_genai) (2.27.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from google-ai-generativelanguage<1.0.0,>=0.7.0->langchain_google_genai) (1.76.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from google-ai-generativelanguage<1.0.0,>=0.7.0->langchain_google_genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from google-ai-generativelanguage<1.0.0,>=0.7.0->langchain_google_genai) (4.25.8)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain_google_genai) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in c:\\users\\pc\\onedrive\\1.강의\\0.llm\\1 패스트캠퍼스_랭그래프\\fastcampus_langgraph\\.venv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain_google_genai) (1.62.3)\n",
            "Downloading google_genai-1.46.0-py3-none-any.whl (239 kB)\n",
            "Using cached websockets-15.0.1-cp312-cp312-win_amd64.whl (176 kB)\n",
            "Installing collected packages: websockets, google_genai\n",
            "\n",
            "   ---------------------------------------- 0/2 [websockets]\n",
            "   ---------------------------------------- 0/2 [websockets]\n",
            "   -------------------- ------------------- 1/2 [google_genai]\n",
            "   -------------------- ------------------- 1/2 [google_genai]\n",
            "   -------------------- ------------------- 1/2 [google_genai]\n",
            "   ---------------------------------------- 2/2 [google_genai]\n",
            "\n",
            "Successfully installed google_genai-1.46.0 websockets-15.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install google_genai langchain langchain-community langchain_google_genai pymupdf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "A59cI3UHz9l2",
      "metadata": {
        "id": "A59cI3UHz9l2"
      },
      "source": [
        "## Gemini API 준비하기\n",
        "\n",
        "\n",
        "Google API 키를 등록하고 입력합니다.   \n",
        "구글 계정 로그인 후 https://aistudio.google.com  에 접속하면, API 키 생성이 가능합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "955e251a",
      "metadata": {
        "id": "955e251a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['GOOGLE_API_KEY'] = 'AIxxx'\n",
        "\n",
        "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Gemini API는 분당 10개 요청으로 제한\n",
        "# 즉, 초당 약 0.167개 요청 (10/60)\n",
        "rate_limiter = InMemoryRateLimiter(\n",
        "    requests_per_second=0.167,  # 분당 10개 요청\n",
        "    check_every_n_seconds=0.1,  # 100ms마다 체크\n",
        "    max_bucket_size=10,  # 최대 버스트 크기\n",
        ")\n",
        "\n",
        "# rate limiter를 LLM에 적용\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    rate_limiter=rate_limiter,\n",
        "    temperature = 0.5,\n",
        "    max_tokens = 2048\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd54abc8",
      "metadata": {
        "id": "dd54abc8"
      },
      "source": [
        "PDF 파일 준비하기   \n",
        "임의의 PDF 파일을 다운로드하여 준비합니다.   \n",
        "(예시 PDF 파일을 사용하실 분은 첨부된 PDF 파일을 활용해 주세요!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "14579bb5",
      "metadata": {
        "id": "14579bb5"
      },
      "outputs": [],
      "source": [
        "path_material = 'example.pdf'\n",
        "# 자유롭게 경로 변경해서 실행하셔도 됩니다!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0948d1b",
      "metadata": {
        "id": "c0948d1b"
      },
      "source": [
        "랭체인에서는 데이터를 `Document` 클래스로 처리합니다.   \n",
        "데이터의 형식에 따라 적절한 document_loader를 불러와서 사용할 수 있습니다.   \n",
        "\n",
        "이번 실습에서는 PDF를 불러오는 가장 간단한 로더인 PyMuPDFLoader를 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3c18c40c",
      "metadata": {
        "id": "3c18c40c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# Number of Pages: 32\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "\n",
        "loader = PyMuPDFLoader(path_material)\n",
        "# 페이지별로 저장\n",
        "pages = loader.load()\n",
        "print(\"# Number of Pages:\", len(pages))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4554d08",
      "metadata": {
        "id": "a4554d08"
      },
      "source": [
        "파일이 너무 긴 경우,일부만 선택하여 요약할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "4b061ea2",
      "metadata": {
        "id": "4b061ea2"
      },
      "outputs": [],
      "source": [
        "pages = pages[:11]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "41633081",
      "metadata": {
        "id": "41633081"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'example.pdf', 'file_path': 'example.pdf', 'total_pages': 32, 'format': 'PDF 1.5', 'title': 'MCPEval: Automatic MCP-based Deep Evaluation for AI Agent Models', 'author': 'Zhiwei Liu; Jielin Qiu; Shiyu Wang; Jianguo Zhang; Zuxin Liu; Roshan Ram; Haolin Chen; Weiran Yao; Huan Wang; Shelby Heinecke; Silvio Savarese; Caiming Xiong', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10}, page_content='A\\nComprehensive Experiments and Results\\nWe present a comprehensive evaluation of 10 state-of-the-art LLM models across 5 diverse domains using\\nthe MCPEval framework. Our analysis encompasses 5k trajectory records and 5k completion records\\nwith detailed individual task analysis, plus 50 model-domain combinations from systematic LLM judger\\nevaluation, representing the most extensive evaluation of LLM tool-use capabilities to date.\\nA.1\\nExperimental Setup\\nModel Selection\\nOur evaluation includes 10 models spanning different architectures and capabilities:\\n• OpenAI Models (7): GPT-4o, GPT-4o-mini, GPT-4.1-mini, GPT-4.1-nano, O3, O3-mini, O4-mini\\n• Open-Source Models (3): Mistral-Small-24B, Qwen3-32B, Qwen3-30B-A3B\\nDomain and Tools Coverage\\nWe evaluate performance across 5 diverse application domains with the\\nfollowing tools:\\n• Finance: Stock prices, financial data, market analysis, portfolio management\\n• Healthcare: Medical terminology lookup, drug information, clinical trials, health topics, PubMed\\nsearch\\n• Airbnb:airbnb_search, airbnb_listing_details\\n• Sports: Team statistics, player information, game schedules, league standings\\n• National Parks: Park information, visitor services, trail details, facility booking\\nB\\nEvaluation Criteria\\nTo holistically assess the capabilities of AI agents in complex task environments, we adopt a two-\\ndimensional evaluation criteria: (1) Tool Call Performance, which measures the correctness of predicted\\ntool usage against the ground truth, and (2) LLM Judger Performance, which scores the overall quality of\\nexecution trajectories and task completion using rubric-based judgment. These complementary dimensions\\nenable both precise operational assessment and high-level behavioral evaluation.\\nB.1\\nTool Call Criteria\\nTool call evaluation is conducted using the MCP Model Evaluator’s analyze command. This system\\ncompares the agent’s predicted tool usage with the ground truth across both strict and flexible matching\\nprotocols.\\nStrict Matching:\\nRequires exact correspondence on tool names, parameter values, and execution order.\\nIt represents a binary success paradigm: either the task is fully correct or it fails.\\nFlexible Matching:\\nAllows partial credit by applying similarity thresholds for parameter values (≥\\n0.6) and tool order (≥0.5). It reflects more tolerant criteria aligned with approximate but contextually\\nappropriate predictions.\\nMetric Definitions:\\n• Average Name Match Score: Measures the proportion of correctly predicted tool names.\\n• Average Parameter Match Score: Assesses correctness or similarity of parameter values.\\n• Average Order Match Score: Evaluates sequence alignment between predicted and actual tool\\ncalls.\\n• Average Overall Score: A weighted combination of the above metrics, using configurable weights\\n(default: 0.4 for name and parameter, 0.2 for order).\\nIn addition to aggregate statistics, the system also provides fine-grained diagnostics such as missing/extra\\ntools, parameter mismatches, and tool-specific success rates, facilitating detailed error analysis.\\n11')"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pages[10]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08e8c187",
      "metadata": {
        "id": "08e8c187"
      },
      "source": [
        "PDF 파일은 페이지별 Document를 저장합니다.   \n",
        "요약을 수행하기 위해, 전체 텍스트를 하나의 Document에 합칩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "bb0f968d",
      "metadata": {
        "id": "bb0f968d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "43650"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.documents import Document\n",
        "# Document 클래스 만들기\n",
        "\n",
        "corpus = Document(page_content='')\n",
        "for page in pages:\n",
        "    corpus.page_content += page.page_content + '\\n'\n",
        "\n",
        "corpus.page_content = corpus.page_content.replace('\\n\\n','\\n')\n",
        "len(corpus.page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6721b57a",
      "metadata": {
        "id": "6721b57a"
      },
      "source": [
        "LLM에 처리하기 전, 토큰 수를 체크합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "787f4fb5",
      "metadata": {
        "id": "787f4fb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt tokens: 14155\n"
          ]
        }
      ],
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "client = genai.Client()\n",
        "\n",
        "\n",
        "response = client.models.count_tokens(\n",
        "    model='gemini-2.0-flash',\n",
        "    contents=corpus.page_content\n",
        ")\n",
        "\n",
        "print(\"Prompt tokens:\",response.total_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7be7d52",
      "metadata": {
        "id": "c7be7d52"
      },
      "source": [
        "요약 체인을 만들고 구성합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee52b512",
      "metadata": {
        "id": "ee52b512"
      },
      "source": [
        "## [실습] 요약 체인 만들기\n",
        "\n",
        "`corpus.page_content`를 입력으로 받는 요약 체인을 만들고 실행하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "622b8bc1",
      "metadata": {
        "id": "622b8bc1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'## MCPEval: AI 에이전트 모델을 위한 자동 MCP 기반 심층 평가\\n\\n본 논문에서는 LLM 기반 AI 에이전트의 자동 심층 평가를 위한 MCPEval 프레임워크를 소개합니다. 기존의 평가 방법은 정적인 벤치마크와 수동 데이터 수집에 의존하여 실제적인 평가에 제한이 있었습니다. MCPEval은 MCP(Model Context Protocol)를 기반으로 하여 다양한 도메인에서 LLM 에이전트의 엔드 투 엔드 작업 생성 및 심층 평가를 자동화합니다. 이를 통해 평가 파이프라인 구축에 필요한 수동 노력을 줄이고, 표준화된 메트릭을 제공하며, 에이전트의 도구와 원활하게 통합됩니다.\\n\\nMCPEval 프레임워크는 크게 세 가지 주요 단계로 구성됩니다. 첫째, 작업 생성 단계에서는 MCP 서버의 도구 호출 메서드를 활용하여 도구 사양을 수집하고, Task-LLM을 통해 상세한 작업 지침을 생성합니다. 둘째, 작업 검증 단계에서는 프론티어 에이전트가 MCP 클라이언트로서 MCP 서버와 상호 작용하며 생성된 작업을 실행합니다. 성공적인 실행 궤적은 검증된 작업과 해당 정답 궤적으로 이어지며, 실패 시에는 작업 업데이트 요청을 통해 작업 설명을 개선합니다. 이러한 반복적인 검증 및 개선 프로세스를 통해 고품질의 작업 데이터셋을 구축합니다.\\n\\n모델 평가 단계에서는 평가 대상 모델을 MCP 클라이언트로 설정하고 검증된 작업 세트를 완료하도록 합니다. 수집된 궤적은 두 가지 관점에서 분석됩니다. 첫째, 도구 호출 매칭은 모델의 도구 사용을 정답 궤적과 엄격하게 비교합니다. 둘째, LLM 심판은 계획, 실행 흐름, 컨텍스트 인식 등 다양한 차원을 평가합니다. MCPEval은 이러한 분석 결과를 결합하여 각 에이전트 모델의 강점, 약점 및 성능을 상세히 설명하는 포괄적인 보고서를 자동 생성합니다.\\n\\nMCPEval은 다양한 아키텍처와 기능을 가진 10개의 모델을 대상으로 5개의 실제 도메인에서 실험을 진행했습니다. 평가에는 GPT-4o, GPT-4o-mini, Mistral-Small-24B, Qwen3-32B 등 다양한 모델이 포함되었으며, 의료, 에어비앤비, 스포츠, 국립 공원, YFinance 등 다양한 도메인에서 성능을 측정했습니다. 평가 기준은 도구 호출 분석과 LLM 심판 분석을 결합하여 모델의 도구 사용 정확도와 전반적인 작업 수행 능력을 종합적으로 평가했습니다.\\n\\n실험 결과, GPT-4 모델이 전반적으로 우수한 성능을 보였으며, 특히 gpt4o 모델은 다양한 도메인에서 일관되게 높은 점수를 획득했습니다. 또한, 작은 모델이 특정 작업에서 더 큰 오픈 소스 모델보다 뛰어난 성능을 보이는 경우도 있어, MCPEval이 기존 벤치마크에서 간과할 수 있는 미묘한 강점과 약점을 드러내는 데 효과적임을 입증했습니다. MCPEval은 오픈 소스 플랫폼으로 제공되어 재현 가능한 평가를 지원하고, LLM 연구 커뮤니티의 발전을 가속화할 것으로 기대됩니다.'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "summary_prompt = ChatPromptTemplate(\n",
        "    [('user', '''\n",
        "당신은 LLM/AI 전문가입니다.\n",
        "LLM에 대한 최신 모델의 논문이 주어집니다.\n",
        "전문 개발자를 대상으로 이를 설명한다고 가정하고,\n",
        "모델의 구조에 집중해서 논문의 내용을 요약하세요.\n",
        "\n",
        "전체는 5문단으로 하고, 문단별 4~8개 문장으로 구성하세요.\n",
        "---\n",
        "{text}''')\n",
        "\n",
        "    ]\n",
        "\n",
        ")\n",
        "summary_chain = summary_prompt | llm | StrOutputParser()\n",
        "\n",
        "summary = summary_chain.invoke(corpus.page_content)\n",
        "summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e499d89",
      "metadata": {
        "id": "4e499d89"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05e7e658",
      "metadata": {
        "id": "05e7e658"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "5ee49e0e",
      "metadata": {
        "id": "5ee49e0e"
      },
      "source": [
        "Gemini 2.0 Flash의 Context Window는 1M이므로 전체를 모두 하나의 컨텍스트로 입력해도 되지만,   \n",
        "Context가 짧은 모델들의 경우, 전체를 분할하여 요약 작업을 수행할 수 있습니다.\n",
        "\n",
        "**Map-Reduce** 방식의 요약을 만들어 보겠습니다.   \n",
        "\n",
        "Map-Reduce는 텍스트를 청크로 분할하고, 청크별 요약을 생성한 뒤  \n",
        "전체 요약문을 합쳐 프롬프트로 넣고 최종 요약문을 생성하는 방식입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0740625",
      "metadata": {
        "id": "c0740625"
      },
      "source": [
        "문서를 청크로 나누기 위해, RecursiveCharacterTextSplitter를 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "f8031c4c",
      "metadata": {
        "id": "f8031c4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3\n"
          ]
        }
      ],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=20000,\n",
        "    chunk_overlap=4000,\n",
        ")\n",
        "\n",
        "chunks = text_splitter.split_documents([corpus])\n",
        "print(len(chunks))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac546a56",
      "metadata": {
        "id": "ac546a56"
      },
      "source": [
        "Map: 청크별 요약을 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "93b6596c",
      "metadata": {
        "id": "93b6596c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 33%|███▎      | 1/3 [00:06<00:13,  6.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "# 0\n",
            "## MCPEval 논문 요약\n",
            "\n",
            "**서론:**\n",
            "최근 대규모 언어 모델(LLM) 기반 지능형 에이전트의 급증으로 인해, 확장 가능하고 강력한 평가 프레임워크의 필요성이 대두되고 있습니다. 기존 방법들은 정적인 벤치마크와 노동 집약적인 데이터 수집에 의존하여 실제적인 평가에 한계가 있습니다. 이에 본 논문에서는 다양한 도메인에서 LLM 에이전트의 엔드 투 엔드 작업 생성 및 심층 평가를 자동화하는 오픈 소스 모델 컨텍스트 프로토콜(MCP) 기반 프레임워크인 MCPEval을 소개합니다. MCPEval은 표준화된 지표를 제공하고, 에이전트의 기본 도구와 원활하게 통합되며, 평가 파이프라인 구축에 필요한 수동 노력을 제거합니다.\n",
            "\n",
            "**관련 연구:**\n",
            "LLM 평가 방식은 정적 벤치마크에서 벗어나 에이전트 기반의 동적인 방식으로 진화하고 있습니다. 초기에는 HELM, BIG-bench, MMLU와 같은 정적 벤치마크가 사용되었지만, 실제 환경과의 상호작용을 제대로 반영하지 못한다는 한계가 있었습니다. 이후 MT-Bench, AgentBoard, AgentBench와 같은 대화형 및 에이전트 기반 벤치마크가 등장했지만, 여전히 도구 통합이 부족했습니다. 이러한 문제를 해결하기 위해 MCP가 LLM과 외부 시스템 간의 상호 작용을 관리하는 핵심 표준으로 떠올랐으며, MCP-Radar, MCPWorld와 같은 프레임워크가 개발되었습니다.\n",
            "\n",
            "**MCPEval 프레임워크:**\n",
            "MCPEval은 작업 생성, 검증 및 모델 평가로 구성된 평가 워크플로우를 채택하여 LLM 에이전트의 효율적이고 확장 가능한 평가를 지원합니다. 작업 생성은 MCP 서버에서 도구 호출 메서드를 호출하여 도구 사양을 프롬프트로 수집하는 것으로 시작합니다. Task-LLM은 도구 호출에 필요한 정보가 포함되도록 상세한 작업 지침을 생성합니다. 생성된 작업이 도구 매개변수를 채우는 데 필요한 모든 정보를 포함하지 않을 수 있으므로, 작업 검증을 통해 고품질 작업을 생성하고 실제 실행 궤적을 수집합니다.\n",
            "\n",
            "**실험 및 결과:**\n",
            "MCPEval은 다양한 아키텍처와 기능을 가진 10개의 모델(GPT-4o, GPT-4o-mini, GPT-4.1-mini, GPT-4.1-nano, O3, O3-mini, O4-mini, Mistral-Small-24B, Qwen3-32B, Qwen3-30B-A3B)을 대상으로 5가지 실제 애플리케이션 도메인(의료, 에어비앤비, 스포츠, 국립공원, YFinance)에서 성능을 평가했습니다. 평가 기준은 도구 호출 분석(매개변수 일치 점수, 도구 이름 일치 점수, 도구 순서 일치 점수)과 LLM 심판 분석(계획, 실행 흐름, 상황 인식 등)을 결합한 다단계 접근 방식을 사용했습니다. 실험 결과, GPT-4 변형 모델들이 전반적으로 우수한 성능을 보였으며, 특히 gpt4o 모델이 두각을 나타냈습니다.\n",
            "\n",
            "**결론 및 논의:**\n",
            "MCPEval을 통해 다양한 도메인에서 모델의 성능을 비교 분석한 결과, 정적 벤치마크에서는 간과될 수 있는 미묘한 강점과 약점을 파악할 수 있었습니다. 도구 호출 성능 측면에서는 GPT-4 모델들이 우위를 점했지만, LLM 심판 분석에서는 o3 모델이 뛰어난 계획 및 작업 이해 능력을 보여주었습니다. 또한, MCPEval은 도메인별 성능 분석을 통해 API 구조와 작업 복잡성에 따라 성능 차이가 발생한다는 것을 밝혔습니다. MCPEval은 오픈 소스 플랫폼으로 제공되어 재현 가능성을 높이고, LLM 연구 커뮤니티의 발전에 기여할 것으로 기대됩니다.\n",
            "===========================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 67%|██████▋   | 2/3 [00:13<00:06,  6.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "# 1\n",
            "다음은 제공된 논문 발췌문의 5개 문단 요약입니다.\n",
            "\n",
            "**1. LLM 평가 결과 개요**\n",
            "본 논문은 LLM(Large Language Model)의 MCP(Model Context Protocol) 환경에서의 성능을 평가한 결과를 제시합니다.  평가에는 다양한 모델이 사용되었으며, 각 모델은 trajectory(Traj)와 completion(Comp) 두 가지 측면에서 평가되었습니다.  평가 결과는 금융, Airbnb, 의료, 스포츠, 국립공원 등 다양한 도메인에 걸쳐 제시되었으며, 각 도메인별 평균 점수도 함께 제공됩니다.  OpenAI의 O3 모델이 계획, 요구사항 커버리지, 완전성 측면에서 뛰어난 성능을 보였으며, gpt4o 모델들은 효과적인 계획과 도구 호출 접근 방식을 결합하여 좋은 결과를 나타냈습니다.  반면, mistral-small-24b 및 qwen3 모델과 같은 오픈 소스 모델은 성능 격차와 모델별 도구 사용 패턴의 차이로 인해 낮은 점수를 받았습니다.\n",
            "\n",
            "**2. 도메인별 성능 분석**\n",
            "도메인별 성능 분석 결과, 의료 분야가 가장 높은 성능을 보였고, Airbnb, 스포츠, 금융, 국립공원 순으로 나타났습니다.  의료 분야는 표준화된 데이터와 API 출력과 사용자 요구 간의 강력한 일치 덕분에 우수한 성적을 거두었습니다.  반면, 국립공원 분야는 복잡한 지리적 데이터 통합으로 인해 어려움을 겪었습니다.  금융 분야는 trajectory 점수는 중간 수준이지만 completion 점수가 높은 반면, Airbnb는 큰 격차를 보여 종합적인 결과 생성에 어려움이 있음을 시사합니다.  의료 및 Airbnb 분야는 작업 분포가 가장 높아 분석의 신뢰성을 높였습니다.  전반적으로, 도메인별 격차는 API 설계 품질과 작업 요구 사항의 차이를 반영합니다.\n",
            "\n",
            "**3. 성능 격차 분석**\n",
            "모델 전반적으로 trajectory 실행이 completion보다 더 나은 성능을 보이는 일관된 추세가 나타났습니다.  이는 LLM의 종합 능력에 아직 개선의 여지가 있음을 시사합니다.  대부분의 모델은 긍정적인 실행-완성 격차를 보였으며, 오픈 소스 모델이 OpenAI 모델보다 더 큰 격차를 나타냈습니다.  특이하게도 O3 모델은 completion 품질이 우수한 드문 음의 격차를 보였습니다.  도메인 수준 분석에서도 유사한 추세가 나타났으며, 금융 분야가 가장 작은 격차를, Airbnb가 가장 큰 격차를 보였습니다.  성능과 격차 크기 간의 약한 음의 상관관계는 강력한 모델이 더 균형 잡힌 경향이 있음을 시사하지만, 아키텍처별 요인도 역할을 합니다.\n",
            "\n",
            "**4. 모델 성능 계층 구조**\n",
            "도구 호출 및 LLM 평가 기준 모두에서 OpenAI의 O3, GPT-4o-mini, GPT-4.1-mini 모델이 일관되게 높은 성능을 보였습니다.  OpenAI 모델은 오픈 소스 모델보다 전반적인 품질이 우수하고 분산이 낮아 도구 사용 능력이 더 안정적임을 나타냅니다.  파라미터 일치 점수가 이름 일치 점수보다 높다는 것은 모델이 컨텍스트를 이해하지만 GPT-4.1의 ground truth와 다른 도구를 선택할 수 있음을 시사합니다.  대부분의 모델은 실행에 뛰어나지만 completion에는 그렇지 않은 반면, O3 모델은 completion에서 뛰어난 성능을 보여 trajectory 실행과 completion 품질 간의 trade-off가 분명하게 드러납니다.\n",
            "\n",
            "**5. 결론 및 한계점**\n",
            "본 연구에서는 LLM 에이전트 평가의 중요한 격차를 해결하기 위해 설계된 자동화된 MCP 기반 평가 프레임워크인 MCPEval을 제시합니다.  MCPEval은 다양한 도메인에서 평가 프로세스를 구조화하고, 엔드 투 엔드 작업 생성, 검증, 심층 평가를 자동화합니다.  MCPEval을 오픈 소스 툴킷으로 공개함으로써 LLM 에이전트 커뮤니티를 위한 재현 가능하고 확장 가능하며 표준화된 평가 방법을 촉진하고, 더 강력하고 유능한 AI 에이전트를 개발하는 것을 목표로 합니다.  MCPEval의 강점에도 불구하고, 평가가 전적으로 합성 데이터에 의존하고, LLM 기반 평가자의 사용 비용이 높으며, 자동화된 검증 프로세스가 편향을 도입할 수 있다는 한계점이 존재합니다.\n",
            "===========================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3/3 [00:19<00:00,  6.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "# 2\n",
            "다음은 주어진 논문 일부를 5개의 문단으로 요약한 내용입니다. 각 문단은 4~8개의 문장으로 구성되어 있습니다.\n",
            "\n",
            "이 논문은 대규모 언어 모델(LLM)이 실제 소프트웨어 엔지니어링 문제를 해결할 수 있는지를 평가하기 위한 연구입니다. 다양한 LLM 모델들을 대상으로 여러 도메인에서 툴 사용 능력을 평가하는 MCPEval 프레임워크를 사용하여 광범위한 실험을 수행했습니다. 평가는 5,000개의 궤적 기록과 5,000개의 완료 기록을 포함하며, LLM 심판 평가를 통해 50개의 모델-도메인 조합을 체계적으로 분석하여 LLM의 툴 사용 능력에 대한 포괄적인 평가를 제공합니다.\n",
            "\n",
            "실험 설정에는 OpenAI 모델 7개(GPT-4o, GPT-4o-mini, GPT-4.1-mini, GPT-4.1-nano, O3, O3-mini, O4-mini)와 오픈 소스 모델 3개(Mistral-Small-24B, Qwen3-32B, Qwen3-30B-A3B)가 포함되었습니다. 평가는 금융, 의료, Airbnb, 스포츠, 국립 공원이라는 5가지 다양한 응용 분야에서 수행되었습니다. 각 도메인에는 주가, 의료 용어 검색, 숙소 검색, 팀 통계, 공원 정보와 같은 특정 도구들이 사용되었습니다. 이러한 다양한 모델과 도메인을 통해 LLM의 일반적인 툴 사용 능력을 평가할 수 있습니다.\n",
            "\n",
            "AI 에이전트의 능력을 평가하기 위해 툴 호출 성능과 LLM 심판 성능이라는 두 가지 평가 기준을 사용했습니다. 툴 호출 성능은 실제 정답과 비교하여 예측된 툴 사용의 정확성을 측정합니다. LLM 심판 성능은 루브릭 기반 판단을 사용하여 실행 궤적과 작업 완료의 전반적인 품질을 평가합니다. 이 두 가지 기준은 정확한 운영 평가와 높은 수준의 행동 평가를 가능하게 합니다.\n",
            "\n",
            "툴 호출 평가는 MCP 모델 평가기의 분석 명령을 사용하여 수행됩니다. 이 시스템은 엄격한 매칭 프로토콜과 유연한 매칭 프로토콜을 모두 사용하여 에이전트의 예측된 툴 사용과 실제 정답을 비교합니다. 엄격한 매칭은 툴 이름, 매개변수 값, 실행 순서에 대한 정확한 일치를 요구하며, 유연한 매칭은 매개변수 값에 대한 유사성 임계값과 툴 순서에 대한 유사성 임계값을 적용하여 부분적인 점수를 허용합니다.\n",
            "\n",
            "평가 지표에는 평균 이름 일치 점수, 평균 매개변수 일치 점수, 평균 순서 일치 점수 및 평균 전체 점수가 포함됩니다. 평균 이름 일치 점수는 올바르게 예측된 툴 이름의 비율을 측정하고, 평균 매개변수 일치 점수는 매개변수 값의 정확성 또는 유사성을 평가합니다. 평균 순서 일치 점수는 예측된 툴 호출과 실제 툴 호출 간의 순서 정렬을 평가하고, 평균 전체 점수는 위의 지표들을 가중 조합하여 계산합니다. 또한, 시스템은 누락/추가 툴, 매개변수 불일치, 툴별 성공률과 같은 세부적인 진단 정보를 제공하여 자세한 오류 분석을 용이하게 합니다.\n",
            "===========================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Map 과정 : 각 문서에 대해 요약을 생성합니다.\n",
        "from tqdm import tqdm\n",
        "\n",
        "map_prompt = ChatPromptTemplate([\n",
        "    ('system', '''논문의 일부가 주어집니다.\n",
        "해당 내용을 읽고 한국어로 요약하세요.\n",
        "요약은 5개의 문단과 문단별 4~8개의 문장으로 작성하세요.\n",
        "'''),\n",
        "    ('user', '''{text}''')])\n",
        "\n",
        "raw_summaries = []\n",
        "\n",
        "map_chain  = map_prompt | llm | StrOutputParser()\n",
        "\n",
        "\n",
        "for i in tqdm(range(len(chunks))):\n",
        "    response = map_chain.invoke(chunks[i].page_content)\n",
        "\n",
        "    raw_summaries.append(response)\n",
        "\n",
        "    print('\\n#',i)\n",
        "    print(response)\n",
        "    print('===========================')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "64afc69b",
      "metadata": {
        "id": "64afc69b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "## MCPEval 논문 요약\n",
            "\n",
            "**서론:**\n",
            "최근 대규모 언어 모델(LLM) 기반 지능형 에이전트의 급증으로 인해, 확장 가능하고 강력한 평가 프레임워크의 필요성이 대두되고 있습니다. 기존 방법들은 정적인 벤치마크와 노동 집약적인 데이터 수집에 의존하여 실제적인 평가에 한계가 있습니다. 이에 본 논문에서는 다양한 도메인에서 LLM 에이전트의 엔드 투 엔드 작업 생성 및 심층 평가를 자동화하는 오픈 소스 모델 컨텍스트 프로토콜(MCP) 기반 프레임워크인 MCPEval을 소개합니다. MCPEval은 표준화된 지표를 제공하고, 에이전트의 기본 도구와 원활하게 통합되며, 평가 파이프라인 구축에 필요한 수동 노력을 제거합니다.\n",
            "\n",
            "**관련 연구:**\n",
            "LLM 평가 방식은 정적 벤치마크에서 벗어나 에이전트 기반의 동적인 방식으로 진화하고 있습니다. 초기에는 HELM, BIG-bench, MMLU와 같은 정적 벤치마크가 사용되었지만, 실제 환경과의 상호작용을 제대로 반영하지 못한다는 한계가 있었습니다. 이후 MT-Bench, AgentBoard, AgentBench와 같은 대화형 및 에이전트 기반 벤치마크가 등장했지만, 여전히 도구 통합이 부족했습니다. 이러한 문제를 해결하기 위해 MCP가 LLM과 외부 시스템 간의 상호 작용을 관리하는 핵심 표준으로 떠올랐으며, MCP-Radar, MCPWorld와 같은 프레임워크가 개발되었습니다.\n",
            "\n",
            "**MCPEval 프레임워크:**\n",
            "MCPEval은 작업 생성, 검증 및 모델 평가로 구성된 평가 워크플로우를 채택하여 LLM 에이전트의 효율적이고 확장 가능한 평가를 지원합니다. 작업 생성은 MCP 서버에서 도구 호출 메서드를 호출하여 도구 사양을 프롬프트로 수집하는 것으로 시작합니다. Task-LLM은 도구 호출에 필요한 정보가 포함되도록 상세한 작업 지침을 생성합니다. 생성된 작업이 도구 매개변수를 채우는 데 필요한 모든 정보를 포함하지 않을 수 있으므로, 작업 검증을 통해 고품질 작업을 생성하고 실제 실행 궤적을 수집합니다.\n",
            "\n",
            "**실험 및 결과:**\n",
            "MCPEval은 다양한 아키텍처와 기능을 가진 10개의 모델(GPT-4o, GPT-4o-mini, GPT-4.1-mini, GPT-4.1-nano, O3, O3-mini, O4-mini, Mistral-Small-24B, Qwen3-32B, Qwen3-30B-A3B)을 대상으로 5가지 실제 애플리케이션 도메인(의료, 에어비앤비, 스포츠, 국립공원, YFinance)에서 성능을 평가했습니다. 평가 기준은 도구 호출 분석(매개변수 일치 점수, 도구 이름 일치 점수, 도구 순서 일치 점수)과 LLM 심판 분석(계획, 실행 흐름, 상황 인식 등)을 결합한 다단계 접근 방식을 사용했습니다. 실험 결과, GPT-4 변형 모델들이 전반적으로 우수한 성능을 보였으며, 특히 gpt4o 모델이 두각을 나타냈습니다.\n",
            "\n",
            "**결론 및 논의:**\n",
            "MCPEval을 통해 다양한 도메인에서 모델의 성능을 비교 분석한 결과, 정적 벤치마크에서는 간과될 수 있는 미묘한 강점과 약점을 파악할 수 있었습니다. 도구 호출 성능 측면에서는 GPT-4 모델들이 우위를 점했지만, LLM 심판 분석에서는 o3 모델이 뛰어난 계획 및 작업 이해 능력을 보여주었습니다. 또한, MCPEval은 도메인별 성능 분석을 통해 API 구조와 작업 복잡성에 따라 성능 차이가 발생한다는 것을 밝혔습니다. MCPEval은 오픈 소스 플랫폼으로 제공되어 재현 가능성을 높이고, LLM 연구 커뮤니티의 발전에 기여할 것으로 기대됩니다.\n",
            "---\n",
            "다음은 제공된 논문 발췌문의 5개 문단 요약입니다.\n",
            "\n",
            "**1. LLM 평가 결과 개요**\n",
            "본 논문은 LLM(Large Language Model)의 MCP(Model Context Protocol) 환경에서의 성능을 평가한 결과를 제시합니다.  평가에는 다양한 모델이 사용되었으며, 각 모델은 trajectory(Traj)와 completion(Comp) 두 가지 측면에서 평가되었습니다.  평가 결과는 금융, Airbnb, 의료, 스포츠, 국립공원 등 다양한 도메인에 걸쳐 제시되었으며, 각 도메인별 평균 점수도 함께 제공됩니다.  OpenAI의 O3 모델이 계획, 요구사항 커버리지, 완전성 측면에서 뛰어난 성능을 보였으며, gpt4o 모델들은 효과적인 계획과 도구 호출 접근 방식을 결합하여 좋은 결과를 나타냈습니다.  반면, mistral-small-24b 및 qwen3 모델과 같은 오픈 소스 모델은 성능 격차와 모델별 도구 사용 패턴의 차이로 인해 낮은 점수를 받았습니다.\n",
            "\n",
            "**2. 도메인별 성능 분석**\n",
            "도메인별 성능 분석 결과, 의료 분야가 가장 높은 성능을 보였고, Airbnb, 스포츠, 금융, 국립공원 순으로 나타났습니다.  의료 분야는 표준화된 데이터와 API 출력과 사용자 요구 간의 강력한 일치 덕분에 우수한 성적을 거두었습니다.  반면, 국립공원 분야는 복잡한 지리적 데이터 통합으로 인해 어려움을 겪었습니다.  금융 분야는 trajectory 점수는 중간 수준이지만 completion 점수가 높은 반면, Airbnb는 큰 격차를 보여 종합적인 결과 생성에 어려움이 있음을 시사합니다.  의료 및 Airbnb 분야는 작업 분포가 가장 높아 분석의 신뢰성을 높였습니다.  전반적으로, 도메인별 격차는 API 설계 품질과 작업 요구 사항의 차이를 반영합니다.\n",
            "\n",
            "**3. 성능 격차 분석**\n",
            "모델 전반적으로 trajectory 실행이 completion보다 더 나은 성능을 보이는 일관된 추세가 나타났습니다.  이는 LLM의 종합 능력에 아직 개선의 여지가 있음을 시사합니다.  대부분의 모델은 긍정적인 실행-완성 격차를 보였으며, 오픈 소스 모델이 OpenAI 모델보다 더 큰 격차를 나타냈습니다.  특이하게도 O3 모델은 completion 품질이 우수한 드문 음의 격차를 보였습니다.  도메인 수준 분석에서도 유사한 추세가 나타났으며, 금융 분야가 가장 작은 격차를, Airbnb가 가장 큰 격차를 보였습니다.  성능과 격차 크기 간의 약한 음의 상관관계는 강력한 모델이 더 균형 잡힌 경향이 있음을 시사하지만, 아키텍처별 요인도 역할을 합니다.\n",
            "\n",
            "**4. 모델 성능 계층 구조**\n",
            "도구 호출 및 LLM 평가 기준 모두에서 OpenAI의 O3, GPT-4o-mini, GPT-4.1-mini 모델이 일관되게 높은 성능을 보였습니다.  OpenAI 모델은 오픈 소스 모델보다 전반적인 품질이 우수하고 분산이 낮아 도구 사용 능력이 더 안정적임을 나타냅니다.  파라미터 일치 점수가 이름 일치 점수보다 높다는 것은 모델이 컨텍스트를 이해하지만 GPT-4.1의 ground truth와 다른 도구를 선택할 수 있음을 시사합니다.  대부분의 모델은 실행에 뛰어나지만 completion에는 그렇지 않은 반면, O3 모델은 completion에서 뛰어난 성능을 보여 trajectory 실행과 completion 품질 간의 trade-off가 분명하게 드러납니다.\n",
            "\n",
            "**5. 결론 및 한계점**\n",
            "본 연구에서는 LLM 에이전트 평가의 중요한 격차를 해결하기 위해 설계된 자동화된 MCP 기반 평가 프레임워크인 MCPEval을 제시합니다.  MCPEval은 다양한 도메인에서 평가 프로세스를 구조화하고, 엔드 투 엔드 작업 생성, 검증, 심층 평가를 자동화합니다.  MCPEval을 오픈 소스 툴킷으로 공개함으로써 LLM 에이전트 커뮤니티를 위한 재현 가능하고 확장 가능하며 표준화된 평가 방법을 촉진하고, 더 강력하고 유능한 AI 에이전트를 개발하는 것을 목표로 합니다.  MCPEval의 강점에도 불구하고, 평가가 전적으로 합성 데이터에 의존하고, LLM 기반 평가자의 사용 비용이 높으며, 자동화된 검증 프로세스가 편향을 도입할 수 있다는 한계점이 존재합니다.\n",
            "---\n",
            "다음은 주어진 논문 일부를 5개의 문단으로 요약한 내용입니다. 각 문단은 4~8개의 문장으로 구성되어 있습니다.\n",
            "\n",
            "이 논문은 대규모 언어 모델(LLM)이 실제 소프트웨어 엔지니어링 문제를 해결할 수 있는지를 평가하기 위한 연구입니다. 다양한 LLM 모델들을 대상으로 여러 도메인에서 툴 사용 능력을 평가하는 MCPEval 프레임워크를 사용하여 광범위한 실험을 수행했습니다. 평가는 5,000개의 궤적 기록과 5,000개의 완료 기록을 포함하며, LLM 심판 평가를 통해 50개의 모델-도메인 조합을 체계적으로 분석하여 LLM의 툴 사용 능력에 대한 포괄적인 평가를 제공합니다.\n",
            "\n",
            "실험 설정에는 OpenAI 모델 7개(GPT-4o, GPT-4o-mini, GPT-4.1-mini, GPT-4.1-nano, O3, O3-mini, O4-mini)와 오픈 소스 모델 3개(Mistral-Small-24B, Qwen3-32B, Qwen3-30B-A3B)가 포함되었습니다. 평가는 금융, 의료, Airbnb, 스포츠, 국립 공원이라는 5가지 다양한 응용 분야에서 수행되었습니다. 각 도메인에는 주가, 의료 용어 검색, 숙소 검색, 팀 통계, 공원 정보와 같은 특정 도구들이 사용되었습니다. 이러한 다양한 모델과 도메인을 통해 LLM의 일반적인 툴 사용 능력을 평가할 수 있습니다.\n",
            "\n",
            "AI 에이전트의 능력을 평가하기 위해 툴 호출 성능과 LLM 심판 성능이라는 두 가지 평가 기준을 사용했습니다. 툴 호출 성능은 실제 정답과 비교하여 예측된 툴 사용의 정확성을 측정합니다. LLM 심판 성능은 루브릭 기반 판단을 사용하여 실행 궤적과 작업 완료의 전반적인 품질을 평가합니다. 이 두 가지 기준은 정확한 운영 평가와 높은 수준의 행동 평가를 가능하게 합니다.\n",
            "\n",
            "툴 호출 평가는 MCP 모델 평가기의 분석 명령을 사용하여 수행됩니다. 이 시스템은 엄격한 매칭 프로토콜과 유연한 매칭 프로토콜을 모두 사용하여 에이전트의 예측된 툴 사용과 실제 정답을 비교합니다. 엄격한 매칭은 툴 이름, 매개변수 값, 실행 순서에 대한 정확한 일치를 요구하며, 유연한 매칭은 매개변수 값에 대한 유사성 임계값과 툴 순서에 대한 유사성 임계값을 적용하여 부분적인 점수를 허용합니다.\n",
            "\n",
            "평가 지표에는 평균 이름 일치 점수, 평균 매개변수 일치 점수, 평균 순서 일치 점수 및 평균 전체 점수가 포함됩니다. 평균 이름 일치 점수는 올바르게 예측된 툴 이름의 비율을 측정하고, 평균 매개변수 일치 점수는 매개변수 값의 정확성 또는 유사성을 평가합니다. 평균 순서 일치 점수는 예측된 툴 호출과 실제 툴 호출 간의 순서 정렬을 평가하고, 평균 전체 점수는 위의 지표들을 가중 조합하여 계산합니다. 또한, 시스템은 누락/추가 툴, 매개변수 불일치, 툴별 성공률과 같은 세부적인 진단 정보를 제공하여 자세한 오류 분석을 용이하게 합니다.\n"
          ]
        }
      ],
      "source": [
        "gathered_summaries = '\\n---\\n'.join(raw_summaries)\n",
        "\n",
        "print(gathered_summaries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "fd5d91f1",
      "metadata": {
        "id": "fd5d91f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "이 논문은 LLM 에이전트의 실제 소프트웨어 엔지니어링 문제 해결 능력을 평가하기 위해 MCPEval 프레임워크를 사용하여 다양한 모델을 대상으로 광범위한 실험을 수행했습니다. 5,000개의 궤적 기록과 완료 기록을 분석하고, LLM 심판 평가를 통해 50개의 모델-도메인 조합을 체계적으로 평가하여 LLM의 툴 사용 능력에 대한 포괄적인 평가를 제공합니다. 이 연구는 LLM이 실제 환경에서 복잡한 작업을 수행할 수 있는 잠재력을 탐구하고, 그 한계점을 명확히 밝히는 데 중요한 역할을 합니다.\n",
            "\n",
            "실험에는 OpenAI 모델 7개와 오픈 소스 모델 3개가 포함되었으며, 금융, 의료, Airbnb, 스포츠, 국립 공원이라는 5가지 다양한 응용 분야에서 평가가 진행되었습니다. 각 도메인에는 주가, 의료 용어 검색, 숙소 검색, 팀 통계, 공원 정보와 같은 특정 도구들이 사용되어 LLM의 툴 사용 능력을 다각적으로 평가했습니다. 다양한 모델과 도메인을 통해 LLM이 실제 문제를 해결하는 데 필요한 다양한 기술과 지식을 평가하고, 특정 분야에서의 강점과 약점을 파악할 수 있었습니다.\n",
            "\n",
            "AI 에이전트의 능력을 평가하기 위해 툴 호출 성능과 LLM 심판 성능이라는 두 가지 평가 기준을 사용했습니다. 툴 호출 성능은 실제 정답과 비교하여 예측된 툴 사용의 정확성을 측정하고, LLM 심판 성능은 루브릭 기반 판단을 사용하여 실행 궤적과 작업 완료의 전반적인 품질을 평가했습니다. 이러한 다각적인 평가 접근 방식은 LLM의 툴 사용 능력에 대한 정확하고 포괄적인 평가를 가능하게 합니다.\n",
            "\n",
            "툴 호출 평가는 MCP 모델 평가기의 분석 명령을 사용하여 수행되며, 엄격한 매칭 프로토콜과 유연한 매칭 프로토콜을 모두 사용하여 에이전트의 예측된 툴 사용과 실제 정답을 비교합니다. 엄격한 매칭은 툴 이름, 매개변수 값, 실행 순서에 대한 정확한 일치를 요구하며, 유연한 매칭은 매개변수 값에 대한 유사성 임계값과 툴 순서에 대한 유사성 임계값을 적용하여 부분적인 점수를 허용합니다. 이러한 다양한 매칭 프로토콜은 LLM의 툴 사용 능력에 대한 세밀한 분석을 가능하게 합니다.\n",
            "\n",
            "평가 지표에는 평균 이름 일치 점수, 평균 매개변수 일치 점수, 평균 순서 일치 점수 및 평균 전체 점수가 포함됩니다. 또한, 시스템은 누락/추가 툴, 매개변수 불일치, 툴별 성공률과 같은 세부적인 진단 정보를 제공하여 자세한 오류 분석을 용이하게 합니다. 이러한 포괄적인 평가 지표와 진단 정보는 LLM의 툴 사용 능력을 개선하고, 더 강력하고 신뢰할 수 있는 AI 에이전트를 개발하는 데 중요한 역할을 합니다.\n"
          ]
        }
      ],
      "source": [
        "# Reduce 과정 : 각 문서의 요약을 하나로 합칩니다.\n",
        "reduce_prompt = ChatPromptTemplate([\n",
        "    ('system', '''당신은 인공지능과 거대 언어 모델의 전문가입니다.\n",
        "LLM 논문에 대한 요약문의 리스트가 주어집니다.\n",
        "이를 읽고, 전체 주제를 포함하는 최종 요약을 작성하세요.\n",
        "요약은 5개의 문단과 문단별 4-8개의 문장으로 작성하세요.\n",
        "'''),\n",
        "    ('user', '''{text}\n",
        "---\n",
        "Summary:\n",
        "''')])\n",
        "\n",
        "reduce_chain = reduce_prompt | llm | StrOutputParser()\n",
        "\n",
        "summary = reduce_chain.invoke(gathered_summaries)\n",
        "\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfece695",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
